{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#triton-model-navigator","title":"Triton Model Navigator","text":"<p>The Triton Model Navigator automates the process of moving model from source to deployment on Triton Inference Server. The tool validate possible export and conversion paths to serializable formats like TensorRT and select the most promising format for production deployment.</p>"},{"location":"#how-it-works","title":"How it works?","text":"<p>The Triton Model Navigator is designed to provide a single entrypoint for each supported framework. The usage is simple as call to dedicated <code>optimize</code> function to start the process of searching for the best possible deployment by going through a broad spectrum of model conversions.</p> <p>The <code>optimize</code> internally performs model export, conversion, correctness testing, performance profiling, and saves all generated artifacts in the <code>navigator_workspace</code>, which is represented by a returned <code>package</code> object. The result of <code>optimize</code> process can be saved as a portable Navigator Package with the <code>save</code> function. Saved packages only contain the base model formats along with the best selected format based on latency and throughput. The package can be reused to recreate the process on same or different hardware. The configuration and execution status is saved in the <code>status.yaml</code> file located inside the workspace and the <code>Navigator Package</code>.</p> <p>Finally, the <code>Navigator Package</code> can be used for model deployment on Triton Inference Server. Dedicated API helps with obtaining all necessary parameters and creating <code>model_repository</code> or receive the optimized model for inference in Python environment.</p>"},{"location":"CHANGELOG/","title":"Changelog","text":""},{"location":"CHANGELOG/#changelog","title":"Changelog","text":""},{"location":"CHANGELOG/#052","title":"0.5.2","text":"<ul> <li>new: Added Contributor License Agreement (CLA)</li> <li>fix: Added missing --extra-index-url to installation instruction for pypi</li> <li>fix: Updated wheel readme</li> <li>fix: Do not run TorchScript export when only ONNX in target formats and ONNX extended export is disabled</li> <li> <p>fix: Log full traceback for ModelNavigatorUserInputError</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>PyTorch 2.0.0a0+1767026</li> <li>TensorFlow 2.11.0</li> <li>TensorRT 8.5.3.1</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.44.2</li> <li>GraphSurgeon: 0.3.26</li> <li>tf2onnx v1.14.0</li> <li>Other component versions depend on the used framework containers versions.     See its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#051","title":"0.5.1","text":"<ul> <li>fix: Using relative workspace cause error during Onnx to TensorRT conversion</li> <li>fix: Added external weight in package for ONNX format</li> <li> <p>fix: bugfixes for functional tests</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>PyTorch 1.14.0a0+410ce96</li> <li>TensorFlow 2.11.0</li> <li>TensorRT 8.5.3</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.44.2</li> <li>GraphSurgeon: 0.4.6</li> <li>tf2onnx v1.13.0</li> <li>Other component versions depend on the used framework containers versions.     See its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#050","title":"0.5.0","text":"<ul> <li>new: Support for PyTriton deployment</li> <li>new: Support for Python models with python.optimize API</li> <li>new: PyTorch 2 compile CPU and CUDA runners</li> <li>new: Collect conversion max batch size in status</li> <li>new: PyTorch runners with <code>compile</code> support</li> <li>change: Improved handling CUDA and CPU runners</li> <li>change: Reduced finding device max batch size time by running it once as separate pipeline</li> <li> <p>change: Stored find max batch size result in separate filed in status</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>PyTorch 1.14.0a0+410ce96</li> <li>TensorFlow 2.11.0</li> <li>TensorRT 8.5.3</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.44.2</li> <li>GraphSurgeon: 0.4.6</li> <li>tf2onnx v1.13.0</li> <li>Other component versions depend on the used framework containers versions.     See its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#044","title":"0.4.4","text":"<ul> <li> <p>fix: when exporting single input model to saved model, unwrap one element list with inputs</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>PyTorch 1.14.0a0+410ce96</li> <li>TensorFlow 2.11.0</li> <li>TensorRT 8.5.3</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.44.2</li> <li>GraphSurgeon: 0.4.6</li> <li>tf2onnx v1.13.0</li> <li>Other component versions depend on the used framework containers versions.     See its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#043","title":"0.4.3","text":"<ul> <li> <p>fix: in Keras inference use model.predict(tensor) for single input models</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>PyTorch 1.14.0a0+410ce96</li> <li>TensorFlow 2.11.0</li> <li>TensorRT 8.5.3</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.44.2</li> <li>GraphSurgeon: 0.4.6</li> <li>tf2onnx v1.13.0</li> <li>Other component versions depend on the used framework containers versions.     See its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#042","title":"0.4.2","text":"<ul> <li>fix: loading configuration for trt_profile from package</li> <li>fix: missing reproduction scripts and logs inside package</li> <li>fix: invalid model path in reproduction script for ONNX to TRT conversion</li> <li> <p>fix: collecting metadata from ONNX model in main thread during ONNX to TRT conversion</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>PyTorch 1.14.0a0+410ce96</li> <li>TensorFlow 2.11.0</li> <li>TensorRT 8.5.3</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.44.2</li> <li>GraphSurgeon: 0.4.6</li> <li>tf2onnx v1.13.0</li> <li>Other component versions depend on the used framework containers versions.     See its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#041","title":"0.4.1","text":"<ul> <li> <p>fix: when specified use dynamic axes from custom OnnxConfig</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>PyTorch 1.14.0a0+410ce96</li> <li>TensorFlow 2.11.0</li> <li>TensorRT 8.5.2.2</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.43.1</li> <li>GraphSurgeon: 0.4.6</li> <li>tf2onnx v1.13.0</li> <li>Other component versions depend on the used framework containers versions.     See its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#040","title":"0.4.0","text":"<ul> <li>new: <code>optimize</code> method that replace <code>export</code> and perform max batch size search and improved profiling during process</li> <li>new: Introduced custom configs in <code>optimize</code> for better parametrization of export/conversion commands</li> <li>new: Support for adding user runners for model correctness and profiling</li> <li>new: Search for max possible batch size per format during conversion and profiling</li> <li>new: API for creating Triton model store from Navigator Package and user provided models</li> <li>change: Improved status structure for Navigator Package</li> <li>deprecated: Optimize for Triton Inference Server support</li> <li>deprecated: HuggingFace contrib module</li> <li> <p>Bug fixes and other improvements</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>PyTorch 1.14.0a0+410ce96</li> <li>TensorFlow 2.11.0</li> <li>TensorRT 8.5.2.2</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.43.1</li> <li>GraphSurgeon: 0.4.6</li> <li>tf2onnx v1.13.0</li> <li>Other component versions depend on the used framework containers versions.     See its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#038","title":"0.3.8","text":"<ul> <li> <p>Updated NVIDIA containers defaults to 22.11</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.42.2</li> <li>GraphSurgeon: 0.3.19</li> <li>Triton Model Analyzer 1.20.0</li> <li>tf2onnx: v1.12.1</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#037","title":"0.3.7","text":"<ul> <li> <p>Updated NVIDIA containers defaults to 22.10</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.42.2</li> <li>GraphSurgeon: 0.3.19</li> <li>Triton Model Analyzer 1.20.0</li> <li>tf2onnx: v1.12.1</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#036","title":"0.3.6","text":"<ul> <li>Updated NVIDIA containers defaults to 22.09</li> <li> <p>Model Navigator Export API:</p> <ul> <li>new: cast int64 input data to int32 in runner for Torch-TensorRT</li> <li>new: cast 64-bit data samples to 32-bit values for TensorRT</li> <li>new: verbose flag for logging export and conversion commands to console</li> <li>new: debug flag to enable debug mode for export and conversion commands</li> <li>change: logs from commands are streamed to console during command run</li> <li>change: package load omit the log files and autogenerated scripts</li> </ul> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.42.2</li> <li>GraphSurgeon: 0.3.19</li> <li>Triton Model Analyzer 1.20.0</li> <li>tf2onnx: v1.12.1</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#035","title":"0.3.5","text":"<ul> <li>Updated NVIDIA containers defaults to 22.08</li> <li> <p>Model Navigator Export API:</p> <ul> <li>new: TRTExec runner use <code>use_cuda_graph=True</code> by default</li> <li>new: log warning instead of raising error when dataloader dump inputs with <code>nan</code> or <code>inf</code> values</li> <li>new: enabled logging for command input parameters</li> <li>fix: invalid use of Polygraphy TRT profile when trt_dynamic_axes is passed to export function</li> </ul> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.38.0</li> <li>GraphSurgeon: 0.3.19</li> <li>Triton Model Analyzer 1.19.0</li> <li>tf2onnx: v1.12.1</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#034","title":"0.3.4","text":"<ul> <li>Updated NVIDIA containers defaults to 22.07</li> <li>Model Navigator OTIS:<ul> <li>deprecated: <code>TF32</code> precision for TensorRT from CLI options - will be removed in future versions</li> <li>fix: Tensorflow module was imported when obtaining model signature during conversion</li> </ul> </li> <li> <p>Model Navigator Export API:</p> <ul> <li>new: Support for building framework containers with Model Navigator installed</li> <li>new: Example for loading Navigator Package for reproducing the results</li> <li>new: Create reproducing script for correctness and performance steps</li> <li>new: TrtexecRunner for correctness and performance tests   with trtexec tool</li> <li>new: Use TF32 support by default for models with FP32 precision</li> <li>new: Reset conversion parameters to defaults when using <code>load</code> for package</li> <li>new: Testing all options for JAX export enable_xla and jit_compile parameters</li> <li>change: Profiling stability improvements</li> <li>change: Rename of <code>onnx_runtimes</code> export function parameters to <code>runtimes</code></li> <li>deprecated: <code>TF32</code> precision for TensorRT from available options - will be removed in future versions</li> <li>fix: Do not save TF-TRT models to the .nav package</li> <li>fix: Do not save TF-TRT models from the .nav package</li> <li>fix: Correctly load .nav packages when <code>_input_names</code> or <code>_output_names</code> specified</li> <li>fix: Adjust TF and TF-TRT model signatures to match <code>input_names</code></li> <li>fix: Save ONNX opset for CLI configuration inside package</li> <li>fix: Reproduction scripts were missing for failing paths</li> </ul> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.38.0</li> <li>GraphSurgeon: 0.3.19</li> <li>Triton Model Analyzer 1.17.0</li> <li>tf2onnx: v1.11.1</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#033","title":"0.3.3","text":"<ul> <li> <p>Model Navigator Export API:</p> <ul> <li>new: Improved handling inputs and outputs metadata</li> <li>new: Navigator Package version updated to 0.1.3</li> <li>new: Backward compatibility with previous versions of Navigator Package</li> <li>fix: Dynamic shapes for output shapes were read incorrectly</li> </ul> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.36.2</li> <li>GraphSurgeon: 0.3.19</li> <li>Triton Model Analyzer 1.17.0</li> <li>tf2onnx: v1.11.1</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#032","title":"0.3.2","text":"<ul> <li>Updated NVIDIA containers defaults to 22.06</li> <li>Model Navigator OTIS:<ul> <li>new: Perf Analyzer profiling data use base64 format for content</li> <li>fix: Signature for TensorRT model when has <code>uint64</code> or <code>int64</code> input and/or outputs defined</li> </ul> </li> <li> <p>Model Navigator Export API:</p> <ul> <li>new: Updated navigator package format to 0.1.1</li> <li>new: Added Model Navigator version to status file</li> <li>new: Add atol and rtol configuration to CLI config for model</li> <li>new: Added experimental support for JAX models</li> <li>new: In case of export or conversion failures prepare minimal scripts to reproduce errors</li> <li>fix: Conversion parameters are not stored in Navigator Package for CLI execution</li> </ul> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.36.2</li> <li>GraphSurgeon: 0.3.19</li> <li>Triton Model Analyzer 1.17.0</li> <li>tf2onnx: v1.11.1</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#031","title":"0.3.1","text":"<ul> <li>Updated NVIDIA containers defaults to 22.05</li> <li>Model Navigator OTIS:<ul> <li>fix: Saving paths inside the Triton package status file</li> <li>fix: Empty list of gpus cause the process run on CPU only</li> <li>fix: Reading content from zipped Navigator Package</li> <li>fix: When no GPU or target device set to CPU <code>optimize</code> avoid running unsupported conversions in CLI</li> <li>new: Converter accept passing target device kind to selected CPU or GPU supported conversions</li> <li>new: Added support for OpenVINO accelerator for ONNXRuntime</li> <li>new: Added option <code>--config-search-early-exit-enable</code> for Model Analyzer early exit support   in manual profiling mode</li> <li>new: Added option <code>--model-config-name</code> to the <code>select</code> command.   It allows to pick a particular model configuration for deployment from the set of all configurations   generated by Triton Model Analyzer, even if it's not the best performing one.</li> <li>removed: The <code>--tensorrt-strict-types</code> option has been removed due to deprecation of the functionality   in upstream libraries.</li> </ul> </li> <li> <p>Model Navigator Export API:</p> <ul> <li>new: Added dynamic shapes support and trt dynamic shapes support for TensorFlow2 export</li> <li>new: Improved per format logging</li> <li>new: PyTorch to Torch-TRT precision selection added</li> <li>new: Advanced profiling (measurement windows, configurable batch sizes)</li> </ul> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.36.2</li> <li>GraphSurgeon: 0.3.19</li> <li>Triton Model Analyzer 1.16.0</li> <li>tf2onnx: v1.10.1</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#030","title":"0.3.0","text":"<ul> <li>Updated NVIDIA containers defaults to 22.04</li> <li>Model Navigator Export API<ul> <li>Support for exporting models from TensorFlow2 and PyTorch source code to supported target formats</li> <li>Support for conversion from ONNX to supported target formats</li> <li>Support for exporting HuggingFace models</li> <li>Conversion, Correctness and performance tests for exported models</li> <li>Definition of package structure for storing all exported models and additional metadata</li> </ul> </li> <li>Model Navigator OTIS:<ul> <li>change: <code>run</code> command has been deprecated and may be removed in a future release</li> <li>new: <code>optimize</code> command replace <code>run</code> and produces an output <code>*.triton.nav</code> package</li> <li>new: <code>select</code> selects the best-performing configuration from <code>*.triton.nav</code> package and create a   Triton Inference Server model repository</li> <li>new: Added support for using shared memory option for Perf Analyzer</li> </ul> </li> <li> <p>Remove wkhtmltopdf package dependency</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.35.1</li> <li>GraphSurgeon: 0.3.14</li> <li>Triton Model Analyzer 1.14.0</li> <li>tf2onnx: v1.9.3</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#027","title":"0.2.7","text":"<ul> <li>Updated NVIDIA containers defaults to 22.02</li> <li>Removed support for Python 3.7</li> <li>Triton Model configuration related:<ul> <li>Support dynamic batching without setting preferred batch size value</li> </ul> </li> <li> <p>Profiling related:</p> <ul> <li>Deprecated <code>--config-search-max-preferred-batch-size</code> flag as is no longer supported in Triton Model Analyzer</li> </ul> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.35.1</li> <li>GraphSurgeon: 0.3.14</li> <li>Triton Model Analyzer 1.8.2</li> <li>tf2onnx: v1.9.3</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#026","title":"0.2.6","text":"<ul> <li>Updated NVIDIA containers defaults to 22.01</li> <li>Removed support for Python 3.6 due to EOL</li> <li>Conversion related:<ul> <li>Added support for Torch-TensorRT conversion</li> </ul> </li> <li> <p>Fixes and improvements</p> <ul> <li>Processes inside containers started by Model Navigator now run without root privileges</li> <li>Fix for volume mounts while running Triton Inference Server in container from other container</li> <li>Fix for conversion of models without file extension on input and output paths</li> <li>Fix using <code>--model-format</code> argument when input and output files have no extension</li> </ul> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.35.1</li> <li>GraphSurgeon: 0.3.14</li> <li>Triton Model Analyzer 1.8.2</li> <li>tf2onnx: v1.9.3</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> <li> <p>Known issues and limitations</p> <ul> <li>missing support for stateful models (ex. time-series one)</li> <li>no verification of conversion results for conversions: TF -&gt; ONNX, TF-&gt;TF-TRT, TorchScript -&gt; ONNX</li> <li>possible to define a single profile for TensorRT</li> <li>no custom ops support</li> <li>Triton Inference Server stays in the background when the profile   process is interrupted by the user</li> <li>TF-TRT conversion lost outputs shapes info</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#025","title":"0.2.5","text":"<ul> <li>Updated NVIDIA containers defaults to 21.12</li> <li>Conversion related:<ul> <li>[Experimental] TF-TRT - fixed default dataset profile generation</li> </ul> </li> <li> <p>Configuration Model on Triton related</p> <ul> <li>Fixed name for onnxruntime backend in Triton model deployment configuration</li> </ul> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.33.1</li> <li>GraphSurgeon: 0.3.14</li> <li>Triton Model Analyzer 1.8.2</li> <li>tf2onnx: v1.9.3</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> <li> <p>Known issues and limitations</p> <ul> <li>missing support for stateful models (ex. time-series one)</li> <li>no verification of conversion results for conversions: TF -&gt; ONNX, TF-&gt;TF-TRT, TorchScript -&gt; ONNX</li> <li>possible to define a single profile for TensorRT</li> <li>no custom ops support</li> <li>Triton Inference Server stays in the background when the profile   process is interrupted by the user</li> <li>TF-TRT conversion lost outputs shapes info</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#024-2021-12-07","title":"0.2.4 (2021-12-07)","text":"<ul> <li>Updated NVIDIA containers defaults to 21.10</li> <li>Fixed generating profiling data when <code>dtypes</code> are not passed</li> <li>Conversion related:<ul> <li>[Experimental] Added support for TF-TRT conversion</li> </ul> </li> <li>Configuration Model on Triton related<ul> <li>Added possibility to select batching mode - default, dynamic and disabled options supported</li> </ul> </li> <li>Install dependencies from pip packages instead of wheels for Polygraphy and Triton Model Analyzer</li> <li> <p>fixes and improvements</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.33.1</li> <li>GraphSurgeon: 0.3.14</li> <li>Triton Model Analyzer 1.8.2</li> <li>tf2onnx: v1.9.3</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> <li> <p>Known issues and limitations</p> <ul> <li>missing support for stateful models (ex. time-series one)</li> <li>no verification of conversion results for conversions: TF -&gt; ONNX, TF-&gt;TF-TRT, TorchScript -&gt; ONNX</li> <li>possible to define a single profile for TensorRT</li> <li>no custom ops support</li> <li>Triton Inference Server stays in the background when the profile   process is interrupted by the user</li> <li>TF-TRT conversion lost outputs shapes info</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#023-2021-11-10","title":"0.2.3 (2021-11-10)","text":"<ul> <li>Updated NVIDIA containers defaults to 21.09</li> <li>Improved naming of arguments specific for TensorRT conversion and acceleration with backward compatibility</li> <li>Use pip package for Triton Model Analyzer installation with minimal version 1.8.0</li> <li>Fixed <code>model_repository</code> path to be not relative to <code>&lt;navigator_workspace&gt;</code> dir</li> <li>Handle exit codes correctly from CLI commands</li> <li>Support for use device ids for <code>--gpus</code> argument</li> <li>Conversion related<ul> <li>Added support for precision modes to support multiple precisions during conversion to TensorRT</li> <li>Added <code>--tensorrt-sparse-weights</code> flag for sparse weight optimization for TensorRT</li> <li>Added <code>--tensorrt-strict-types</code> flag forcing it to choose tactics based on the layer precision for TensorRT</li> <li>Added <code>--tensorrt-explicit-precision</code> flag enabling explicit precision mode</li> <li>Fixed nan values appearing in relative tolerance during conversion to TensorRT</li> </ul> </li> <li>Configuration Model on Triton related<ul> <li>Removed default value for <code>engine_count_per_device</code></li> <li>Added possibility to define Triton Custom Backend parameters with <code>triton_backend_parameters</code> command</li> <li>Added possibility to define max workspace size for TensorRT backend accelerator using   argument <code>tensorrt_max_workspace_size</code></li> </ul> </li> <li>Profiling related<ul> <li>Added <code>config_search</code> prefix to all profiling parameters (BREAKING CHANGE)</li> <li>Added <code>config_search_max_preferred_batch_size</code> parameter</li> <li>Added <code>config_search_backend_parameters</code> parameter</li> </ul> </li> <li> <p>fixes and improvements</p> </li> <li> <p>Versions of used external components:</p> <ul> <li>Polygraphy: 0.32.0</li> <li>GraphSurgeon: 0.3.13</li> <li>tf2onnx: v1.9.2 (support for ONNX opset 14,   tf 1.15 and 2.6)</li> <li>Triton Model Analyzer 1.8.2</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> <li> <p>Known issues and limitations</p> <ul> <li>missing support for stateful models (ex. time-series one)</li> <li>missing support for models without batching support</li> <li>no verification of conversion results for conversions: TF -&gt; ONNX, TorchScript -&gt; ONNX</li> <li>possible to define a single profile for TensorRT</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#022-2021-09-06","title":"0.2.2 (2021-09-06)","text":"<ul> <li> <p>Updated NVIDIA containers defaults to 21.08</p> </li> <li> <p>Versions of used external components:</p> <ul> <li>Triton Model Analyzer: 1.7.0</li> <li>Triton Inference Server Client: 2.13.0</li> <li>Polygraphy: 0.31.1</li> <li>GraphSurgeon: 0.3.11</li> <li>tf2onnx: v1.9.1 (support for ONNX opset 14,   tf 1.15 and 2.5)</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> <li> <p>Known issues and limitations</p> <ul> <li>missing support for stateful models (ex. time-series one)</li> <li>missing support for models without batching support</li> <li>no verification of conversion results for conversions: TF -&gt; ONNX, TorchScript -&gt; ONNX</li> <li>possible to define a single profile for TensorRT</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#021-2021-08-17","title":"0.2.1 (2021-08-17)","text":"<ul> <li>Fixed triton-model-config error when tensorrt_capture_cuda_graph flag is not passed</li> <li>Dump Conversion Comparator inputs and outputs into JSON files</li> <li>Added information in logs on the tolerance parameters values to pass the conversion verification</li> <li>Use <code>count_windows</code> mode as default option for Perf Analyzer</li> <li>Added possibility to define custom docker images</li> <li> <p>Bugfixes</p> </li> <li> <p>Versions of used external components:</p> <ul> <li>Triton Model Analyzer: 1.6.0</li> <li>Triton Inference Server Client: 2.12.0</li> <li>Polygraphy: 0.31.1</li> <li>GraphSurgeon: 0.3.11</li> <li>tf2onnx: v1.9.1 (support for ONNX opset 14,   tf 1.15 and 2.5)</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> <li> <p>Known issues and limitations</p> <ul> <li>missing support for stateful models (ex. time-series one)</li> <li>missing support for models without batching support</li> <li>no verification of conversion results for conversions: TF -&gt; ONNX, TorchScript -&gt; ONNX</li> <li>possible to define a single profile for TensorRT</li> <li>TensorRT backend acceleration not supported for ONNX Runtime in Triton Inference Server ver. 21.07</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#020-2021-07-05","title":"0.2.0 (2021-07-05)","text":"<ul> <li> <p>comprehensive refactor of command-line API in order to provide more gradual   pipeline steps execution</p> </li> <li> <p>Versions of used external components:</p> <ul> <li>Triton Model Analyzer: 21.05</li> <li>tf2onnx: v1.8.5 (support for ONNX opset 13,   tf 1.15 and 2.5)</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> <li> <p>Known issues and limitations</p> <ul> <li>missing support for stateful models (ex. time-series one)</li> <li>missing support for models without batching support</li> <li>no verification of conversion results for conversions: TF -&gt; ONNX, TorchScript -&gt; ONNX</li> <li>issues with TorchScript -&gt; ONNX conversion due   to issue in PyTorch 1.8<ul> <li>affected NVIDIA PyTorch containers: 20.12, 21.02, 21.03</li> <li>workaround: use PyTorch containers newer than 21.03</li> </ul> </li> <li>possible to define a single profile for TensorRT</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#011-2021-04-12","title":"0.1.1 (2021-04-12)","text":"<ul> <li>documentation update</li> </ul>"},{"location":"CHANGELOG/#010-2021-04-09","title":"0.1.0 (2021-04-09)","text":"<ul> <li> <p>Release of main components:</p> <ul> <li>Model Converter - converts the model to a set of variants optimized for inference or to be later optimized by   Triton Inference Server backend.</li> <li>Model Repo Builder - setup Triton Inference Server Model Repository, including its configuration.</li> <li>Model Analyzer - select optimal Triton Inference Server configuration based on models compute and memory   requirements,   available computation infrastructure, and model application constraints.</li> <li>Helm Chart Generator - deploy Triton Inference Server and model with optimal configuration to cloud.</li> </ul> </li> <li> <p>Versions of used external components:</p> <ul> <li>Triton Model Analyzer: 21.03+616e8a30</li> <li>tf2onnx: v1.8.4 (support for ONNX opset 13, tf 1.15   and 2.4)</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   Refer to its support matrix   for a detailed summary.</li> </ul> </li> <li> <p>Known issues</p> <ul> <li>missing support for stateful models (ex. time-series one)</li> <li>missing support for models without batching support</li> <li>no verification of conversion results for conversions: TF -&gt; ONNX, TorchScript -&gt; ONNX</li> <li>issues with TorchScript -&gt; ONNX conversion due   to issue in PyTorch 1.8<ul> <li>affected NVIDIA PyTorch containers: 20.12, 21.03</li> <li>workaround: use containers different from above</li> </ul> </li> <li>Triton Inference Server stays in the background when the profile process is interrupted by the user</li> </ul> </li> </ul>"},{"location":"CONTRIBUTING/","title":"Contributing","text":""},{"location":"CONTRIBUTING/#contributing","title":"Contributing","text":"<p>Contributions are welcome, and they are much appreciated! Every little helps, and we will always give credit.</p>"},{"location":"CONTRIBUTING/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"CONTRIBUTING/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/triton-inference-server/model_navigator/issues.</p> <p>If you are reporting a bug, include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"CONTRIBUTING/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"CONTRIBUTING/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"CONTRIBUTING/#write-documentation","title":"Write Documentation","text":"<p>The Triton Model Navigator could always use more documentation, whether as part of the official Triton Model Navigator docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"CONTRIBUTING/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/triton-inference-server/model_navigator/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible to make it easier to implement.</li> </ul>"},{"location":"CONTRIBUTING/#sign-your-work","title":"Sign your Work","text":"<p>We require that all contributors \"sign-off\" on their commits. This certifies that the contribution is your original work, or you have the rights to submit it under the same license or a compatible license.</p> <p>Any contribution which contains commits that are not Signed-Off will not be accepted.</p> <p>To sign off on a commit, you simply use the <code>--signoff</code> (or <code>-s</code>) option when committing your changes: <pre><code>$ git commit -s -m \"Add cool feature.\n</code></pre></p> <p>This will append the following to your commit message:</p> <pre><code>Signed-off-by: Your Name &lt;your@email.com&gt;\n</code></pre> <p>By doing this, you certify the below:</p> <pre><code>Developer Certificate of Origin\nVersion 1.1\n\nCopyright (C) 2004, 2006 The Linux Foundation and its contributors.\n1 Letterman Drive\nSuite D4700\nSan Francisco, CA, 94129\n\nEveryone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.\n\nDeveloper's Certificate of Origin 1.1\n\nBy making a contribution to this project, I certify that:\n\n(a) The contribution was created in whole or in part by me and I have the right to submit it under the open source license indicated in the file; or\n\n(b) The contribution is based upon previous work that, to the best of my knowledge, is covered under an appropriate open source license and I have the right under that license to submit that work with modifications, whether created in whole or in part by me, under the same open source license (unless I am permitted to submit under a different license), as indicated in the file; or\n\n(c) The contribution was provided directly to me by some other person who certified (a), (b) or (c) and I have not modified it.\n\n(d) I understand and agree that this project and the contribution are public and that a record of the contribution (including all personal information I submit with it, including my sign-off) is maintained indefinitely and may be redistributed consistent with this project or the open source license(s) involved.\n</code></pre>"},{"location":"CONTRIBUTING/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up the <code>Triton Model Navigator</code> for local development.</p> <ol> <li>Fork the <code>Triton Model Navigator</code> repo on GitHub.</li> <li>Clone your fork locally:</li> </ol> <pre><code>$ git clone git@github.com:your_name_here/model-navigator.git\n</code></pre> <ol> <li>Install your local copy into a virtualenv. Assuming you have virtualenvwrapper installed, this is how you set up your fork for local development:</li> </ol> <pre><code>$ mkvirtualenv model_navigator\n$ cd model_navigator/\n$ make install-dev\n</code></pre> <ol> <li>Create a branch for local development:</li> </ol> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> <ol> <li>When you're done making changes, check that your changes pass linters and the    tests, including testing other Python versions with tox:</li> </ol> <pre><code>$ make lint  # will run i.a. flake8 and pytype linters\n$ make test  # will run a test with on your current virtualenv\n$ make test-fw  # will run a framework test inside framework container\n</code></pre> <ol> <li>Commit your changes and push your branch to GitHub:</li> </ol> <pre><code>$ git add .\n$ git commit -s -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> <ol> <li>Submit a pull request through the GitHub website.</li> </ol>"},{"location":"CONTRIBUTING/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, you should update the docs. Put    your new functionality into a function with a docstring, and add the    feature to the list in README.md.</li> </ol>"},{"location":"CONTRIBUTING/#tips","title":"Tips","text":"<p>To run a subset of tests:</p> <pre><code>$ pytest tests.test_model_navigator\n</code></pre>"},{"location":"CONTRIBUTING/#releasing","title":"Releasing","text":"<p>As a reminder for the maintainers on how to deploy - make sure all your changes are committed (including an entry in CHANGELOG.md) into the master branch. Then run:</p> <pre><code>$ bump2version patch # possible: major / minor / patch\n$ git push\n$ git push --tags\n</code></pre>"},{"location":"CONTRIBUTING/#documentation","title":"Documentation","text":"<p>Add/update docstrings as defined in Google Style Guide.</p>"},{"location":"CONTRIBUTING/#contributor-license-agreement-cla","title":"Contributor License Agreement (CLA)","text":"<p>Triton requires that all contributors (or their corporate entity) send a signed copy of the Contributor License Agreement to triton-cla@nvidia.com. NOTE: Contributors with no company affiliation can fill <code>N/A</code> in the <code>Corporation Name</code> and <code>Corporation Address</code> fields.</p>"},{"location":"LICENSE/","title":"License","text":"<pre><code>                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n</code></pre> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol>"},{"location":"common/","title":"Common","text":""},{"location":"common/#common-api","title":"Common API","text":"<p>Framework indepedent definitions and enumerations.</p>"},{"location":"common/#model_navigator.api.config.SizedIterable","title":"<code>model_navigator.api.config.SizedIterable</code>","text":"<p>         Bases: <code>Protocol</code></p> <p>Protocol representing sized iterable. Used by dataloader.</p>"},{"location":"common/#model_navigator.api.config.SizedIterable.__iter__","title":"<code>__iter__()</code>","text":"<p>Magic method iter.</p> <p>Returns:</p> Type Description <code>Iterator</code> <p>Iterator to next item.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def __iter__(self) -&gt; Iterator:\n\"\"\"Magic method __iter__.\n    Returns:\n        Iterator to next item.\n    \"\"\"\n...\n</code></pre>"},{"location":"common/#model_navigator.api.config.SizedIterable.__len__","title":"<code>__len__()</code>","text":"<p>Magic method len.</p> <p>Returns:</p> Type Description <code>int</code> <p>Length of size iterable.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def __len__(self) -&gt; int:\n\"\"\"Magic method __len__.\n    Returns:\n        Length of size iterable.\n    \"\"\"\n...\n</code></pre>"},{"location":"common/#model_navigator.api.config.SizedDataLoader","title":"<code>model_navigator.api.config.SizedDataLoader = Union[SizedIterable, Sequence]</code>  <code>module-attribute</code>","text":""},{"location":"common/#model_navigator.api.config.Format","title":"<code>model_navigator.api.config.Format</code>","text":"<p>         Bases: <code>Enum</code></p> <p>All model formats supported by Model Navigator 'optimize' function.</p>"},{"location":"common/#model_navigator.api.config.CustomConfig","title":"<code>model_navigator.api.config.CustomConfig</code>","text":"<p>         Bases: <code>abc.ABC</code></p> <p>Base class used for custom configs. Input for Model Navigator <code>optimize</code> method.</p>"},{"location":"common/#model_navigator.api.config.CustomConfig.defaults","title":"<code>defaults()</code>","text":"<p>Update parameters to defaults.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def defaults(self) -&gt; None:\n\"\"\"Update parameters to defaults.\"\"\"\nreturn None\n</code></pre>"},{"location":"common/#model_navigator.api.config.CustomConfig.from_dict","title":"<code>from_dict(config_dict)</code>  <code>classmethod</code>","text":"<p>Instantiate CustomConfig from a dictionary.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: Dict[str, Any]) -&gt; \"CustomConfig\":\n\"\"\"Instantiate CustomConfig from a dictionary.\"\"\"\nreturn cls(**config_dict)\n</code></pre>"},{"location":"common/#model_navigator.api.config.CustomConfig.name","title":"<code>name()</code>  <code>classmethod</code> <code>abstractmethod</code>","text":"<p>Name of the CustomConfig.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\n@abc.abstractmethod\ndef name(cls) -&gt; str:\n\"\"\"Name of the CustomConfig.\"\"\"\nraise NotImplementedError()\n</code></pre>"},{"location":"common/#model_navigator.api.config.CustomConfigForFormat","title":"<code>model_navigator.api.config.CustomConfigForFormat</code>","text":"<p>         Bases: <code>DataObject</code>, <code>CustomConfig</code></p> <p>Abstract base class used for custom configs representing particular format.</p>"},{"location":"common/#model_navigator.api.config.CustomConfigForFormat.format","title":"<code>format: Format</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Format represented by CustomConfig.</p>"},{"location":"common/#model_navigator.api.config.Sample","title":"<code>model_navigator.api.config.Sample = Dict[str, numpy.ndarray]</code>  <code>module-attribute</code>","text":""},{"location":"common/#model_navigator.api.config.VerifyFunction","title":"<code>model_navigator.api.config.VerifyFunction = Callable[[Iterable[Sample], Iterable[Sample]], bool]</code>  <code>module-attribute</code>","text":""},{"location":"common/#model_navigator.api.ProfilerConfig","title":"<code>model_navigator.api.ProfilerConfig</code>  <code>dataclass</code>","text":"<p>         Bases: <code>DataObject</code></p> <p>Profiler configuration.</p> <p>For each batch size profiler will run measurments in windows. Depending on the measurement mode, each window will have fixed time length (MeasurementMode.TIME_WINDOWS) or fixed number of requests (MeasurementMode.COUNT_WINDOWS). Batch sizes are profiled in the ascending order.</p> <p>Profiler will run multiple trials and will stop when the measurements are stable (within <code>stability_percentage</code> from the mean) within three consecutive windows. If the measurements are not stable after <code>max_trials</code> trials, the profiler will stop with an error. Profiler will also stop profiling when the throughput does not increase at least by <code>throughput_cutoff_threshold</code>.</p> <p>Parameters:</p> Name Type Description Default <code>run_profiling</code> <code>bool</code> <p>If True, run profiling, otherwise skip profiling.</p> <code>True</code> <code>batch_sizes</code> <code>Optional[List[Union[int, None]]]</code> <p>List of batch sizes to profile. None means that the model does not support batching.</p> <code>None</code> <code>measurement_mode</code> <code>MeasurementMode</code> <p>Measurement mode.</p> <code>MeasurementMode.COUNT_WINDOWS</code> <code>measurement_interval</code> <code>Optional[float]</code> <p>Measurement interval in milliseconds. Used only in MeasurementMode.TIME_WINDOWS mode.</p> <code>5000</code> <code>measurement_request_count</code> <code>Optional[int]</code> <p>Number of requests to measure in each window. Used only in MeasurementMode.COUNT_WINDOWS mode.</p> <code>50</code> <code>stability_percentage</code> <code>float</code> <p>Allowed percentage of variation from the mean in three consecutive windows.</p> <code>10.0</code> <code>max_trials</code> <code>int</code> <p>Maximum number of window trials.</p> <code>10</code> <code>throughput_cutoff_threshold</code> <code>float</code> <p>Minimum throughput increase to continue profiling.</p> <code>DEFAULT_PROFILING_THROUGHPUT_CUTOFF_THRESHOLD</code>"},{"location":"common/#model_navigator.api.config.ProfilerConfig.from_dict","title":"<code>from_dict(profiler_config_dict)</code>  <code>classmethod</code>","text":"<p>Instantiate ProfilerConfig class from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>profiler_config_dict</code> <code>Mapping</code> <p>Data dictionary.</p> required <p>Returns:</p> Type Description <code>ProfilerConfig</code> <p>ProfilerConfig</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\ndef from_dict(cls, profiler_config_dict: Mapping) -&gt; \"ProfilerConfig\":\n\"\"\"Instantiate ProfilerConfig class from a dictionary.\n    Args:\n        profiler_config_dict (Mapping): Data dictionary.\n    Returns:\n        ProfilerConfig\n    \"\"\"\nreturn cls(\nrun_profiling=profiler_config_dict.get(\"run_profiling\", True),\nbatch_sizes=profiler_config_dict.get(\"batch_sizes\"),\nmeasurement_interval=profiler_config_dict.get(\"measurement_interval\"),\nmeasurement_mode=MeasurementMode(\nprofiler_config_dict.get(\"measurement_mode\", MeasurementMode.TIME_WINDOWS)\n),\nmeasurement_request_count=profiler_config_dict.get(\"measurement_request_count\"),\nstability_percentage=profiler_config_dict.get(\"stability_percentage\", 10.0),\nmax_trials=profiler_config_dict.get(\"max_trials\", 10),\nthroughput_cutoff_threshold=profiler_config_dict.get(\"throughput_cutoff_threshold\", -2),\n)\n</code></pre>"},{"location":"common/#model_navigator.api.MeasurementMode","title":"<code>model_navigator.api.MeasurementMode</code>","text":"<p>         Bases: <code>Enum</code></p> <p>Measurement mode.</p> <p><code>TIME_WINDOWS</code> mode run measurement windows with fixed time length. <code>COUNT_WINDOWS</code> mode run measurement windows with fixed number of requests.</p>"},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#examples","title":"Examples","text":"<p>We provide examples how to use Model Navigator to optimize models in frameworks (PyTorch, TensorFlow2, JAX, ONNX), from existing .nav packages, and also how to deploy optimized models on the NVIDIA Triton Inference Server.</p>"},{"location":"examples/#optimize-models-in-frameworks","title":"Optimize models in frameworks","text":"<p>You can find examples per each supported framework.</p> <p><code>Python</code>: - Identity Model</p> <p><code>PyTorch</code>:</p> <ul> <li>Linear Model</li> <li>ResNet50</li> <li>BERT</li> </ul> <p><code>TensorFlow</code>:</p> <ul> <li>Linear Model</li> <li>EfficientNet</li> <li>BERT</li> </ul> <p><code>JAX</code>:</p> <ul> <li>Linear Model</li> <li>GPT-2</li> </ul> <p><code>ONNX</code>:</p> <ul> <li>Identity Model</li> </ul>"},{"location":"examples/#optimize-navigator-package","title":"Optimize Navigator Package","text":"<p>The Navigator Package can be reused for optimize e.g. on the new hardware or with newer libraries. The example code can be found in examples/package.</p>"},{"location":"examples/#using-model-on-pytriton","title":"Using model on PyTriton","text":"<p>The optimized model by Triton Model Navigator can be used for serving inference through PyTriton. The example code can be found in examples/pytriton.</p>"},{"location":"examples/#using-model-on-triton-inference-server","title":"Using model on Triton Inference Server","text":"<p>The optimized model by Triton Model Navigator can be used for serving inference through Triton Inference Server. The example code can be found in examples/triton.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation","title":"Installation","text":"<p>This section describe how to install the tool. We assume you are comfortable with Python programming language and familiar with Machine Learning models.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>The following prerequisites must be fulfilled to use Triton Model Navigator</p> <ul> <li>Installed Python <code>3.8+</code></li> <li>Installed NVIDIA TensorRT for TensorRT models export.</li> </ul> <p>We recommend to use NGC Containers for PyTorch and TensorFlow which provide have all necessary dependencies:</p> <ul> <li>PyTorch</li> <li>TensorFlow</li> </ul> <p>The library can be installed in:</p> <ul> <li>system environment</li> <li>virtualenv</li> <li>Docker</li> </ul> <p>The NVIDIA optimized Docker images for Python frameworks could be obtained from NVIDIA NGC Catalog.</p> <p>For using NVIDIA optimized Docker images we recommend to install NVIDIA Container Toolkit to run model inference on NVIDIA GPU.</p>"},{"location":"installation/#installation_1","title":"Installation","text":"<p>The package can be installed from <code>pypi.org</code> using extra index url:</p> <pre><code>pip install -U --extra-index-url https://pypi.ngc.nvidia.com triton-model-navigator[&lt;extras,&gt;]\n</code></pre> <p>or with nvidia-pyindex:</p> <pre><code>pip install nvidia-pyindex\npip install -U triton-model-navigator[&lt;extras,&gt;]\n</code></pre> <p>To install Triton Model Navigator from source use pip command:</p> <pre><code>$ pip install --extra-index-url https://pypi.ngc.nvidia.com .[&lt;extras,&gt;]\n</code></pre> <p>Extras:</p> <ul> <li><code>tensorflow</code> - Model Navigator with dependencies for TensorFlow2</li> <li><code>jax</code> - Model Navigator with dependencies for JAX</li> </ul> <p>For using with PyTorch no extras are needed.</p>"},{"location":"installation/#building-the-wheel","title":"Building the wheel","text":"<p>The Triton Model Navigator can be built as wheel. On that purpose the Makefile provide necessary commands.</p> <p>The first is required to install necessary packages to perform build. <pre><code>make install-dev\n</code></pre></p> <p>Once the environment contain required packages run: <pre><code>make dist\n</code></pre></p> <p>The wheel is going to be generated in <code>dist</code> catalog.</p>"},{"location":"jax/","title":"JAX","text":""},{"location":"jax/#model_navigator.api.jax","title":"<code>model_navigator.api.jax</code>","text":"<p>JAX optimize API.</p>"},{"location":"jax/#model_navigator.api.jax.optimize","title":"<code>optimize(model, model_params, dataloader, sample_count=DEFAULT_SAMPLE_COUNT, batching=True, target_formats=None, target_device=DeviceKind.CUDA, runners=None, profiler_config=None, workspace=None, verbose=False, debug=False, verify_func=None, custom_configs=None)</code>","text":"<p>Function exports JAX model to all supported formats.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Callable</code> <p>JAX forward function</p> required <code>model_params</code> <code>Any</code> <p>JAX model parameters (weights)</p> required <code>dataloader</code> <code>SizedDataLoader</code> <p>Sized iterable with data that will be feed to the model</p> required <code>sample_count</code> <code>int</code> <p>Limits how many samples will be used from dataloader</p> <code>DEFAULT_SAMPLE_COUNT</code> <code>batching</code> <code>Optional[bool]</code> <p>Enable or disable batching on first (index 0) dimension of the model</p> <code>True</code> <code>target_formats</code> <code>Optional[Union[Union[str, Format], Tuple[Union[str, Format], ...]]]</code> <p>Target model formats for optimize process</p> <code>None</code> <code>target_device</code> <code>Optional[DeviceKind]</code> <p>Target device for optimize process, default is CUDA</p> <code>DeviceKind.CUDA</code> <code>runners</code> <code>Optional[Union[Union[str, Type[NavigatorRunner]], Tuple[Union[str, Type[NavigatorRunner]], ...]]]</code> <p>Use only runners provided as paramter</p> <code>None</code> <code>profiler_config</code> <code>Optional[ProfilerConfig]</code> <p>Profiling config</p> <code>None</code> <code>workspace</code> <code>Optional[Path]</code> <p>Workspace where packages will be extracted</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Enable verbose logging</p> <code>False</code> <code>debug</code> <code>bool</code> <p>Enable debug logging from commands</p> <code>False</code> <code>verify_func</code> <code>Optional[VerifyFunction]</code> <p>Function for additional model verifcation</p> <code>None</code> <code>custom_configs</code> <code>Optional[Sequence[CustomConfig]]</code> <p>Sequence of CustomConfigs used to control produced artifacts</p> <code>None</code> <p>Returns:</p> Type Description <code>Package</code> <p>Package descriptor representing created package.</p> Source code in <code>model_navigator/api/jax.py</code> <pre><code>def optimize(\nmodel: Callable,\nmodel_params: Any,\ndataloader: SizedDataLoader,\nsample_count: int = DEFAULT_SAMPLE_COUNT,\nbatching: Optional[bool] = True,\ntarget_formats: Optional[Union[Union[str, Format], Tuple[Union[str, Format], ...]]] = None,\ntarget_device: Optional[DeviceKind] = DeviceKind.CUDA,\nrunners: Optional[Union[Union[str, Type[NavigatorRunner]], Tuple[Union[str, Type[NavigatorRunner]], ...]]] = None,\nprofiler_config: Optional[ProfilerConfig] = None,\nworkspace: Optional[Path] = None,\nverbose: bool = False,\ndebug: bool = False,\nverify_func: Optional[VerifyFunction] = None,\ncustom_configs: Optional[Sequence[CustomConfig]] = None,\n) -&gt; Package:\n\"\"\"Function exports JAX model to all supported formats.\n    Args:\n        model: JAX forward function\n        model_params: JAX model parameters (weights)\n        dataloader: Sized iterable with data that will be feed to the model\n        sample_count: Limits how many samples will be used from dataloader\n        batching: Enable or disable batching on first (index 0) dimension of the model\n        target_formats: Target model formats for optimize process\n        target_device: Target device for optimize process, default is CUDA\n        runners: Use only runners provided as paramter\n        profiler_config: Profiling config\n        workspace: Workspace where packages will be extracted\n        verbose: Enable verbose logging\n        debug: Enable debug logging from commands\n        verify_func: Function for additional model verifcation\n        custom_configs: Sequence of CustomConfigs used to control produced artifacts\n    Returns:\n        Package descriptor representing created package.\n    \"\"\"\nif target_device == DeviceKind.CPU and any(\n[device.device_type == \"GPU\" for device in tensorflow.config.get_visible_devices()]\n):\nraise ModelNavigatorConfigurationError(\n\"\\n\"\n\"    'target_device == nav.DeviceKind.CPU' is not supported for TensorFlow2 \"\n\"(exported from JAX) when GPU is available.\\n\"\n\"    To optimize model for CPU, disable GPU with: \"\n\"'tf.config.set_visible_devices([], 'GPU')' directly after importing TensorFlow.\\n\"\n)\nif isinstance(model, str):\nmodel = Path(model)\nif workspace is None:\nworkspace = get_default_workspace()\nif target_formats is None:\ntarget_formats = DEFAULT_JAX_TARGET_FORMATS\nsample = next(iter(dataloader))\nforward_kw_names = tuple(sample.keys()) if isinstance(sample, Mapping) else None\nif runners is None:\nrunners = default_runners(device_kind=target_device)\nif profiler_config is None:\nprofiler_config = ProfilerConfig()\ntarget_formats_enums = enums.parse(target_formats, Format)\nrunner_names = enums.parse(runners, lambda runner: runner if isinstance(runner, str) else runner.name())\nif Format.JAX not in target_formats_enums:\ntarget_formats_enums = (Format.JAX,) + target_formats_enums\nconfig = CommonConfig(\nFramework.JAX,\nmodel=JaxModel(model=model, params=model_params),\ndataloader=dataloader,\nforward_kw_names=forward_kw_names,\nworkspace=workspace,\ntarget_formats=target_formats_enums,\ntarget_device=target_device,\nsample_count=sample_count,\nbatch_dim=0 if batching else None,\nrunner_names=runner_names,\nprofiler_config=profiler_config,\nverbose=verbose,\ndebug=debug,\nverify_func=verify_func,\ncustom_configs=map_custom_configs(custom_configs=custom_configs),\n)\nmodels_config = ModelConfigBuilder.generate_model_config(\nframework=Framework.JAX,\ntarget_formats=target_formats_enums,\ncustom_configs=custom_configs,\n)\nbuilders = [\npreprocessing_builder,\njax_export_builder,\nfind_device_max_batch_size_builder,\ntensorflow_conversion_builder,\ncorrectness_builder,\n]\nif profiler_config.run_profiling:\nbuilders.append(profiling_builder)\nbuilders.append(verify_builder)\npackage = PipelineManager.run(\npipeline_builders=builders,\nconfig=config,\nmodels_config=models_config,\n)\nreturn package\n</code></pre>"},{"location":"known_issues/","title":"Known Issues","text":""},{"location":"known_issues/#known-issues-and-limitations","title":"Known Issues and Limitations","text":"<ul> <li>Source model running in Python can cause OOM issue when GPU memory is larger than CPU RAM memory</li> <li>Verify command could potentially experience CUDA OOM errors while trying to run inference on two models at the same time.</li> </ul>"},{"location":"onnx/","title":"ONNX","text":""},{"location":"onnx/#model_navigator.api.config.OnnxConfig","title":"<code>model_navigator.api.config.OnnxConfig</code>  <code>dataclass</code>","text":"<p>         Bases: <code>CustomConfigForFormat</code></p> <p>ONNX custom config used for ONNX export and conversion.</p> <p>Parameters:</p> Name Type Description Default <code>opset</code> <code>Optional[int]</code> <p>ONNX opset used for conversion.</p> <code>DEFAULT_ONNX_OPSET</code> <code>dynamic_axes</code> <code>Optional[Dict[str, Union[Dict[int, str], List[int]]]]</code> <p>Dynamic axes for ONNX conversion.</p> <code>None</code> <code>onnx_extended_conversion</code> <code>bool</code> <p>Enables additional conversions from TorchScript to ONNX.</p> <code>False</code>"},{"location":"onnx/#model_navigator.api.config.OnnxConfig.format","title":"<code>format: Format</code>  <code>property</code>","text":"<p>Format represented by CustomConfig.</p> <p>Returns:</p> Type Description <code>Format</code> <p>OnnxConfig format</p>"},{"location":"onnx/#model_navigator.api.config.OnnxConfig.name","title":"<code>name()</code>  <code>classmethod</code>","text":"<p>Name of the config.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n\"\"\"Name of the config.\"\"\"\nreturn \"Onnx\"\n</code></pre>"},{"location":"onnx/#model_navigator.api.onnx","title":"<code>model_navigator.api.onnx</code>","text":"<p>ONNX optimize API.</p>"},{"location":"onnx/#model_navigator.api.onnx.optimize","title":"<code>optimize(model, dataloader, sample_count=DEFAULT_SAMPLE_COUNT, batching=True, target_formats=None, target_device=DeviceKind.CUDA, runners=None, profiler_config=None, workspace=None, verbose=False, debug=False, verify_func=None, custom_configs=None)</code>","text":"<p>Function exports ONNX model to all supported formats.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[Path, str]</code> <p>ONNX model path or string</p> required <code>dataloader</code> <code>SizedDataLoader</code> <p>Sized iterable with data that will be feed to the model</p> required <code>sample_count</code> <code>int</code> <p>Limits how many samples will be used from dataloader</p> <code>DEFAULT_SAMPLE_COUNT</code> <code>batching</code> <code>Optional[bool]</code> <p>Enable or disable batching on first (index 0) dimension of the model</p> <code>True</code> <code>target_formats</code> <code>Optional[Union[Union[str, Format], Tuple[Union[str, Format], ...]]]</code> <p>Target model formats for optimize process</p> <code>None</code> <code>target_device</code> <code>Optional[DeviceKind]</code> <p>Target device for optimize process, default is CUDA</p> <code>DeviceKind.CUDA</code> <code>runners</code> <code>Optional[Union[Union[str, Type[NavigatorRunner]], Tuple[Union[str, Type[NavigatorRunner]], ...]]]</code> <p>Use only runners provided as parameter</p> <code>None</code> <code>profiler_config</code> <code>Optional[ProfilerConfig]</code> <p>Profiling config</p> <code>None</code> <code>workspace</code> <code>Optional[Path]</code> <p>Workspace where packages will be extracted</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Enable verbose logging</p> <code>False</code> <code>debug</code> <code>bool</code> <p>Enable debug logging from commands</p> <code>False</code> <code>verify_func</code> <code>Optional[VerifyFunction]</code> <p>Function for additional model verification</p> <code>None</code> <code>custom_configs</code> <code>Optional[Sequence[CustomConfig]]</code> <p>Sequence of CustomConfigs used to control produced artifacts</p> <code>None</code> <p>Returns:</p> Type Description <code>Package</code> <p>Package descriptor representing created package.</p> Source code in <code>model_navigator/api/onnx.py</code> <pre><code>def optimize(\nmodel: Union[Path, str],\ndataloader: SizedDataLoader,\nsample_count: int = DEFAULT_SAMPLE_COUNT,\nbatching: Optional[bool] = True,\ntarget_formats: Optional[Union[Union[str, Format], Tuple[Union[str, Format], ...]]] = None,\ntarget_device: Optional[DeviceKind] = DeviceKind.CUDA,\nrunners: Optional[Union[Union[str, Type[NavigatorRunner]], Tuple[Union[str, Type[NavigatorRunner]], ...]]] = None,\nprofiler_config: Optional[ProfilerConfig] = None,\nworkspace: Optional[Path] = None,\nverbose: bool = False,\ndebug: bool = False,\nverify_func: Optional[VerifyFunction] = None,\ncustom_configs: Optional[Sequence[CustomConfig]] = None,\n) -&gt; Package:\n\"\"\"Function exports ONNX model to all supported formats.\n    Args:\n        model: ONNX model path or string\n        dataloader: Sized iterable with data that will be feed to the model\n        sample_count: Limits how many samples will be used from dataloader\n        batching: Enable or disable batching on first (index 0) dimension of the model\n        target_formats: Target model formats for optimize process\n        target_device: Target device for optimize process, default is CUDA\n        runners: Use only runners provided as parameter\n        profiler_config: Profiling config\n        workspace: Workspace where packages will be extracted\n        verbose: Enable verbose logging\n        debug: Enable debug logging from commands\n        verify_func: Function for additional model verification\n        custom_configs: Sequence of CustomConfigs used to control produced artifacts\n    Returns:\n        Package descriptor representing created package.\n    \"\"\"\nif isinstance(model, str):\nmodel = Path(model)\nif workspace is None:\nworkspace = get_default_workspace()\nif target_formats is None:\ntarget_formats = DEFAULT_ONNX_TARGET_FORMATS\nif runners is None:\nrunners = default_runners(device_kind=target_device)\nif profiler_config is None:\nprofiler_config = ProfilerConfig()\ntarget_formats_enums = enums.parse(target_formats, Format)\nrunner_names = enums.parse(runners, lambda runner: runner if isinstance(runner, str) else runner.name())\nif Format.ONNX not in target_formats_enums:\ntarget_formats_enums = (Format.ONNX,) + target_formats_enums\nconfig = CommonConfig(\nFramework.ONNX,\nmodel=model,\ndataloader=dataloader,\nworkspace=workspace,\ntarget_formats=target_formats_enums,\ntarget_device=target_device,\nsample_count=sample_count,\nbatch_dim=0 if batching else None,\nrunner_names=runner_names,\nprofiler_config=profiler_config,\nverbose=verbose,\ndebug=debug,\nverify_func=verify_func,\ncustom_configs=map_custom_configs(custom_configs=custom_configs),\n)\nmodels_config = ModelConfigBuilder.generate_model_config(\nframework=Framework.ONNX,\ntarget_formats=target_formats_enums,\ncustom_configs=custom_configs,\n)\nbuilders = [\npreprocessing_builder,\nonnx_export_builder,\nfind_device_max_batch_size_builder,\nonnx_conversion_builder,\ncorrectness_builder,\n]\nif profiler_config.run_profiling:\nbuilders.append(profiling_builder)\nbuilders.append(verify_builder)\npackage = PipelineManager.run(\npipeline_builders=builders,\nconfig=config,\nmodels_config=models_config,\n)\nreturn package\n</code></pre>"},{"location":"package/","title":"Package","text":""},{"location":"package/#model_navigator.api.package","title":"<code>model_navigator.api.package</code>","text":"<p>Package operations related API.</p>"},{"location":"package/#model_navigator.api.package.get_best_model_status","title":"<code>get_best_model_status(package, strategy=None, include_source=True)</code>","text":"<p>Returns ModelStatus of best model for given strategy.</p> <p>If model with given strategy cannot be found, search is repeated with MaxThroughputStrategy. If there is no model match given strategy or MaxThroughputStrategy, function returns None.</p> <p>Parameters:</p> Name Type Description Default <code>package</code> <code>Package</code> <p>A package object to be searched for best model.</p> required <code>strategy</code> <code>Optional[RuntimeSearchStrategy]</code> <p>Strategy for finding the best model. Defaults to <code>MaxThroughputAndMinLatencyStrategy</code></p> <code>None</code> <code>include_source</code> <code>bool</code> <p>Flag if Python based model has to be included in analysis</p> <code>True</code> <p>Returns:</p> Type Description <code>Optional[ModelStatus]</code> <p>ModelStatus of best model for given strategy or None.</p> Source code in <code>model_navigator/api/package.py</code> <pre><code>def get_best_model_status(\npackage: Package,\nstrategy: Optional[RuntimeSearchStrategy] = None,\ninclude_source: bool = True,\n) -&gt; Optional[ModelStatus]:\n\"\"\"Returns ModelStatus of best model for given strategy.\n    If model with given strategy cannot be found, search is repeated with MaxThroughputStrategy.\n    If there is no model match given strategy or MaxThroughputStrategy, function returns None.\n    Args:\n        package: A package object to be searched for best model.\n        strategy: Strategy for finding the best model. Defaults to `MaxThroughputAndMinLatencyStrategy`\n        include_source: Flag if Python based model has to be included in analysis\n    Returns:\n        ModelStatus of best model for given strategy or None.\n    \"\"\"\nreturn package.get_best_model_status(strategy=strategy, include_source=include_source)\n</code></pre>"},{"location":"package/#model_navigator.api.package.load","title":"<code>load(path, workspace=None)</code>","text":"<p>Load package from provided path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>The location of package to load</p> required <code>workspace</code> <code>Optional[Union[str, Path]]</code> <p>Workspace where packages will be extracted</p> <code>None</code> <p>Returns:</p> Type Description <code>Package</code> <p>Package.</p> Source code in <code>model_navigator/api/package.py</code> <pre><code>def load(\npath: Union[str, Path],\nworkspace: Optional[Union[str, Path]] = None,\n) -&gt; Package:\n\"\"\"Load package from provided path.\n    Args:\n        path: The location of package to load\n        workspace: Workspace where packages will be extracted\n    Returns:\n        Package.\n    \"\"\"\nreturn Package.load(path=path, workspace=workspace)\n</code></pre>"},{"location":"package/#model_navigator.api.package.optimize","title":"<code>optimize(package, target_formats=None, target_device=DeviceKind.CUDA, runners=None, profiler_config=None, verbose=False, debug=False, verify_func=None, custom_configs=None, defaults=True)</code>","text":"<p>Generate target formats and run correctness and profiling tests for available runners.</p> <p>Parameters:</p> Name Type Description Default <code>package</code> <code>Package</code> <p>Package to optimize.</p> required <code>target_formats</code> <code>Optional[Union[Union[str, Format], Tuple[Union[str, Format], ...]]]</code> <p>Formats to generate and profile. Defaults to target formats from the package.</p> <code>None</code> <code>target_device</code> <code>Optional[DeviceKind]</code> <p>Target device for optimize process, default is CUDA</p> <code>DeviceKind.CUDA</code> <code>runners</code> <code>Optional[Union[Union[str, Type[NavigatorRunner]], Tuple[Union[str, Type[NavigatorRunner]], ...]]]</code> <p>Runners to run correctness tests and profiling on. Defaults to runners from the package.</p> <code>None</code> <code>profiler_config</code> <code>Optional[ProfilerConfig]</code> <p>Configuration of the profiler. Defaults to config from the package.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>If True enable verbose logging. Defaults to False.</p> <code>False</code> <code>debug</code> <code>bool</code> <p>If True print debugging logs. Defaults to False.</p> <code>False</code> <code>verify_func</code> <code>Optional[VerifyFunction]</code> <p>Function used for verifying generated models. Defaults to None.</p> <code>None</code> <code>custom_configs</code> <code>Optional[List[CustomConfig]]</code> <p>Custom formats configuration. Defaults to None.</p> <code>None</code> <code>defaults</code> <code>bool</code> <p>reset configuration of custom configs to defaults</p> <code>True</code> <p>Returns:</p> Type Description <code>Package</code> <p>Optimized package</p> Source code in <code>model_navigator/api/package.py</code> <pre><code>def optimize(\npackage: Package,\ntarget_formats: Optional[Union[Union[str, Format], Tuple[Union[str, Format], ...]]] = None,\ntarget_device: Optional[DeviceKind] = DeviceKind.CUDA,\nrunners: Optional[Union[Union[str, Type[NavigatorRunner]], Tuple[Union[str, Type[NavigatorRunner]], ...]]] = None,\nprofiler_config: Optional[ProfilerConfig] = None,\nverbose: bool = False,\ndebug: bool = False,\nverify_func: Optional[VerifyFunction] = None,\ncustom_configs: Optional[List[CustomConfig]] = None,\ndefaults: bool = True,\n) -&gt; Package:\n\"\"\"Generate target formats and run correctness and profiling tests for available runners.\n    Args:\n        package: Package to optimize.\n        target_formats: Formats to generate and profile. Defaults to target formats from the package.\n        target_device: Target device for optimize process, default is CUDA\n        runners: Runners to run correctness tests and profiling on. Defaults to runners from the package.\n        profiler_config: Configuration of the profiler. Defaults to config from the package.\n        verbose: If True enable verbose logging. Defaults to False.\n        debug: If True print debugging logs. Defaults to False.\n        verify_func: Function used for verifying generated models. Defaults to None.\n        custom_configs: Custom formats configuration. Defaults to None.\n        defaults: reset configuration of custom configs to defaults\n    Returns:\n        Optimized package\n    \"\"\"\nif package.is_empty() and package.model is None:\nraise ModelNavigatorEmptyPackageError(\n\"Package is empty and source model is not loaded. Unable to run optimize.\"\n)\nconfig = package.config\nif target_formats is None:\ntarget_formats = DEFAULT_TARGET_FORMATS[package.framework]\nif package.framework == Framework.TORCH and config.batch_dim is not None:\ntarget_formats, custom_configs = update_allowed_batching_parameters(\ntarget_formats=target_formats,\ncustom_configs=custom_configs,\n)\nif runners is None:\nrunners = default_runners(device_kind=target_device)\nif profiler_config is None:\nprofiler_config = ProfilerConfig()\nis_source_available = package.model is not None\n_update_config(\nconfig=config,\nis_source_available=is_source_available,\ntarget_formats=target_formats,\nrunners=runners,\nprofiler_config=profiler_config,\nverbose=verbose,\ndebug=debug,\nverify_func=verify_func,\ncustom_configs=custom_configs,\ndefaults=defaults,\ntarget_device=target_device,\n)\nbuilders = _get_builders(\nframework=package.framework,\nrun_profiling=config.profiler_config.run_profiling,\n)\nmodel_configs = _get_model_configs(\nconfig=config,\ncustom_configs=list(config.custom_configs.values()),\n)\noptimized_package = PipelineManager.run(\npipeline_builders=builders,\nconfig=config,\nmodels_config=model_configs,\npackage=package,\n)\nreturn optimized_package\n</code></pre>"},{"location":"package/#model_navigator.api.package.save","title":"<code>save(package, path, keep_workspace=True, override=False, save_data=True)</code>","text":"<p>Save export results into the .nav package at given path.</p> <p>Parameters:</p> Name Type Description Default <code>package</code> <code>Package</code> <p>A package object to prepare the package</p> required <code>path</code> <code>Union[str, Path]</code> <p>A path to file where the package has to be saved</p> required <code>keep_workspace</code> <code>bool</code> <p>flag to remove the working directory after saving the package</p> <code>True</code> <code>override</code> <code>bool</code> <p>flag to override existing package in provided path</p> <code>False</code> <code>save_data</code> <code>bool</code> <p>disable saving samples from the dataloader</p> <code>True</code> Source code in <code>model_navigator/api/package.py</code> <pre><code>def save(\npackage: Package,\npath: Union[str, Path],\nkeep_workspace: bool = True,\noverride: bool = False,\nsave_data: bool = True,\n) -&gt; None:\n\"\"\"Save export results into the .nav package at given path.\n    Args:\n        package: A package object to prepare the package\n        path: A path to file where the package has to be saved\n        keep_workspace: flag to remove the working directory after saving the package\n        override: flag to override existing package in provided path\n        save_data: disable saving samples from the dataloader\n    \"\"\"\npackage.save(\npath=path,\nkeep_workspace=keep_workspace,\noverride=override,\nsave_data=save_data,\n)\n</code></pre>"},{"location":"package/#model_navigator.api.package.set_verified","title":"<code>set_verified(package, model_key, runner_name)</code>","text":"<p>Set verified status for model and runner.</p> <p>Parameters:</p> Name Type Description Default <code>package</code> <code>Package</code> <p>Package.</p> required <code>model_key</code> <code>str</code> <p>Unique key of the model.</p> required <code>runner_name</code> <code>str</code> <p>Name of the runner.</p> required <p>Raises:</p> Type Description <code>ModelNavigatorNotFoundError</code> <p>When model and runner not found.</p> Source code in <code>model_navigator/api/package.py</code> <pre><code>def set_verified(\npackage: Package,\nmodel_key: str,\nrunner_name: str,\n) -&gt; None:\n\"\"\"Set verified status for model and runner.\n    Args:\n        package (Package): Package.\n        model_key (str): Unique key of the model.\n        runner_name (str): Name of the runner.\n    Raises:\n        ModelNavigatorNotFoundError: When model and runner not found.\n    \"\"\"\ntry:\nrunner_results = package.status.models_status[model_key].runners_status[runner_name]\nexcept KeyError:\nraise ModelNavigatorNotFoundError(f\"Model {model_key} and runner {runner_name} not found.\")\nrunner_results.status[VerifyModel.__name__] = CommandStatus.OK\n</code></pre>"},{"location":"quick_start/","title":"Quick start","text":""},{"location":"quick_start/#quick-start","title":"Quick Start","text":"<p>The prerequisite for this section is installing the Triton Model Navigator which can be found in installation section.</p> <p>The quick start presents how to optimize Python model for deployment on Triton Inference Server. In the example we are using a simple TensorFlow 2 model.</p>"},{"location":"quick_start/#export-and-optimize-model","title":"Export and optimize model","text":"<p>To use Triton Model Navigator you must prepare model and dataloader. We recommend to create following helper functions:</p> <ul> <li><code>get_model</code> - return model object</li> <li><code>get_dataloader</code> - generate samples required for export and conversion</li> <li><code>get_verify_func</code> (optionally) - validate the correctness of models based on implemented metric</li> </ul> <p>Next you can use Triton Model Navigator <code>optimize</code> function with provided model, dataloader and verify function to export and convert model to all supported formats.</p> <p>See the below example of optimizing a simple TensorFlow model.</p> <pre><code>import logging\nimport numpy as np\nimport tensorflow as tf\nimport model_navigator as nav\n# enable tensorflow memory growth to avoid allocating all GPU memory\ngpus = tf.config.experimental.list_physical_devices(\"GPU\")\nfor gpu in gpus:\ntf.config.experimental.set_memory_growth(gpu, True)\nLOGGER = logging.getLogger(__name__)\n# dataloader is used for inference and finding input shapes of the model.\n# If you do not have dataloader, create one with samples with min and max shapes.\ndef get_dataloader():\nreturn [np.random.rand(1, 224, 224, 3).astype(\"float32\") for _ in range(10)]\ndef get_verify_function():\ndef verify_func(ys_runner, ys_expected):\nfor a, b in zip(ys_runner, ys_expected):\nif not (a[\"output__0\"] == b[\"output__0\"]).all():\nreturn False\nreturn True\nreturn verify_func\n# Model inputs must be a Tensor to support deployment on Triton Inference Server.\ndef get_model():\ninp = tf.keras.layers.Input((224, 224, 3))\nlayer_output = tf.keras.layers.Lambda(lambda x: x)(inp)\nlayer_output = tf.keras.layers.Lambda(lambda x: x)(layer_output)\nlayer_output = tf.keras.layers.Lambda(lambda x: x)(layer_output)\nlayer_output = tf.keras.layers.Lambda(lambda x: x)(layer_output)\nlayer_output = tf.keras.layers.Lambda(lambda x: x)(layer_output)\nmodel_output = tf.keras.layers.Lambda(lambda x: x)(layer_output)\nreturn tf.keras.Model(inp, model_output)\n# Check documentation for more details about Profiler Configuration options.\ndef get_profiler_config():\nreturn nav.ProfilerConfig()\nmodel = get_model()\ndataloader = get_dataloader()\nverify_func = get_verify_function()\nprofiler_config = get_profiler_config()\n# Model Navigator optimize starts export, optimization and testing process.\n# The resulting package represents all artifacts produced by Model Navigator.\npackage = nav.tensorflow.optimize(\nmodel=model,\nprofiler_config=profiler_config,\ntarget_formats=(nav.Format.ONNX,),\ndataloader=dataloader,\nverify_func=verify_func,\n)\n# Save nav package that can be used for Triton Inference Server deployment or obtaining model runner later.\n# The package contains base format checkpoints that can be used for all other conversions.\n# Models with minimal latency and maximal throughput are added to the package.\nnav.package.save(package=package, path=\"mlp.nav\")\n</code></pre> <p>You can customize behavior of export and conversion steps passing CustomConfig to <code>optimize</code> function.</p>"},{"location":"quick_start/#pytriton-deployment","title":"PyTriton deployment","text":"<p>At this point you can use NVIDIA PyTriton for easy deployment of the exported model. Below you can find an example <code>serve.py</code> that will select the best model from a previously saved <code>Navigator Package</code>, get the best runner, and use it to start <code>PyTriton</code>.</p> <pre><code>from pytriton.decorators import batch\nfrom pytriton.triton import Triton\nimport model_navigator as nav\npackage = nav.package.load(\"mlp.nav\", \"load_workspace\")\npytriton_adapter = nav.pytriton.PyTritonAdapter(package=package)\nrunner = pytriton_adapter.runner\nrunner.activate()\n@batch\ndef infer_func(**inputs):\nreturn runner.infer(inputs)\n# Connecting inference callback with Triton Inference Server\nwith Triton() as triton:\n# Load model into Triton Inference Server\ntriton.bind(\nmodel_name=\"mlp\",\ninfer_func=infer_func,\ninputs=pytriton_adapter.inputs,\noutputs=pytriton_adapter.outputs,\nconfig=pytriton_adapter.config,\n)\n# Serve model through Triton Inference Server\ntriton.serve()\n</code></pre>"},{"location":"quick_start/#triton-inference-server-deployment","title":"Triton Inference Server deployment","text":"<p>If you prefer the standalone NVIDIA Triton Inference Server you can create and use <code>model_repository</code>.</p> <pre><code>import logging\nimport pathlib\nfrom model_navigator.exceptions import ModelNavigatorEmptyPackageError, ModelNavigatorError, ModelNavigatorWrongParameterError\nimport model_navigator as nav\nLOGGER = logging.getLogger(__name__)\npackage = nav.package.load(\"mlp.nav\", \"load_workspace\")\n# Create model_repository for standalone Triton deployment\ntry:\nnav.triton.model_repository.add_model_from_package(\nmodel_repository_path=pathlib.Path(\"model_repository\"), model_name=\"dummy_model\", package=package\n)\nexcept (ModelNavigatorWrongParameterError, ModelNavigatorEmptyPackageError, ModelNavigatorError) as e:\nLOGGER.warning(f\"Model repository cannot be created.\\n{str(e)}\")\n</code></pre> <p>Use command to start server with provided <code>model_repository</code>:</p> <pre><code>$ docker run --gpus=1 --rm \\\n-p8000:8000 \\\n-p8001:8001 \\\n-p8002:8002 \\\n-v ${PWD}/model_repository:/models \\\nnvcr.io/nvidia/tritonserver:23.01-py3 \\\ntritonserver --model-repository=/models\n</code></pre>"},{"location":"support_matrix/","title":"Support matrix","text":""},{"location":"support_matrix/#support-matrix","title":"Support Matrix","text":"<p>Please find below information about tested models, used environment and libraries.</p>"},{"location":"support_matrix/#verified-models","title":"Verified Models","text":"<p>We have verified that the NVIDIA Model Navigator Optimize API works correctly for the following models.</p> Source Model NVIDIA DeepLearningExamples ResNet50 PyT NVIDIA DeepLearningExamples EfficientNet PyT NVIDIA DeepLearningExamples EfficientNet TF2 NVIDIA DeepLearningExamples BERT TF2 HuggingFace GPT2 Jax HuggingFace GPT2 PyT HuggingFace GPT2 TF2 HuggingFace DistilBERT PyT HuggingFace DistilGPT2 TF2"},{"location":"support_matrix/#third-party-packages","title":"Third-Party Packages","text":"<p>A set of component versions are imposed by the used NGC container. During testing we have used <code>23.03</code> container version that contains:</p> <ul> <li>PyTorch 2.0.0a0+1767026</li> <li>TensorFlow 2.11.0</li> <li>TensorRT 8.5.3.1</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.44.2</li> <li>GraphSurgeon: 0.3.26</li> <li> <p>tf2onnx v1.14.0</p> </li> <li> <p>Refer to the containers support matrix for a detailed summary for each version.</p> </li> </ul>"},{"location":"tensorflow/","title":"TensorFlow 2","text":""},{"location":"tensorflow/#model_navigator.api.config.TensorFlowConfig","title":"<code>model_navigator.api.config.TensorFlowConfig</code>  <code>dataclass</code>","text":"<p>         Bases: <code>CustomConfigForFormat</code></p> <p>TensorFlow custom config used for SavedModel export.</p> <p>Parameters:</p> Name Type Description Default <code>jit_compile</code> <code>Tuple[Optional[bool], ...]</code> <p>Enable or Disable jit_compile flag for tf.function wrapper for Jax infer function.</p> <code>(None)</code> <code>enable_xla</code> <code>Tuple[Optional[bool], ...]</code> <p>Enable or Disable enable_xla flag for jax2tf converter.</p> <code>(None)</code>"},{"location":"tensorflow/#model_navigator.api.config.TensorFlowConfig.format","title":"<code>format: Format</code>  <code>property</code>","text":"<p>Format represented by CustomConfig.</p> <p>Returns:</p> Type Description <code>Format</code> <p>TensorFlowConfig format</p>"},{"location":"tensorflow/#model_navigator.api.config.TensorFlowConfig.defaults","title":"<code>defaults()</code>","text":"<p>Update parameters to defaults.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def defaults(self) -&gt; None:\n\"\"\"Update parameters to defaults.\"\"\"\nself.jit_compile = (None,)\nself.enable_xla = (None,)\n</code></pre>"},{"location":"tensorflow/#model_navigator.api.config.TensorFlowConfig.name","title":"<code>name()</code>  <code>classmethod</code>","text":"<p>Name of the config.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n\"\"\"Name of the config.\"\"\"\nreturn \"TensorFlow\"\n</code></pre>"},{"location":"tensorflow/#model_navigator.api.config.TensorFlowTensorRTConfig","title":"<code>model_navigator.api.config.TensorFlowTensorRTConfig</code>  <code>dataclass</code>","text":"<p>         Bases: <code>CustomConfigForFormat</code></p> <p>TensorFlow TensorRT custom config used for TensorRT SavedModel export.</p> <p>Parameters:</p> Name Type Description Default <code>precision</code> <code>Union[Union[str, TensorRTPrecision], Tuple[Union[str, TensorRTPrecision], ...]]</code> <p>TensorRT precision.</p> <code>DEFAULT_TENSORRT_PRECISION</code> <code>max_workspace_size</code> <code>Optional[int]</code> <p>Max workspace size used by converter.</p> <code>DEFAULT_MAX_WORKSPACE_SIZE</code> <code>minimum_segment_size</code> <code>int</code> <p>Min size of subgraph.</p> <code>DEFAULT_MIN_SEGMENT_SIZE</code> <code>trt_profile</code> <code>Optional[TensorRTProfile]</code> <p>TensorRT profile.</p> <code>None</code>"},{"location":"tensorflow/#model_navigator.api.config.TensorFlowTensorRTConfig.format","title":"<code>format: Format</code>  <code>property</code>","text":"<p>Format represented by CustomConfig.</p> <p>Returns:</p> Type Description <code>Format</code> <p>TensorFlowTensorRTConfig format</p>"},{"location":"tensorflow/#model_navigator.api.config.TensorFlowTensorRTConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Parse dataclass enums.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n\"\"\"Parse dataclass enums.\"\"\"\nprecision = (self.precision,) if not isinstance(self.precision, (list, tuple)) else self.precision\nself.precision = tuple(TensorRTPrecision(p) for p in precision)\n</code></pre>"},{"location":"tensorflow/#model_navigator.api.config.TensorFlowTensorRTConfig.defaults","title":"<code>defaults()</code>","text":"<p>Update parameters to defaults.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def defaults(self) -&gt; None:\n\"\"\"Update parameters to defaults.\"\"\"\nself.precision = tuple(TensorRTPrecision(p) for p in DEFAULT_TENSORRT_PRECISION)\nself.max_workspace_size = DEFAULT_MAX_WORKSPACE_SIZE\nself.minimum_segment_size = DEFAULT_MIN_SEGMENT_SIZE\nself.trt_profile = None\n</code></pre>"},{"location":"tensorflow/#model_navigator.api.config.TensorFlowTensorRTConfig.from_dict","title":"<code>from_dict(config_dict)</code>  <code>classmethod</code>","text":"<p>Instantiate TensorFlowTensorRTConfig from  adictionary.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: Dict[str, Any]) -&gt; \"TensorFlowTensorRTConfig\":\n\"\"\"Instantiate TensorFlowTensorRTConfig from  adictionary.\"\"\"\nif config_dict.get(\"trt_profile\") is not None and not isinstance(config_dict[\"trt_profile\"], TensorRTProfile):\nconfig_dict[\"trt_profile\"] = TensorRTProfile.from_dict(config_dict[\"trt_profile\"])\nreturn cls(**config_dict)\n</code></pre>"},{"location":"tensorflow/#model_navigator.api.config.TensorFlowTensorRTConfig.name","title":"<code>name()</code>  <code>classmethod</code>","text":"<p>Name of the config.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n\"\"\"Name of the config.\"\"\"\nreturn \"TensorFlowTensorRT\"\n</code></pre>"},{"location":"tensorflow/#model_navigator.api.tensorflow","title":"<code>model_navigator.api.tensorflow</code>","text":"<p>TensorFlow optimize API.</p>"},{"location":"tensorflow/#model_navigator.api.tensorflow.optimize","title":"<code>optimize(model, dataloader, sample_count=DEFAULT_SAMPLE_COUNT, batching=True, input_names=None, output_names=None, target_formats=None, target_device=DeviceKind.CUDA, runners=None, profiler_config=None, workspace=None, verbose=False, debug=False, verify_func=None, custom_configs=None)</code>","text":"<p>Function exports TensorFlow2 model to all supported formats.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>tensorflow.keras.Model</code> <p>TensorFlow2 model object</p> required <code>dataloader</code> <code>SizedDataLoader</code> <p>Sized iterable with data that will be feed to the model</p> required <code>sample_count</code> <code>int</code> <p>Limits how many samples will be used from dataloader</p> <code>DEFAULT_SAMPLE_COUNT</code> <code>batching</code> <code>Optional[bool]</code> <p>Enable or disable batching on first (index 0) dimension of the model</p> <code>True</code> <code>input_names</code> <code>Optional[Tuple[str, ...]]</code> <p>Model input names</p> <code>None</code> <code>output_names</code> <code>Optional[Tuple[str, ...]]</code> <p>Model output names</p> <code>None</code> <code>target_formats</code> <code>Optional[Union[Union[str, Format], Tuple[Union[str, Format], ...]]]</code> <p>Target model formats for optimize process</p> <code>None</code> <code>target_device</code> <code>Optional[DeviceKind]</code> <p>Target device for optimize process, default is CUDA</p> <code>DeviceKind.CUDA</code> <code>runners</code> <code>Optional[Union[Union[str, Type[NavigatorRunner]], Tuple[Union[str, Type[NavigatorRunner]], ...]]]</code> <p>Use only runners provided as paramter</p> <code>None</code> <code>profiler_config</code> <code>Optional[ProfilerConfig]</code> <p>Profiling config</p> <code>None</code> <code>workspace</code> <code>Optional[Path]</code> <p>Workspace where packages will be extracted</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Enable verbose logging</p> <code>False</code> <code>debug</code> <code>bool</code> <p>Enable debug logging from commands</p> <code>False</code> <code>verify_func</code> <code>Optional[VerifyFunction]</code> <p>Function for additional model verifcation</p> <code>None</code> <code>custom_configs</code> <code>Optional[Sequence[CustomConfig]]</code> <p>Sequence of CustomConfigs used to control produced artifacts</p> <code>None</code> <p>Returns:</p> Type Description <code>Package</code> <p>Package descriptor representing created package.</p> Source code in <code>model_navigator/api/tensorflow.py</code> <pre><code>def optimize(\nmodel: tensorflow.keras.Model,\ndataloader: SizedDataLoader,\nsample_count: int = DEFAULT_SAMPLE_COUNT,\nbatching: Optional[bool] = True,\ninput_names: Optional[Tuple[str, ...]] = None,\noutput_names: Optional[Tuple[str, ...]] = None,\ntarget_formats: Optional[Union[Union[str, Format], Tuple[Union[str, Format], ...]]] = None,\ntarget_device: Optional[DeviceKind] = DeviceKind.CUDA,\nrunners: Optional[Union[Union[str, Type[NavigatorRunner]], Tuple[Union[str, Type[NavigatorRunner]], ...]]] = None,\nprofiler_config: Optional[ProfilerConfig] = None,\nworkspace: Optional[Path] = None,\nverbose: bool = False,\ndebug: bool = False,\nverify_func: Optional[VerifyFunction] = None,\ncustom_configs: Optional[Sequence[CustomConfig]] = None,\n) -&gt; Package:\n\"\"\"Function exports TensorFlow2 model to all supported formats.\n    Args:\n        model: TensorFlow2 model object\n        dataloader: Sized iterable with data that will be feed to the model\n        sample_count: Limits how many samples will be used from dataloader\n        batching: Enable or disable batching on first (index 0) dimension of the model\n        input_names: Model input names\n        output_names: Model output names\n        target_formats: Target model formats for optimize process\n        target_device: Target device for optimize process, default is CUDA\n        runners: Use only runners provided as paramter\n        profiler_config: Profiling config\n        workspace: Workspace where packages will be extracted\n        verbose: Enable verbose logging\n        debug: Enable debug logging from commands\n        verify_func: Function for additional model verifcation\n        custom_configs: Sequence of CustomConfigs used to control produced artifacts\n    Returns:\n        Package descriptor representing created package.\n    \"\"\"\nif target_device == DeviceKind.CPU and any(\n[device.device_type == \"GPU\" for device in tensorflow.config.get_visible_devices()]\n):\nraise ModelNavigatorConfigurationError(\n\"\\n\"\n\"    'target_device == nav.DeviceKind.CPU' is not supported for TensorFlow2 when GPU is available.\\n\"\n\"    To optimize model for CPU, disable GPU with: \"\n\"'tf.config.set_visible_devices([], 'GPU')' directly after importing TensorFlow.\\n\"\n)\nif workspace is None:\nworkspace = get_default_workspace()\nif target_formats is None:\ntarget_formats = DEFAULT_TENSORFLOW_TARGET_FORMATS\nif runners is None:\nrunners = default_runners(device_kind=target_device)\nforward_kw_names = None\nsample = next(iter(dataloader))\nif isinstance(sample, Mapping):\nforward_kw_names = tuple(sample.keys())\ntarget_formats_enums = enums.parse(target_formats, Format)\nrunner_names = enums.parse(runners, lambda runner: runner if isinstance(runner, str) else runner.name())\nif profiler_config is None:\nprofiler_config = ProfilerConfig()\nif Format.TENSORFLOW not in target_formats_enums:\ntarget_formats_enums = (Format.TENSORFLOW,) + target_formats_enums\nconfig = CommonConfig(\nFramework.TENSORFLOW,\nmodel=model,\ndataloader=dataloader,\ntarget_formats=target_formats_enums,\ntarget_device=target_device,\nworkspace=workspace,\nsample_count=sample_count,\n_input_names=input_names,\n_output_names=output_names,\nbatch_dim=0 if batching else None,\nrunner_names=runner_names,\nprofiler_config=profiler_config,\nforward_kw_names=forward_kw_names,\nverbose=verbose,\ndebug=debug,\nverify_func=verify_func,\ncustom_configs=map_custom_configs(custom_configs=custom_configs),\n)\nmodels_config = ModelConfigBuilder.generate_model_config(\nframework=Framework.TENSORFLOW,\ntarget_formats=target_formats_enums,\ncustom_configs=custom_configs,\n)\nbuilders = [\npreprocessing_builder,\ntensorflow_export_builder,\nfind_device_max_batch_size_builder,\ntensorflow_conversion_builder,\ncorrectness_builder,\n]\nif profiler_config.run_profiling:\nbuilders.append(profiling_builder)\nbuilders.append(verify_builder)\npackage = PipelineManager.run(\npipeline_builders=builders,\nconfig=config,\nmodels_config=models_config,\n)\nreturn package\n</code></pre>"},{"location":"tensorrt/","title":"TensorRT","text":""},{"location":"tensorrt/#model_navigator.api.config.TensorRTPrecision","title":"<code>model_navigator.api.config.TensorRTPrecision</code>","text":"<p>         Bases: <code>Enum</code></p> <p>Precisions supported during TensorRT conversions.</p>"},{"location":"tensorrt/#model_navigator.api.config.TensorRTPrecisionMode","title":"<code>model_navigator.api.config.TensorRTPrecisionMode</code>","text":"<p>         Bases: <code>Enum</code></p> <p>Precision modes for TensorRT conversions.</p>"},{"location":"tensorrt/#model_navigator.api.config.TensorRTConfig","title":"<code>model_navigator.api.config.TensorRTConfig</code>  <code>dataclass</code>","text":"<p>         Bases: <code>CustomConfigForFormat</code></p> <p>TensorRT custom config used for TensorRT conversion.</p> <p>Parameters:</p> Name Type Description Default <code>precision</code> <code>Union[Union[str, TensorRTPrecision], Tuple[Union[str, TensorRTPrecision], ...]]</code> <p>TensorRT precision.</p> <code>DEFAULT_TENSORRT_PRECISION</code> <code>max_workspace_size</code> <code>Optional[int]</code> <p>Max workspace size used by converter.</p> <code>DEFAULT_MAX_WORKSPACE_SIZE</code> <code>trt_profile</code> <code>Optional[TensorRTProfile]</code> <p>TensorRT profile.</p> <code>None</code>"},{"location":"tensorrt/#model_navigator.api.config.TensorRTConfig.format","title":"<code>format: Format</code>  <code>property</code>","text":"<p>Format represented by CustomConfig.</p> <p>Returns:</p> Type Description <code>Format</code> <p>TensorRTConfig format</p>"},{"location":"tensorrt/#model_navigator.api.config.TensorRTConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Parse dataclass enums.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n\"\"\"Parse dataclass enums.\"\"\"\nself.precision_mode = TensorRTPrecisionMode(self.precision_mode)\nprecision = (self.precision,) if not isinstance(self.precision, (list, tuple)) else self.precision\nself.precision = tuple(TensorRTPrecision(p) for p in precision)\n</code></pre>"},{"location":"tensorrt/#model_navigator.api.config.TensorRTConfig.defaults","title":"<code>defaults()</code>","text":"<p>Update parameters to defaults.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def defaults(self) -&gt; None:\n\"\"\"Update parameters to defaults.\"\"\"\nself.precision = tuple(TensorRTPrecision(p) for p in DEFAULT_TENSORRT_PRECISION)\nself.precision_mode = DEFAULT_TENSORRT_PRECISION_MODE\nself.trt_profile = None\nself.max_workspace_size = DEFAULT_MAX_WORKSPACE_SIZE\n</code></pre>"},{"location":"tensorrt/#model_navigator.api.config.TensorRTConfig.from_dict","title":"<code>from_dict(config_dict)</code>  <code>classmethod</code>","text":"<p>Instantiate TensorRTConfig from  adictionary.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: Dict[str, Any]) -&gt; \"TensorRTConfig\":\n\"\"\"Instantiate TensorRTConfig from  adictionary.\"\"\"\nif config_dict.get(\"trt_profile\") is not None and not isinstance(config_dict[\"trt_profile\"], TensorRTProfile):\nconfig_dict[\"trt_profile\"] = TensorRTProfile.from_dict(config_dict[\"trt_profile\"])\nreturn cls(**config_dict)\n</code></pre>"},{"location":"tensorrt/#model_navigator.api.config.TensorRTConfig.name","title":"<code>name()</code>  <code>classmethod</code>","text":"<p>Name of the config.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n\"\"\"Name of the config.\"\"\"\nreturn \"TensorRT\"\n</code></pre>"},{"location":"torch/","title":"PyTorch","text":""},{"location":"torch/#model_navigator.api.config.JitType","title":"<code>model_navigator.api.config.JitType</code>","text":"<p>         Bases: <code>Enum</code></p> <p>TorchScript export paramter.</p>"},{"location":"torch/#model_navigator.api.config.TorchConfig","title":"<code>model_navigator.api.config.TorchConfig</code>  <code>dataclass</code>","text":"<p>         Bases: <code>CustomConfigForFormat</code></p> <p>Torch custom config used for TorchScript models export.</p> <p>Parameters:</p> Name Type Description Default <code>jit_type</code> <code>Union[Union[str, JitType], Tuple[Union[str, JitType], ...]]</code> <p>Type of TorchScript export.</p> <code>(JitType.SCRIPT, JitType.TRACE)</code>"},{"location":"torch/#model_navigator.api.config.TorchConfig.format","title":"<code>format: Format</code>  <code>property</code>","text":"<p>Format represented by CustomConfig.</p> <p>Returns:</p> Type Description <code>Format</code> <p>TorchConfig format</p>"},{"location":"torch/#model_navigator.api.config.TorchConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Parse dataclass enums.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n\"\"\"Parse dataclass enums.\"\"\"\njit_type = (self.jit_type,) if not isinstance(self.jit_type, (list, tuple)) else self.jit_type\nself.jit_type = tuple(JitType(j) for j in jit_type)\n</code></pre>"},{"location":"torch/#model_navigator.api.config.TorchConfig.defaults","title":"<code>defaults()</code>","text":"<p>Update parameters to defaults.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def defaults(self) -&gt; None:\n\"\"\"Update parameters to defaults.\"\"\"\nself.jit_type = (JitType.SCRIPT, JitType.TRACE)\n</code></pre>"},{"location":"torch/#model_navigator.api.config.TorchConfig.name","title":"<code>name()</code>  <code>classmethod</code>","text":"<p>Name of the config.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n\"\"\"Name of the config.\"\"\"\nreturn \"Torch\"\n</code></pre>"},{"location":"torch/#model_navigator.api.config.TorchTensorRTConfig","title":"<code>model_navigator.api.config.TorchTensorRTConfig</code>  <code>dataclass</code>","text":"<p>         Bases: <code>CustomConfigForFormat</code></p> <p>Torch custom config used for TensorRT TorchScript conversion.</p> <p>Parameters:</p> Name Type Description Default <code>precision</code> <code>Union[Union[str, TensorRTPrecision], Tuple[Union[str, TensorRTPrecision], ...]]</code> <p>TensorRT precision.</p> <code>DEFAULT_TENSORRT_PRECISION</code> <code>max_workspace_size</code> <code>Optional[int]</code> <p>Max workspace size used by converter.</p> <code>DEFAULT_MAX_WORKSPACE_SIZE</code> <code>trt_profile</code> <code>Optional[TensorRTProfile]</code> <p>TensorRT profile.</p> <code>None</code>"},{"location":"torch/#model_navigator.api.config.TorchTensorRTConfig.format","title":"<code>format: Format</code>  <code>property</code>","text":"<p>Format represented by CustomConfig.</p> <p>Returns:</p> Type Description <code>Format</code> <p>TorchTensorRTConfig format</p>"},{"location":"torch/#model_navigator.api.config.TorchTensorRTConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Parse dataclass enums.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n\"\"\"Parse dataclass enums.\"\"\"\nprecision = (self.precision,) if not isinstance(self.precision, (list, tuple)) else self.precision\nself.precision = tuple(TensorRTPrecision(p) for p in precision)\nself.precision_mode = TensorRTPrecisionMode(self.precision_mode)\n</code></pre>"},{"location":"torch/#model_navigator.api.config.TorchTensorRTConfig.defaults","title":"<code>defaults()</code>","text":"<p>Update parameters to defaults.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def defaults(self) -&gt; None:\n\"\"\"Update parameters to defaults.\"\"\"\nself.precision = tuple(TensorRTPrecision(p) for p in DEFAULT_TENSORRT_PRECISION)\nself.precision_mode = DEFAULT_TENSORRT_PRECISION_MODE\nself.trt_profile = None\nself.max_workspace_size = DEFAULT_MAX_WORKSPACE_SIZE\n</code></pre>"},{"location":"torch/#model_navigator.api.config.TorchTensorRTConfig.from_dict","title":"<code>from_dict(config_dict)</code>  <code>classmethod</code>","text":"<p>Instantiate TorchTensorRTConfig from  adictionary.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: Dict[str, Any]) -&gt; \"TorchTensorRTConfig\":\n\"\"\"Instantiate TorchTensorRTConfig from  adictionary.\"\"\"\nif config_dict.get(\"trt_profile\") is not None and not isinstance(config_dict[\"trt_profile\"], TensorRTProfile):\nconfig_dict[\"trt_profile\"] = TensorRTProfile.from_dict(config_dict[\"trt_profile\"])\nreturn cls(**config_dict)\n</code></pre>"},{"location":"torch/#model_navigator.api.config.TorchTensorRTConfig.name","title":"<code>name()</code>  <code>classmethod</code>","text":"<p>Name of the config.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n\"\"\"Name of the config.\"\"\"\nreturn \"TorchTensorRT\"\n</code></pre>"},{"location":"torch/#model_navigator.api.torch","title":"<code>model_navigator.api.torch</code>","text":"<p>Torch optimize API.</p>"},{"location":"torch/#model_navigator.api.torch.optimize","title":"<code>optimize(model, dataloader, sample_count=DEFAULT_SAMPLE_COUNT, batching=True, input_names=None, output_names=None, target_formats=None, target_device=DeviceKind.CUDA, runners=None, profiler_config=None, workspace=None, verbose=False, debug=False, verify_func=None, custom_configs=None)</code>","text":"<p>Function exports PyTorch model to all supported formats.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>torch.nn.Module</code> <p>PyTorch model object</p> required <code>dataloader</code> <code>SizedDataLoader</code> <p>Sized iterable with data that will be feed to the model</p> required <code>sample_count</code> <code>Optional[int]</code> <p>Limits how many samples will be used from dataloader</p> <code>DEFAULT_SAMPLE_COUNT</code> <code>batching</code> <code>Optional[bool]</code> <p>Enable or disable batching on first (index 0) dimension of the model</p> <code>True</code> <code>input_names</code> <code>Optional[Tuple[str, ...]]</code> <p>Model input names</p> <code>None</code> <code>output_names</code> <code>Optional[Tuple[str, ...]]</code> <p>Model output names</p> <code>None</code> <code>target_formats</code> <code>Optional[Union[Union[str, Format], Tuple[Union[str, Format], ...]]]</code> <p>Target model formats for optimize process</p> <code>None</code> <code>target_device</code> <code>Optional[DeviceKind]</code> <p>Target device for optimize process, default is CUDA</p> <code>DeviceKind.CUDA</code> <code>runners</code> <code>Optional[Union[Union[str, Type[NavigatorRunner]], Tuple[Union[str, Type[NavigatorRunner]], ...]]]</code> <p>Use only runners provided as parameter</p> <code>None</code> <code>profiler_config</code> <code>Optional[ProfilerConfig]</code> <p>Profiling config</p> <code>None</code> <code>workspace</code> <code>Optional[Path]</code> <p>Workspace where packages will be extracted</p> <code>None</code> <code>verbose</code> <code>Optional[bool]</code> <p>Enable verbose logging</p> <code>False</code> <code>debug</code> <code>Optional[bool]</code> <p>Enable debug logging from commands</p> <code>False</code> <code>verify_func</code> <code>Optional[VerifyFunction]</code> <p>Function for additional model verification</p> <code>None</code> <code>custom_configs</code> <code>Optional[Sequence[CustomConfig]]</code> <p>Sequence of CustomConfigs used to control produced artifacts</p> <code>None</code> <p>Returns:</p> Type Description <code>Package</code> <p>Package descriptor representing created package.</p> Source code in <code>model_navigator/api/torch.py</code> <pre><code>def optimize(\nmodel: torch.nn.Module,\ndataloader: SizedDataLoader,\nsample_count: Optional[int] = DEFAULT_SAMPLE_COUNT,\nbatching: Optional[bool] = True,\ninput_names: Optional[Tuple[str, ...]] = None,\noutput_names: Optional[Tuple[str, ...]] = None,\ntarget_formats: Optional[Union[Union[str, Format], Tuple[Union[str, Format], ...]]] = None,\ntarget_device: Optional[DeviceKind] = DeviceKind.CUDA,\nrunners: Optional[Union[Union[str, Type[NavigatorRunner]], Tuple[Union[str, Type[NavigatorRunner]], ...]]] = None,\nprofiler_config: Optional[ProfilerConfig] = None,\nworkspace: Optional[Path] = None,\nverbose: Optional[bool] = False,\ndebug: Optional[bool] = False,\nverify_func: Optional[VerifyFunction] = None,\ncustom_configs: Optional[Sequence[CustomConfig]] = None,\n) -&gt; Package:\n\"\"\"Function exports PyTorch model to all supported formats.\n    Args:\n        model: PyTorch model object\n        dataloader: Sized iterable with data that will be feed to the model\n        sample_count: Limits how many samples will be used from dataloader\n        batching: Enable or disable batching on first (index 0) dimension of the model\n        input_names: Model input names\n        output_names: Model output names\n        target_formats: Target model formats for optimize process\n        target_device: Target device for optimize process, default is CUDA\n        runners: Use only runners provided as parameter\n        profiler_config: Profiling config\n        workspace: Workspace where packages will be extracted\n        verbose: Enable verbose logging\n        debug: Enable debug logging from commands\n        verify_func: Function for additional model verification\n        custom_configs: Sequence of CustomConfigs used to control produced artifacts\n    Returns:\n        Package descriptor representing created package.\n    \"\"\"\nif workspace is None:\nworkspace = get_default_workspace()\nif target_formats is None:\ntarget_formats = DEFAULT_TORCH_TARGET_FORMATS\nif batching:\ntarget_formats, custom_configs = update_allowed_batching_parameters(\ntarget_formats=target_formats,\ncustom_configs=custom_configs,\n)\nLOGGER.info(f\"Using default target formats: {[tf.name for tf in target_formats]}\")\nsample = next(iter(dataloader))\nif isinstance(sample, Mapping):\nforward_kw_names = tuple(sample.keys())\nelse:\nforward_kw_names = None\nif runners is None:\nrunners = default_runners(device_kind=target_device)\ntarget_formats = enums.parse(target_formats, Format)\nrunner_names = enums.parse(runners, lambda runner: runner if isinstance(runner, str) else runner.name())\nif profiler_config is None:\nprofiler_config = ProfilerConfig()\nif Format.TORCH not in target_formats:\ntarget_formats = (Format.TORCH,) + target_formats\nconfig = CommonConfig(\nframework=Framework.TORCH,\nmodel=model,\ndataloader=dataloader,\ntarget_formats=target_formats,\nworkspace=workspace,\nsample_count=sample_count,\n_input_names=input_names,\n_output_names=output_names,\ntarget_device=target_device,\nforward_kw_names=forward_kw_names,\nbatch_dim=0 if batching else None,\nrunner_names=runner_names,\nprofiler_config=profiler_config,\nverbose=verbose,\ndebug=debug,\nverify_func=verify_func,\ncustom_configs=map_custom_configs(custom_configs=custom_configs),\n)\nmodels_config = ModelConfigBuilder.generate_model_config(\nframework=Framework.TORCH,\ntarget_formats=target_formats,\ncustom_configs=custom_configs,\n)\nbuilders = [\npreprocessing_builder,\ntorch_export_builder,\nfind_device_max_batch_size_builder,\ntorch_conversion_builder,\ncorrectness_builder,\n]\nif profiler_config.run_profiling:\nbuilders.append(profiling_builder)\nbuilders.append(verify_builder)\npackage = PipelineManager.run(\npipeline_builders=builders,\nconfig=config,\nmodels_config=models_config,\n)\nreturn package\n</code></pre>"},{"location":"triton/accelerators/","title":"Accelerators","text":""},{"location":"triton/accelerators/#accelerators","title":"Accelerators","text":""},{"location":"triton/accelerators/#model_navigator.api.triton.AutoMixedPrecisionAccelerator","title":"<code>model_navigator.api.triton.AutoMixedPrecisionAccelerator</code>  <code>dataclass</code>","text":"<p>Auto-mixed-precision accelerator for TensorFlow. Enable automatic FP16 precision.</p> <p>Currently empty - no arguments required.</p>"},{"location":"triton/accelerators/#model_navigator.api.triton.GPUIOAccelerator","title":"<code>model_navigator.api.triton.GPUIOAccelerator</code>  <code>dataclass</code>","text":"<p>GPU IO accelerator for TensorFlow.</p> <p>Currently empty - no arguments required.</p>"},{"location":"triton/accelerators/#model_navigator.api.triton.OpenVINOAccelerator","title":"<code>model_navigator.api.triton.OpenVINOAccelerator</code>  <code>dataclass</code>","text":"<p>OpenVINO optimization.</p> <p>Currently empty - no arguments required.</p>"},{"location":"triton/accelerators/#model_navigator.api.triton.OpenVINOAccelerator","title":"<code>model_navigator.api.triton.OpenVINOAccelerator</code>  <code>dataclass</code>","text":"<p>OpenVINO optimization.</p> <p>Currently empty - no arguments required.</p>"},{"location":"triton/accelerators/#model_navigator.api.triton.TensorRTAccelerator","title":"<code>model_navigator.api.triton.TensorRTAccelerator</code>  <code>dataclass</code>","text":"<p>TensorRT accelerator configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> Name Type Description Default <code>precision</code> <code>TensorRTOptPrecision</code> <p>The precision used for optimization</p> <code>TensorRTOptPrecision.FP32</code> <code>max_workspace_size</code> <code>Optional[int]</code> <p>The maximum GPU memory the model can use temporarily during execution</p> <code>None</code> <code>max_cached_engines</code> <code>Optional[int]</code> <p>The maximum number of cached TensorRT engines in dynamic TensorRT ops</p> <code>None</code> <code>minimum_segment_size</code> <code>Optional[int]</code> <p>The smallest model subgraph that will be considered for optimization by TensorRT</p> <code>None</code>"},{"location":"triton/accelerators/#model_navigator.api.triton.TensorRTOptPrecision","title":"<code>model_navigator.api.triton.TensorRTOptPrecision</code>","text":"<p>         Bases: <code>enum.Enum</code></p> <p>TensorRT optimization allowed precision.</p> <p>Parameters:</p> Name Type Description Default <code>FP16</code> <p>fp16 precision</p> required <code>FP32</code> <p>fp32 precision</p> required"},{"location":"triton/dynamic_batcher/","title":"Dynamic Batcher","text":""},{"location":"triton/dynamic_batcher/#dynamic-batcher","title":"Dynamic Batcher","text":""},{"location":"triton/dynamic_batcher/#model_navigator.api.triton.DynamicBatcher","title":"<code>model_navigator.api.triton.DynamicBatcher</code>  <code>dataclass</code>","text":"<p>Dynamic batching configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> Name Type Description Default <code>max_queue_delay_microseconds</code> <code>int</code> <p>The maximum time, in microseconds, a request will be delayed in                           the scheduling queue to wait for additional requests for batching.</p> <code>0</code> <code>preferred_batch_size</code> <code>Optional[list]</code> <p>Preferred batch sizes for dynamic batching.</p> <code>None</code> <code>preserve_ordering</code> <p>Should the dynamic batcher preserve the ordering of responses to                 match the order of requests received by the scheduler.</p> <code>False</code> <code>priority_levels</code> <code>int</code> <p>The number of priority levels to be enabled for the model.</p> <code>0</code> <code>default_priority_level</code> <code>int</code> <p>The priority level used for requests that don't specify their priority.</p> <code>0</code> <code>default_queue_policy</code> <code>Optional[QueuePolicy]</code> <p>The default queue policy used for requests.</p> <code>None</code> <code>priority_queue_policy</code> <code>Optional[Dict[int, QueuePolicy]]</code> <p>Specify the queue policy for the priority level.</p> <code>None</code>"},{"location":"triton/dynamic_batcher/#model_navigator.triton.specialized_configs.common.DynamicBatcher.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/common.py</code> <pre><code>def __post_init__(self):\n\"\"\"Validate the configuration for early error handling.\"\"\"\nif self.default_priority_level &gt; self.priority_levels:\nraise ModelNavigatorWrongParameterError(\n\"The `default_priority_level` must be between 1 and \" f\"{self.priority_levels}.\"\n)\nif self.priority_queue_policy:\nif not self.priority_levels:\nraise ModelNavigatorWrongParameterError(\n\"Provide the `priority_levels` if you want to define `priority_queue_policy` \"\n\"for Dynamic Batching.\"\n)\nfor priority in self.priority_queue_policy.keys():\nif priority &lt; 0 or priority &gt; self.priority_levels:\nraise ModelNavigatorWrongParameterError(\nf\"Invalid `priority`={priority} provided. The value must be between \"\nf\"1 and {self.priority_levels}.\"\n)\n</code></pre>"},{"location":"triton/dynamic_batcher/#model_navigator.api.triton.QueuePolicy","title":"<code>model_navigator.api.triton.QueuePolicy</code>  <code>dataclass</code>","text":"<p>Model queue policy configuration.</p> <p>Used for <code>default_queue_policy</code> and <code>priority_queue_policy</code> fields in DynamicBatcher configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> Name Type Description Default <code>timeout_action</code> <code>TimeoutAction</code> <p>The action applied to timed-out request.</p> <code>TimeoutAction.REJECT</code> <code>default_timeout_microseconds</code> <code>int</code> <p>The default timeout for every request, in microseconds.</p> <code>0</code> <code>allow_timeout_override</code> <code>bool</code> <p>Whether individual request can override the default timeout value.</p> <code>False</code> <code>max_queue_size</code> <code>int</code> <p>The maximum queue size for holding requests.</p> <code>0</code>"},{"location":"triton/dynamic_batcher/#model_navigator.api.triton.TimeoutAction","title":"<code>model_navigator.api.triton.TimeoutAction</code>","text":"<p>         Bases: <code>enum.Enum</code></p> <p>Timeout action definition for timeout_action QueuePolicy field.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> Name Type Description Default <code>REJECT</code> <p>\"REJECT\"</p> required <code>DELAY</code> <p>\"DELAY\"</p> required"},{"location":"triton/inputs_and_outputs/","title":"Inputs and Outputs","text":""},{"location":"triton/inputs_and_outputs/#model-inputs-and-outputs","title":"Model Inputs and Outputs","text":""},{"location":"triton/inputs_and_outputs/#model_navigator.api.triton.InputTensorSpec","title":"<code>model_navigator.api.triton.InputTensorSpec</code>  <code>dataclass</code>","text":"<p>         Bases: <code>BaseTensorSpec</code></p> <p>Stores specification of single input tensor.</p> <p>This includes name, shape, dtype and more parameters available for input tensor in Triton Inference Server:</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> Name Type Description Default <code>optional</code> <code>bool</code> <p>Flag marking the input is optional for the model execution</p> <code>False</code> <code>format</code> <code>Optional[InputTensorFormat]</code> <p>The format of the input.</p> <code>None</code> <code>allow_ragged_batch</code> <code>bool</code> <p>Flag marking the input is allowed to be \"ragged\" in a dynamically created batch.</p> <code>False</code>"},{"location":"triton/inputs_and_outputs/#model_navigator.api.triton.InputTensorFormat","title":"<code>model_navigator.api.triton.InputTensorFormat</code>","text":"<p>         Bases: <code>enum.Enum</code></p> <p>Format for input tensor.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> Name Type Description Default <code>FORMAT_NONE</code> <p>0</p> required <code>FORMAT_NHWC</code> <p>1</p> required <code>FORMAT_NCHW</code> <p>2</p> required"},{"location":"triton/inputs_and_outputs/#model_navigator.api.triton.OutputTensorSpec","title":"<code>model_navigator.api.triton.OutputTensorSpec</code>  <code>dataclass</code>","text":"<p>         Bases: <code>BaseTensorSpec</code></p> <p>Stores specification of single output tensor.</p> <p>This includes name, shape, dtype and more parameters available for output tensor in Triton Inference Server:</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> Name Type Description Default <code>label_filename</code> <code>Optional[str]</code> <p>The label file associated with this output.</p> <code>None</code>"},{"location":"triton/instance_groups/","title":"Instance Group","text":""},{"location":"triton/instance_groups/#model-instance-group","title":"Model Instance Group","text":""},{"location":"triton/instance_groups/#model_navigator.api.triton.InstanceGroup","title":"<code>model_navigator.api.triton.InstanceGroup</code>  <code>dataclass</code>","text":"<p>Configuration for model instance group.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> Name Type Description Default <code>kind</code> <code>Optional[DeviceKind]</code> <p>Kind of this instance group.</p> <code>None</code> <code>count</code> <code>Optional[int]</code> <p>For a group assigned to GPU, the number of instances created for    each GPU listed in 'gpus'. For a group assigned to CPU the number    of instances created.</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>Optional name of this group of instances.</p> <code>None</code> <code>gpus</code> <code>List[int]</code> <p>GPU(s) where instances should be available.</p> <code>dataclasses.field(default_factory=lambda : [])</code> <code>passive</code> <code>bool</code> <p>Whether the instances within this instance group will be accepting      inference requests from the scheduler.</p> <code>False</code> <code>host_policy</code> <code>Optional[str]</code> <p>The host policy name that the instance to be associated with.</p> <code>None</code> <code>profile</code> <code>List[str]</code> <p>For TensorRT models containing multiple optimization profile, this      parameter specifies a set of optimization profiles available to this      instance group.</p> <code>dataclasses.field(default_factory=lambda : [])</code>"},{"location":"triton/instance_groups/#model_navigator.triton.specialized_configs.common.InstanceGroup.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/common.py</code> <pre><code>def __post_init__(self) -&gt; None:\n\"\"\"Validate the configuration for early error handling.\"\"\"\nif self.count is not None and self.count &lt; 1:\nraise ModelNavigatorWrongParameterError(\"The `count` must be greater or equal 1.\")\nif self.kind not in [None, DeviceKind.KIND_GPU, DeviceKind.KIND_AUTO] and len(self.gpus) &gt; 0:\nraise ModelNavigatorWrongParameterError(\nf\"`gpus` cannot be set when device is not {DeviceKind.KIND_GPU} or {DeviceKind.KIND_AUTO}\"\n)\n</code></pre>"},{"location":"triton/instance_groups/#model_navigator.api.triton.DeviceKind","title":"<code>model_navigator.api.triton.DeviceKind</code>","text":"<p>         Bases: <code>enum.Enum</code></p> <p>Device kind for model deployment.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> Name Type Description Default <code>KIND_AUTO</code> <p>\"KIND_AUTO\"</p> required <code>KIND_CPU</code> <p>\"KIND_CPU\"</p> required <code>KIND_GPU</code> <p>\"KIND_GPU\"</p> required"},{"location":"triton/model_store_api/","title":"Python API","text":""},{"location":"triton/model_store_api/#triton-model-store-api","title":"Triton Model Store API","text":""},{"location":"triton/model_store_api/#model_navigator.triton.model_repository.add_model","title":"<code>model_navigator.triton.model_repository.add_model(model_repository_path, model_name, model_path, config, model_version=1)</code>","text":"<p>Generate model deployment inside provided model store path.</p> <p>The config requires specialized configuration to be passed for backend on which model is executed. Example: - ONNX model requires ONNXModelConfig - TensorRT model requires TensorRTModelConfig - TorchScript or Torch-TensorRT models requires PyTorchModelConfig - TensorFlow SavedModel or TensorFlow-TensorRT models requires TensorFlowModelConfig - Python model requires PythonModelConfig</p> <p>Parameters:</p> Name Type Description Default <code>model_repository_path</code> <code>Union[str, pathlib.Path]</code> <p>Path where deployment should be created</p> required <code>model_name</code> <code>str</code> <p>Name under which model is deployed in Triton Inference Server</p> required <code>model_path</code> <code>pathlib.Path</code> <p>Path to model</p> required <code>config</code> <code>Union[ONNXModelConfig, TensorRTModelConfig, PyTorchModelConfig, PythonModelConfig, TensorFlowModelConfig]</code> <p>Specialized configuration of model for backend on which model is executed</p> required <code>model_version</code> <code>int</code> <p>Version of model that is deployed</p> <code>1</code> <p>Returns:</p> Type Description <code>pathlib.Path</code> <p>Path to created model store</p> Source code in <code>model_navigator/triton/model_repository.py</code> <pre><code>def add_model(\nmodel_repository_path: Union[str, pathlib.Path],\nmodel_name: str,\nmodel_path: pathlib.Path,\nconfig: Union[\nONNXModelConfig,\nTensorRTModelConfig,\nPyTorchModelConfig,\nPythonModelConfig,\nTensorFlowModelConfig,\n],\nmodel_version: int = 1,\n) -&gt; pathlib.Path:\n\"\"\"Generate model deployment inside provided model store path.\n    The config requires specialized configuration to be passed for backend on which model is executed. Example:\n    - ONNX model requires ONNXModelConfig\n    - TensorRT model requires TensorRTModelConfig\n    - TorchScript or Torch-TensorRT models requires PyTorchModelConfig\n    - TensorFlow SavedModel or TensorFlow-TensorRT models requires TensorFlowModelConfig\n    - Python model requires PythonModelConfig\n    Args:\n        model_repository_path: Path where deployment should be created\n        model_name: Name under which model is deployed in Triton Inference Server\n        model_path: Path to model\n        config: Specialized configuration of model for backend on which model is executed\n        model_version: Version of model that is deployed\n    Returns:\n         Path to created model store\n    \"\"\"\nif isinstance(config, ONNXModelConfig):\nmodel_config = ModelConfigBuilder.from_onnx_config(\nmodel_name=model_name,\nmodel_version=model_version,\nonnx_config=config,\n)\nelif isinstance(config, TensorFlowModelConfig):\nmodel_config = ModelConfigBuilder.from_tensorflow_config(\nmodel_name=model_name,\nmodel_version=model_version,\ntensorflow_config=config,\n)\nelif isinstance(config, PythonModelConfig):\nmodel_config = ModelConfigBuilder.from_python_config(\nmodel_name=model_name,\nmodel_version=model_version,\npython_config=config,\n)\nelif isinstance(config, PyTorchModelConfig):\nmodel_config = ModelConfigBuilder.from_pytorch_config(\nmodel_name=model_name,\nmodel_version=model_version,\npytorch_config=config,\n)\nelif isinstance(config, TensorRTModelConfig):\nmodel_config = ModelConfigBuilder.from_tensorrt_config(\nmodel_name=model_name,\nmodel_version=model_version,\ntensorrt_config=config,\n)\nelse:\nraise ModelNavigatorWrongParameterError(f\"Unsupported model config provided: {config.__class__}\")\ntriton_model_repository = _TritonModelRepository(model_repository_path=model_repository_path)\nreturn triton_model_repository.deploy_model(\nmodel_path=model_path,\nmodel_config=model_config,\n)\n</code></pre>"},{"location":"triton/model_store_api/#model_navigator.triton.model_repository.add_model_from_package","title":"<code>model_navigator.triton.model_repository.add_model_from_package(model_repository_path, model_name, package, model_version=1, strategy=None, response_cache=False)</code>","text":"<p>Create the Triton Model Store with optimized model and save it to <code>model_repository_path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_repository_path</code> <code>pathlib.Path</code> <p>Path where the model store is located</p> required <code>model_name</code> <code>str</code> <p>Name under which model is deployed in Triton Inference Server</p> required <code>model_version</code> <code>int</code> <p>Version of model that is deployed</p> <code>1</code> <code>package</code> <code>Package</code> <p>Package for which model store is created</p> required <code>strategy</code> <code>Optional[RuntimeSearchStrategy]</code> <p>Strategy for finding the best runtime.       When not set the <code>MaxThroughputAndMinLatencyStrategy</code> is used.</p> <code>None</code> <code>response_cache</code> <code>bool</code> <p>Enable response cache for model</p> <code>False</code> <p>Returns:</p> Type Description <p>Path to created model store</p> Source code in <code>model_navigator/triton/model_repository.py</code> <pre><code>def add_model_from_package(\nmodel_repository_path: pathlib.Path,\nmodel_name: str,\npackage: Package,\nmodel_version: int = 1,\nstrategy: Optional[RuntimeSearchStrategy] = None,\nresponse_cache: bool = False,\n):\n\"\"\"Create the Triton Model Store with optimized model and save it to `model_repository_path`.\n    Args:\n        model_repository_path: Path where the model store is located\n        model_name: Name under which model is deployed in Triton Inference Server\n        model_version: Version of model that is deployed\n        package: Package for which model store is created\n        strategy: Strategy for finding the best runtime.\n                  When not set the `MaxThroughputAndMinLatencyStrategy` is used.\n        response_cache: Enable response cache for model\n    Returns:\n        Path to created model store\n    \"\"\"\nif package.is_empty():\nraise ModelNavigatorEmptyPackageError(\"No models available in the package. Triton deployment is not possible.\")\nif package.config.batch_dim not in [0, None]:\nraise ModelNavigatorWrongParameterError(\n\"Only models without batching or batch dimension on first place in shape are supported for Triton.\"\n)\nif strategy is None:\nstrategy = MaxThroughputAndMinLatencyStrategy()\nbatching = package.config.batch_dim == 0\nruntime_result = RuntimeAnalyzer.get_runtime(\nmodels_status=package.status.models_status,\nstrategy=strategy,\nformats=[fmt.value for fmt in TRITON_FORMATS],\nrunners=[runner.name() for runner in TRITON_RUNNERS],\n)\nmax_batch_size = max(\nprofiling_results.batch_size\nfor profiling_results in runtime_result.runner_status.result[Performance.name()][\"profiling_results\"]\n)\nif runtime_result.model_status.model_config.format == Format.ONNX:\nconfig = _onnx_config_from_runtime_result(\nbatching=batching,\nmax_batch_size=max_batch_size,\nresponse_cache=response_cache,\nruntime_result=runtime_result,\n)\nelif runtime_result.model_status.model_config.format in [Format.TF_SAVEDMODEL, Format.TF_TRT]:\nconfig = _tensorflow_config_from_runtime_result(\nbatching=batching,\nmax_batch_size=max_batch_size,\nresponse_cache=response_cache,\nruntime_result=runtime_result,\n)\nelif runtime_result.model_status.model_config.format in [Format.TORCHSCRIPT, Format.TORCH_TRT]:\ninputs = input_tensor_from_metadata(\npackage.status.input_metadata,\nbatching=batching,\n)\noutputs = output_tensor_from_metadata(\npackage.status.output_metadata,\nbatching=batching,\n)\nconfig = _pytorch_config_from_runtime_result(\nbatching=batching,\nmax_batch_size=max_batch_size,\ninputs=inputs,\noutputs=outputs,\nresponse_cache=response_cache,\nruntime_result=runtime_result,\n)\nelif runtime_result.model_status.model_config.format == Format.TENSORRT:\nconfig = _tensorrt_config_from_runtime_result(\nbatching=batching,\nmax_batch_size=max_batch_size,\nresponse_cache=response_cache,\n)\nelse:\nraise ModelNavigatorError(\nf\"Unsupported model format selected: {runtime_result.model_status.model_config.format}\"\n)\nreturn add_model(\nmodel_repository_path=model_repository_path,\nmodel_name=model_name,\nmodel_version=model_version,\nmodel_path=package.workspace / runtime_result.model_status.model_config.path,\nconfig=config,\n)\n</code></pre>"},{"location":"triton/sequence_batcher/","title":"Sequence Batcher","text":""},{"location":"triton/sequence_batcher/#sequence-batcher","title":"Sequence Batcher","text":""},{"location":"triton/sequence_batcher/#model_navigator.api.triton.SequenceBatcher","title":"<code>model_navigator.api.triton.SequenceBatcher</code>  <code>dataclass</code>","text":"<p>Sequence batching configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> Name Type Description Default <code>strategy</code> <code>Optional[Union[SequenceBatcherStrategyDirect, SequenceBatcherStrategyOldest]]</code> <p>The strategy used by the sequence batcher.</p> <code>None</code> <code>max_sequence_idle_microseconds</code> <code>Optional[int]</code> <p>The maximum time, in microseconds, that a sequence is allowed to                             be idle before it is aborted.</p> <code>None</code> <code>control_inputs</code> <code>List[SequenceBatcherControlInput]</code> <p>The model input(s) that the server should use to communicate             sequence start, stop, ready and similar control values to the model.</p> <code>dataclasses.field(default_factory=lambda : [])</code> <code>states</code> <code>List[SequenceBatcherState]</code> <p>The optional state that can be stored in Triton for performing     inference requests on a sequence.</p> <code>dataclasses.field(default_factory=lambda : [])</code>"},{"location":"triton/sequence_batcher/#model_navigator.triton.specialized_configs.common.SequenceBatcher.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/common.py</code> <pre><code>def __post_init__(self):\n\"\"\"Validate the configuration for early error handling.\"\"\"\nif self.strategy and (\nnot isinstance(self.strategy, SequenceBatcherStrategyDirect)\nand not isinstance(self.strategy, SequenceBatcherStrategyOldest)\n):\nraise ModelNavigatorWrongParameterError(\"Unsupported strategy type provided.\")\n</code></pre>"},{"location":"triton/sequence_batcher/#model_navigator.api.triton.SequenceBatcherControl","title":"<code>model_navigator.api.triton.SequenceBatcherControl</code>  <code>dataclass</code>","text":"<p>Sequence Batching control configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> Name Type Description Default <code>kind</code> <code>SequenceBatcherControlKind</code> <p>The kind of this control.</p> required <code>dtype</code> <code>Optional[Union[np.dtype, Type[np.dtype]]]</code> <p>The control's datatype.</p> <code>None</code> <code>int32_false_true</code> <code>List[int]</code> <p>The control's true and false setting is indicated by setting               a value in an int32 tensor.</p> <code>dataclasses.field(default_factory=lambda : [])</code> <code>fp32_false_true</code> <code>List[float]</code> <p>The control's true and false setting is indicated by setting              a value in a fp32 tensor.</p> <code>dataclasses.field(default_factory=lambda : [])</code> <code>bool_false_true</code> <code>List[bool]</code> <p>The control's true and false setting is indicated by setting              a value in a bool tensor.</p> <code>dataclasses.field(default_factory=lambda : [])</code>"},{"location":"triton/sequence_batcher/#model_navigator.triton.specialized_configs.common.SequenceBatcherControl.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/common.py</code> <pre><code>def __post_init__(self):\n\"\"\"Validate the configuration for early error handling.\"\"\"\nif self.kind == SequenceBatcherControlKind.CONTROL_SEQUENCE_CORRID and self.dtype is None:\nraise ModelNavigatorWrongParameterError(f\"The {self.kind} control type requires `dtype` to be specified.\")\nif self.kind == SequenceBatcherControlKind.CONTROL_SEQUENCE_CORRID and any(\n[self.int32_false_true, self.fp32_false_true, self.bool_false_true]\n):\nraise ModelNavigatorWrongParameterError(\nf\"The {self.kind} control type requires `dtype` to be specified only.\"\n)\ncontrols = [\nSequenceBatcherControlKind.CONTROL_SEQUENCE_START,\nSequenceBatcherControlKind.CONTROL_SEQUENCE_END,\nSequenceBatcherControlKind.CONTROL_SEQUENCE_READY,\n]\nif self.kind in controls and self.dtype:\nraise ModelNavigatorWrongParameterError(f\"The {self.kind} control does not support `dtype` parameter.\")\nif self.kind in controls and not (self.int32_false_true or self.fp32_false_true or self.bool_false_true):\nraise ModelNavigatorWrongParameterError(\nf\"The {self.kind} control type requires one of: \"\n\"`int32_false_true`, `fp32_false_true`, `bool_false_true` to be specified.\"\n)\nif self.int32_false_true and len(self.int32_false_true) != 2:\nraise ModelNavigatorWrongParameterError(\n\"The `int32_false_true` field should be two element list with false and true values. Example: [0 , 1]\"\n)\nif self.fp32_false_true and len(self.fp32_false_true) != 2:\nraise ModelNavigatorWrongParameterError(\n\"The `fp32_false_true` field should be two element list with false and true values. Example: [0 , 1]\"\n)\nif self.bool_false_true and len(self.bool_false_true) != 2:\nraise ModelNavigatorWrongParameterError(\n\"The `bool_false_true` field should be two element list with false and true values. \"\n\"Example: [False, True]\"\n)\nif self.dtype:\nself.dtype = cast_dtype(dtype=self.dtype)\nexpect_type(\"dtype\", self.dtype, np.dtype, optional=True)\n</code></pre>"},{"location":"triton/sequence_batcher/#model_navigator.api.triton.SequenceBatcherControlInput","title":"<code>model_navigator.api.triton.SequenceBatcherControlInput</code>  <code>dataclass</code>","text":"<p>Sequence Batching control input configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> Name Type Description Default <code>input_name</code> <code>str</code> <p>The name of the model input.</p> required <code>controls</code> <code>List[SequenceBatcherControl]</code> <p>List of  control value(s) that should be communicated to the       model using this model input.</p> required"},{"location":"triton/sequence_batcher/#model_navigator.api.triton.SequenceBatcherControlKind","title":"<code>model_navigator.api.triton.SequenceBatcherControlKind</code>","text":"<p>         Bases: <code>enum.Enum</code></p> <p>Sequence Batching control options.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> Name Type Description Default <code>CONTROL_SEQUENCE_START</code> <p>\"CONTROL_SEQUENCE_START\"</p> required <code>CONTROL_SEQUENCE_READY</code> <p>\"CONTROL_SEQUENCE_READY\"</p> required <code>CONTROL_SEQUENCE_END</code> <p>\"CONTROL_SEQUENCE_END\"</p> required <code>CONTROL_SEQUENCE_CORRID</code> <p>\"CONTROL_SEQUENCE_CORRID\"</p> required"},{"location":"triton/sequence_batcher/#model_navigator.api.triton.SequenceBatcherInitialState","title":"<code>model_navigator.api.triton.SequenceBatcherInitialState</code>  <code>dataclass</code>","text":"<p>Sequence Batching initial state configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> required <code>shape</code> <code>Tuple[int, ...]</code> <p>The shape of the state tensor, not including the batch dimension.</p> required <code>dtype</code> <code>Optional[Union[np.dtype, Type[np.dtype]]]</code> <p>The data-type of the state.</p> <code>None</code> <code>zero_data</code> <code>Optional[bool]</code> <p>The identifier for using zeros as initial state data.</p> <code>None</code> <code>data_file</code> <code>Optional[str]</code> <p>The file whose content will be used as the initial data for        the state in row-major order.</p> <code>None</code>"},{"location":"triton/sequence_batcher/#model_navigator.triton.specialized_configs.common.SequenceBatcherInitialState.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/common.py</code> <pre><code>def __post_init__(self):\n\"\"\"Validate the configuration for early error handling.\"\"\"\nif not self.zero_data and not self.data_file:\nraise ModelNavigatorWrongParameterError(\"zero_data or data_file has to be defined. None was provided.\")\nif self.zero_data and self.data_file:\nraise ModelNavigatorWrongParameterError(\"zero_data or data_file has to be defined. Both were provided.\")\nif self.dtype:\nself.dtype = cast_dtype(dtype=self.dtype)\nexpect_type(\"name\", self.name, str)\nexpect_type(\"shape\", self.shape, tuple)\nexpect_type(\"dtype\", self.dtype, np.dtype, optional=True)\nis_shape_correct(\"shape\", self.shape)\n</code></pre>"},{"location":"triton/sequence_batcher/#model_navigator.api.triton.SequenceBatcherState","title":"<code>model_navigator.api.triton.SequenceBatcherState</code>  <code>dataclass</code>","text":"<p>Sequence Batching state configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> Name Type Description Default <code>input_name</code> <code>str</code> <p>The name of the model state input.</p> required <code>output_name</code> <code>str</code> <p>The name of the model state output.</p> required <code>dtype</code> <code>Union[np.dtype, Type[np.dtype]]</code> <p>The data-type of the state.</p> required <code>shape</code> <code>Tuple[int, ...]</code> <p>The shape of the state tensor.</p> required <code>initial_states</code> <code>List[SequenceBatcherInitialState]</code> <p>The optional field to specify the list of initial states for the model.</p> <code>dataclasses.field(default_factory=lambda : [])</code>"},{"location":"triton/sequence_batcher/#model_navigator.triton.specialized_configs.common.SequenceBatcherState.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/common.py</code> <pre><code>def __post_init__(self):\n\"\"\"Validate the configuration for early error handling.\"\"\"\nself.dtype = cast_dtype(dtype=self.dtype)\nexpect_type(\"shape\", self.shape, tuple)\nexpect_type(\"dtype\", self.dtype, np.dtype, optional=True)\nis_shape_correct(\"shape\", self.shape)\n</code></pre>"},{"location":"triton/sequence_batcher/#model_navigator.api.triton.SequenceBatcherStrategyDirect","title":"<code>model_navigator.api.triton.SequenceBatcherStrategyDirect</code>  <code>dataclass</code>","text":"<p>Sequence Batching strategy direct configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> Name Type Description Default <code>max_queue_delay_microseconds</code> <code>int</code> <p>The maximum time, in microseconds, a candidate request                           will be delayed in the sequence batch scheduling queue to                           wait for additional requests for batching.</p> <code>0</code> <code>minimum_slot_utilization</code> <code>float</code> <p>The minimum slot utilization that must be satisfied to                       execute the batch before 'max_queue_delay_microseconds' expires.</p> <code>0.0</code>"},{"location":"triton/sequence_batcher/#model_navigator.api.triton.SequenceBatcherStrategyOldest","title":"<code>model_navigator.api.triton.SequenceBatcherStrategyOldest</code>  <code>dataclass</code>","text":"<p>Sequence Batching strategy oldest configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> Name Type Description Default <code>max_candidate_sequences</code> <code>int</code> <p>Maximum number of candidate sequences that the batcher maintains.</p> required <code>preferred_batch_size</code> <code>List[int]</code> <p>Preferred batch sizes for dynamic batching of candidate sequences.</p> <code>dataclasses.field(default_factory=lambda : [])</code> <code>max_queue_delay_microseconds</code> <code>int</code> <p>The maximum time, in microseconds, a candidate request                           will be delayed in the dynamic batch scheduling queue to                           wait for additional requests for batching.</p> <code>0</code>"},{"location":"triton/specialized_configs/","title":"Specialized Configs","text":""},{"location":"triton/specialized_configs/#specialized-configs-for-triton-backends","title":"Specialized Configs for Triton Backends","text":"<p>The Python API provides specialized configuration classes that help provide only available options for the given type of model.</p>"},{"location":"triton/specialized_configs/#model_navigator.api.triton.ONNXModelConfig","title":"<code>model_navigator.api.triton.ONNXModelConfig</code>  <code>dataclass</code>","text":"<p>         Bases: <code>BaseSpecializedModelConfig</code></p> <p>Specialized model config for ONNX backend supported model.</p> <p>Parameters:</p> Name Type Description Default <code>platform</code> <code>Optional[Platform]</code> <p>Override backend parameter with platform.       Possible options: Platform.ONNXRuntimeONNX</p> <code>None</code> <code>optimization</code> <code>Optional[ONNXOptimization]</code> <p>Possible optimization for ONNX models</p> <code>None</code>"},{"location":"triton/specialized_configs/#model_navigator.triton.specialized_configs.onnx_model_config.ONNXModelConfig.backend","title":"<code>backend: Backend</code>  <code>property</code>","text":"<p>Define backend value for config.</p>"},{"location":"triton/specialized_configs/#model_navigator.triton.specialized_configs.onnx_model_config.ONNXModelConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/onnx_model_config.py</code> <pre><code>def __post_init__(self):\n\"\"\"Validate the configuration for early error handling.\"\"\"\nsuper().__post_init__()\nif self.optimization and not isinstance(self.optimization, ONNXOptimization):\nraise ModelNavigatorWrongParameterError(\"Unsupported optimization type provided.\")\nif self.platform and self.platform != Platform.ONNXRuntimeONNX:\nraise ModelNavigatorWrongParameterError(f\"Unsupported platform provided. Use: {Platform.ONNXRuntimeONNX}.\")\n</code></pre>"},{"location":"triton/specialized_configs/#model_navigator.api.triton.ONNXOptimization","title":"<code>model_navigator.api.triton.ONNXOptimization</code>  <code>dataclass</code>","text":"<p>ONNX possible optimizations.</p> <p>Parameters:</p> Name Type Description Default <code>accelerator</code> <code>Union[OpenVINOAccelerator, TensorRTAccelerator]</code> <p>Execution accelerator for model</p> required"},{"location":"triton/specialized_configs/#model_navigator.triton.specialized_configs.onnx_model_config.ONNXOptimization.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/onnx_model_config.py</code> <pre><code>def __post_init__(self):\n\"\"\"Validate the configuration for early error handling.\"\"\"\nif self.accelerator and type(self.accelerator) not in [OpenVINOAccelerator, TensorRTAccelerator]:\nraise ModelNavigatorWrongParameterError(\"Unsupported optimization type provided.\")\n</code></pre>"},{"location":"triton/specialized_configs/#model_navigator.api.triton.PythonModelConfig","title":"<code>model_navigator.api.triton.PythonModelConfig</code>  <code>dataclass</code>","text":"<p>         Bases: <code>BaseSpecializedModelConfig</code></p> <p>Specialized model config for Python backend supported model.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Sequence[InputTensorSpec]</code> <p>Required definition of model inputs</p> <code>dataclasses.field(default_factory=lambda : [])</code> <code>outputs</code> <code>Sequence[OutputTensorSpec]</code> <p>Required definition of model outputs</p> <code>dataclasses.field(default_factory=lambda : [])</code>"},{"location":"triton/specialized_configs/#model_navigator.triton.specialized_configs.python_model_config.PythonModelConfig.backend","title":"<code>backend: Backend</code>  <code>property</code>","text":"<p>Define backend value for config.</p>"},{"location":"triton/specialized_configs/#model_navigator.triton.specialized_configs.python_model_config.PythonModelConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/python_model_config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n\"\"\"Validate the configuration for early error handling.\"\"\"\nsuper().__post_init__()\nassert len(self.inputs) &gt; 0, \"Model inputs definition is required for Python backend.\"\nassert len(self.outputs) &gt; 0, \"Model outputs definition is required for Python backend.\"\n</code></pre>"},{"location":"triton/specialized_configs/#model_navigator.api.triton.PyTorchModelConfig","title":"<code>model_navigator.api.triton.PyTorchModelConfig</code>  <code>dataclass</code>","text":"<p>         Bases: <code>BaseSpecializedModelConfig</code></p> <p>Specialized model config for PyTorch backend supported model.</p> <p>Parameters:</p> Name Type Description Default <code>platform</code> <code>Optional[Platform]</code> <p>Override backend parameter with platform.       Possible options: Platform.PyTorchLibtorch</p> <code>None</code> <code>inputs</code> <code>Sequence[InputTensorSpec]</code> <p>Required definition of model inputs</p> <code>dataclasses.field(default_factory=lambda : [])</code> <code>outputs</code> <code>Sequence[OutputTensorSpec]</code> <p>Required definition of model outputs</p> <code>dataclasses.field(default_factory=lambda : [])</code>"},{"location":"triton/specialized_configs/#model_navigator.triton.specialized_configs.pytorch_model_config.PyTorchModelConfig.backend","title":"<code>backend: Backend</code>  <code>property</code>","text":"<p>Define backend value for config.</p>"},{"location":"triton/specialized_configs/#model_navigator.triton.specialized_configs.pytorch_model_config.PyTorchModelConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/pytorch_model_config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n\"\"\"Validate the configuration for early error handling.\"\"\"\nsuper().__post_init__()\nassert len(self.inputs) &gt; 0, \"Model inputs definition is required for PyTorch backend.\"\nassert len(self.outputs) &gt; 0, \"Model outputs definition is required for PyTorch backend.\"\nif self.platform and self.platform != Platform.PyTorchLibtorch:\nraise ModelNavigatorWrongParameterError(f\"Unsupported platform provided. Use: {Platform.PyTorchLibtorch}.\")\n</code></pre>"},{"location":"triton/specialized_configs/#model_navigator.api.triton.TensorFlowModelConfig","title":"<code>model_navigator.api.triton.TensorFlowModelConfig</code>  <code>dataclass</code>","text":"<p>         Bases: <code>BaseSpecializedModelConfig</code></p> <p>Specialized model config for TensorFlow backend supported model.</p> <p>Parameters:</p> Name Type Description Default <code>platform</code> <code>Optional[Platform]</code> <p>Override backend parameter with platform.       Possible options: Platform.TensorFlowSavedModel, Platform.TensorFlowGraphDef</p> <code>None</code> <code>optimization</code> <code>Optional[TensorFlowOptimization]</code> <p>Possible optimization for TensorFlow models</p> <code>None</code>"},{"location":"triton/specialized_configs/#model_navigator.triton.specialized_configs.tensorflow_model_config.TensorFlowModelConfig.backend","title":"<code>backend: Backend</code>  <code>property</code>","text":"<p>Define backend value for config.</p>"},{"location":"triton/specialized_configs/#model_navigator.triton.specialized_configs.tensorflow_model_config.TensorFlowModelConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/tensorflow_model_config.py</code> <pre><code>def __post_init__(self):\n\"\"\"Validate the configuration for early error handling.\"\"\"\nsuper().__post_init__()\nif self.optimization and not isinstance(self.optimization, TensorFlowOptimization):\nraise ModelNavigatorWrongParameterError(\"Unsupported optimization type provided.\")\nplatforms = [Platform.TensorFlowSavedModel, Platform.TensorFlowGraphDef]\nif self.platform and self.platform not in platforms:\nraise ModelNavigatorWrongParameterError(f\"Unsupported platform provided. Use one of: {platforms}\")\n</code></pre>"},{"location":"triton/specialized_configs/#model_navigator.api.triton.TensorFlowOptimization","title":"<code>model_navigator.api.triton.TensorFlowOptimization</code>  <code>dataclass</code>","text":"<p>TensorFlow possible optimizations.</p> <p>Parameters:</p> Name Type Description Default <code>accelerator</code> <code>Union[AutoMixedPrecisionAccelerator, GPUIOAccelerator, TensorRTAccelerator]</code> <p>Execution accelerator for model</p> required"},{"location":"triton/specialized_configs/#model_navigator.triton.specialized_configs.tensorflow_model_config.TensorFlowOptimization.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/tensorflow_model_config.py</code> <pre><code>def __post_init__(self):\n\"\"\"Validate the configuration for early error handling.\"\"\"\nif self.accelerator and type(self.accelerator) not in [\nAutoMixedPrecisionAccelerator,\nGPUIOAccelerator,\nTensorRTAccelerator,\n]:\nraise ModelNavigatorWrongParameterError(\"Unsupported optimization type provided.\")\n</code></pre>"},{"location":"triton/specialized_configs/#model_navigator.api.triton.TensorRTModelConfig","title":"<code>model_navigator.api.triton.TensorRTModelConfig</code>  <code>dataclass</code>","text":"<p>         Bases: <code>BaseSpecializedModelConfig</code></p> <p>Specialized model config for TensorRT platform supported model.</p> <p>Parameters:</p> Name Type Description Default <code>platform</code> <code>Optional[Platform]</code> <p>Override backend parameter with platform.       Possible options: Platform.TensorRTPlan</p> <code>None</code> <code>optimization</code> <code>Optional[TensorRTOptimization]</code> <p>Possible optimization for TensorRT models</p> <code>None</code>"},{"location":"triton/specialized_configs/#model_navigator.triton.specialized_configs.tensorrt_model_config.TensorRTModelConfig.backend","title":"<code>backend: Backend</code>  <code>property</code>","text":"<p>Define backend value for config.</p>"},{"location":"triton/specialized_configs/#model_navigator.triton.specialized_configs.tensorrt_model_config.TensorRTModelConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/tensorrt_model_config.py</code> <pre><code>def __post_init__(self):\n\"\"\"Validate the configuration for early error handling.\"\"\"\nsuper().__post_init__()\nif self.optimization and not isinstance(self.optimization, TensorRTOptimization):\nraise ModelNavigatorWrongParameterError(\"Unsupported optimization type provided.\")\nif self.platform and self.platform != Platform.TensorRTPlan:\nraise ModelNavigatorWrongParameterError(f\"Unsupported platform provided. Use: {Platform.TensorRTPlan}.\")\n</code></pre>"},{"location":"triton/specialized_configs/#model_navigator.api.triton.TensorRTOptimization","title":"<code>model_navigator.api.triton.TensorRTOptimization</code>  <code>dataclass</code>","text":"<p>TensorRT possible optimizations.</p> <p>Parameters:</p> Name Type Description Default <code>cuda_graphs</code> <code>bool</code> <p>Use CUDA graphs API to capture model operations and execute them more efficiently.</p> <code>False</code> <code>gather_kernel_buffer_threshold</code> <code>Optional[int]</code> <p>The backend may use a gather kernel to gather input data if the                             device has direct access to the source buffer and the destination                             buffer.</p> <code>None</code> <code>eager_batching</code> <code>bool</code> <p>Start preparing the next batch before the model instance is ready for the next inference.</p> <code>False</code>"},{"location":"triton/specialized_configs/#model_navigator.triton.specialized_configs.tensorrt_model_config.TensorRTOptimization.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/tensorrt_model_config.py</code> <pre><code>def __post_init__(self):\n\"\"\"Validate the configuration for early error handling.\"\"\"\nif not self.cuda_graphs and not self.gather_kernel_buffer_threshold and not self.eager_batching:\nraise ModelNavigatorWrongParameterError(\"At least one of the optimization options should be enabled.\")\n</code></pre>"},{"location":"triton/triton_deployment/","title":"Deploying models","text":""},{"location":"triton/triton_deployment/#deploying-a-model-on-the-triton-inference-server","title":"Deploying a model on the Triton Inference Server","text":"<p>Triton Model Navigator provides an API for working with the Triton model repository. Currently, we support adding your own model or a pre-selected model from a Navigator Package.</p> <p>The API only provides possible functionality for the given model's type and only provides offline validation of the provided configuration. In the end, the model with the configuration is created inside the provided model repository path.</p>"},{"location":"triton/triton_deployment/#adding-your-own-model-to-the-triton-model-repository","title":"Adding your own model to the Triton model repository","text":"<p>When you works with an already exported model you can provide a path to where one's model is located. Then you can use one of the specialized APIs that guides you through what options are possible for deployment of tbe selected model type.</p> <p>Example of deploying a TensorRT model:</p> <pre><code>import model_navigator as nav\nnav.triton.model_repository.add_model(\nmodel_repository_path=\"/path/to/triton/model/repository\",\nmodel_path=\"/path/to/model/plan/file\",\nmodel_name=\"NameOfModel\",\nconfig=nav.triton.TensorRTModelConfig(\nmax_batch_size=256,\noptimization=nav.triton.CUDAGraphOptimization(),\nresponse_cache=True,\n)\n)\n</code></pre> <p>The model catalog with the model file and configuration is going to be created inside <code>model_repository_path</code>.</p>"},{"location":"triton/triton_deployment/#adding-model-from-package-to-the-triton-model-repository","title":"Adding model from package to the Triton model repository","text":"<p>When you want to deploy a model from a package created during the <code>optimize</code> process you can use:</p> <pre><code>import model_navigator as nav\nnav.triton.model_repository.add_model_from_package(\nmodel_repository_path=\"/path/to/triton/model/repository\",\nmodel_name=\"NameOfModel\",\npackage=package,\n)\n</code></pre> <p>The model is automatically selected based on profiling results. The default selection options can be adjusted by changing the <code>strategy</code> argument.</p>"},{"location":"triton/triton_deployment/#using-triton-model-analyzer","title":"Using Triton Model Analyzer","text":"<p>A model added to the Triton Inference Server can be further optimized in the target environment using Triton Model Analyzer.</p> <p>Please, follow the documentation to learn more how to use Triton Model Analyzer.</p>"}]}