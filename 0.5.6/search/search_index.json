{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#triton-model-navigator","title":"Triton Model Navigator","text":"<p>Model conversion plays a crucial role in unlocking the maximum performance capabilities of the underlying hardware. By applying these transformation techniques, models can be optimized to fully utilize the specific features and optimizations offered by the hardware architecture. Furthermore, conversions allow for serialization of models, separating them from the source code. This serialization process enhances portability, allowing the models to be seamlessly deployed in production environments. The decoupling of models from the source code also facilitates maintenance, updates, and collaboration among developers.</p> <p>However, this process comprises multiple steps and offers various potential paths, making manual execution complicated and time-consuming. The Triton Model Navigator offers a user-friendly and automatic solution for converting, optimizing and deploying machine learning models. It offers a single entry point for various supported frameworks, allowing users to start the process of searching for the best deployment option with a single call to the dedicated <code>optimize</code> function. Model Navigator handles model export, conversion, correctness testing, and performance profiling, saving all generated artifacts.</p> <p>Throughout the optimize process, the Model Navigator considers multiple factors, including different precisions, runtimes, and data shapes. This careful consideration allows for the adjustment of the model's configuration to achieve improved performance, effectively minimizing the costs associated with serving the model.</p> <p>Converted models can be easily deployed on the PyTriton or Triton Inference Server.</p> <p>The Model Navigator generates multiple optimized and production-ready models. The table below illustrates the model formats that can be obtained by using the Model Navigator with various frameworks.</p> <p>Table: Supported conversion target formats per each supported Python framework or file.</p> PyTorch TensorFlow 2 JAX ONNX Torch 2 Compile SavedModel SavedModel TensorRT TorchScript Trace TensorFlowTensorRT TensorFlowTensorRT TorchScript Script ONNX ONNX TorchTensorRT TensorRT TensorRT ONNX TensorRT <p>Note: The Model Navigator has the capability to support any Python function as input. However, in this particular case, its role is limited to profiling the function without generating any serialized models.</p> <p>The Model Navigator stores all artifacts within the <code>navigator_workspace</code>. Additionally, it provides the option to save a portable and transferable <code>Navigator Package</code> that includes only the models with minimal latency and maximal throughput. This package also includes base formats that can be used to regenerate the <code>TensorRT</code> plan on the target hardware.</p> <p>Table: Model formats that can be generated from saved <code>Navigator Package</code> and from model sources.</p> From model source From Navigator Package SavedModel TorchTensorRT TensorFlowTensorRT TensorFlowTensorRT TorchScript Trace ONNX TorchScript Script TensorRT Torch 2 Compile TorchTensorRT ONNX TensoRT"},{"location":"CHANGELOG/","title":"Changelog","text":""},{"location":"CHANGELOG/#changelog","title":"Changelog","text":""},{"location":"CHANGELOG/#056","title":"0.5.6","text":"<ul> <li>fix: Load samples as sorted to keep valid order</li> <li>fix: Execute conversion when model already exists in path</li> <li> <p>Other minor fixes and improvements</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>PyTorch 2.1.0a0+fe05266f</li> <li>TensorFlow 2.12.0</li> <li>TensorRT 8.6.1</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.47.1</li> <li>GraphSurgeon: 0.3.26</li> <li>tf2onnx v1.14.0</li> <li>Other component versions depend on the used framework containers versions.     See its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#055","title":"0.5.5","text":"<ul> <li>new: Public nav.utilities module with UnpackedDataloader wrapper</li> <li>new: Added support for strict flag in Torch custom config</li> <li>new: Extended TensorRT custom config to support builder optimization level and hardware compatibility flags</li> <li> <p>fix: Invalid optimal shape calculation for odd values in max batch size</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>PyTorch 2.1.0a0+fe05266f</li> <li>TensorFlow 2.12.0</li> <li>TensorRT 8.6.1</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.47.1</li> <li>GraphSurgeon: 0.3.26</li> <li>tf2onnx v1.14.0</li> <li>Other component versions depend on the used framework containers versions.     See its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#054","title":"0.5.4","text":"<ul> <li>new: Custom implementation for ONNX and TensorRT runners</li> <li>new: Use CUDA 12 for JAX in unit tests and functional tests</li> <li>new: Step-by-step examples</li> <li>new: Updated documentation</li> <li>new: TensorRTCUDAGraph runner introduced with support for CUDA graphs</li> <li>fix: Optimal shape not set correctly during adaptive conversion</li> <li>fix: Find max batch size command for JAX</li> <li> <p>fix: Save stdout to logfiles in debug mode</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>PyTorch 2.1.0a0+fe05266f</li> <li>TensorFlow 2.12.0</li> <li>TensorRT 8.6.1</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.47.1</li> <li>GraphSurgeon: 0.3.26</li> <li>tf2onnx v1.14.0</li> <li>Other component versions depend on the used framework containers versions.     See its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#053","title":"0.5.3","text":"<ul> <li> <p>fix: filter outputs using output_metadata in ONNX runners</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>PyTorch 2.0.0a0+1767026</li> <li>TensorFlow 2.11.0</li> <li>TensorRT 8.5.3.1</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.44.2</li> <li>GraphSurgeon: 0.3.26</li> <li>tf2onnx v1.14.0</li> <li>Other component versions depend on the used framework containers versions.     See its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#052","title":"0.5.2","text":"<ul> <li>new: Added Contributor License Agreement (CLA)</li> <li>fix: Added missing --extra-index-url to installation instruction for pypi</li> <li>fix: Updated wheel readme</li> <li>fix: Do not run TorchScript export when only ONNX in target formats and ONNX extended export is disabled</li> <li> <p>fix: Log full traceback for ModelNavigatorUserInputError</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>PyTorch 2.0.0a0+1767026</li> <li>TensorFlow 2.11.0</li> <li>TensorRT 8.5.3.1</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.44.2</li> <li>GraphSurgeon: 0.3.26</li> <li>tf2onnx v1.14.0</li> <li>Other component versions depend on the used framework containers versions.     See its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#051","title":"0.5.1","text":"<ul> <li>fix: Using relative workspace cause error during Onnx to TensorRT conversion</li> <li>fix: Added external weight in package for ONNX format</li> <li> <p>fix: bugfixes for functional tests</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>PyTorch 1.14.0a0+410ce96</li> <li>TensorFlow 2.11.0</li> <li>TensorRT 8.5.3</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.44.2</li> <li>GraphSurgeon: 0.4.6</li> <li>tf2onnx v1.13.0</li> <li>Other component versions depend on the used framework containers versions.     See its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#050","title":"0.5.0","text":"<ul> <li>new: Support for PyTriton deployment</li> <li>new: Support for Python models with python.optimize API</li> <li>new: PyTorch 2 compile CPU and CUDA runners</li> <li>new: Collect conversion max batch size in status</li> <li>new: PyTorch runners with <code>compile</code> support</li> <li>change: Improved handling CUDA and CPU runners</li> <li>change: Reduced finding device max batch size time by running it once as separate pipeline</li> <li> <p>change: Stored find max batch size result in separate filed in status</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>PyTorch 1.14.0a0+410ce96</li> <li>TensorFlow 2.11.0</li> <li>TensorRT 8.5.3</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.44.2</li> <li>GraphSurgeon: 0.4.6</li> <li>tf2onnx v1.13.0</li> <li>Other component versions depend on the used framework containers versions.     See its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#044","title":"0.4.4","text":"<ul> <li> <p>fix: when exporting single input model to saved model, unwrap one element list with inputs</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>PyTorch 1.14.0a0+410ce96</li> <li>TensorFlow 2.11.0</li> <li>TensorRT 8.5.3</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.44.2</li> <li>GraphSurgeon: 0.4.6</li> <li>tf2onnx v1.13.0</li> <li>Other component versions depend on the used framework containers versions.     See its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#043","title":"0.4.3","text":"<ul> <li> <p>fix: in Keras inference use model.predict(tensor) for single input models</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>PyTorch 1.14.0a0+410ce96</li> <li>TensorFlow 2.11.0</li> <li>TensorRT 8.5.3</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.44.2</li> <li>GraphSurgeon: 0.4.6</li> <li>tf2onnx v1.13.0</li> <li>Other component versions depend on the used framework containers versions.     See its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#042","title":"0.4.2","text":"<ul> <li>fix: loading configuration for trt_profile from package</li> <li>fix: missing reproduction scripts and logs inside package</li> <li>fix: invalid model path in reproduction script for ONNX to TRT conversion</li> <li> <p>fix: collecting metadata from ONNX model in main thread during ONNX to TRT conversion</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>PyTorch 1.14.0a0+410ce96</li> <li>TensorFlow 2.11.0</li> <li>TensorRT 8.5.3</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.44.2</li> <li>GraphSurgeon: 0.4.6</li> <li>tf2onnx v1.13.0</li> <li>Other component versions depend on the used framework containers versions.     See its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#041","title":"0.4.1","text":"<ul> <li> <p>fix: when specified use dynamic axes from custom OnnxConfig</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>PyTorch 1.14.0a0+410ce96</li> <li>TensorFlow 2.11.0</li> <li>TensorRT 8.5.2.2</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.43.1</li> <li>GraphSurgeon: 0.4.6</li> <li>tf2onnx v1.13.0</li> <li>Other component versions depend on the used framework containers versions.     See its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#040","title":"0.4.0","text":"<ul> <li>new: <code>optimize</code> method that replace <code>export</code> and perform max batch size search and improved profiling during process</li> <li>new: Introduced custom configs in <code>optimize</code> for better parametrization of export/conversion commands</li> <li>new: Support for adding user runners for model correctness and profiling</li> <li>new: Search for max possible batch size per format during conversion and profiling</li> <li>new: API for creating Triton model store from Navigator Package and user provided models</li> <li>change: Improved status structure for Navigator Package</li> <li>deprecated: Optimize for Triton Inference Server support</li> <li>deprecated: HuggingFace contrib module</li> <li> <p>Bug fixes and other improvements</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>PyTorch 1.14.0a0+410ce96</li> <li>TensorFlow 2.11.0</li> <li>TensorRT 8.5.2.2</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.43.1</li> <li>GraphSurgeon: 0.4.6</li> <li>tf2onnx v1.13.0</li> <li>Other component versions depend on the used framework containers versions.     See its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#038","title":"0.3.8","text":"<ul> <li> <p>Updated NVIDIA containers defaults to 22.11</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.42.2</li> <li>GraphSurgeon: 0.3.19</li> <li>Triton Model Analyzer 1.20.0</li> <li>tf2onnx: v1.12.1</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#037","title":"0.3.7","text":"<ul> <li> <p>Updated NVIDIA containers defaults to 22.10</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.42.2</li> <li>GraphSurgeon: 0.3.19</li> <li>Triton Model Analyzer 1.20.0</li> <li>tf2onnx: v1.12.1</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#036","title":"0.3.6","text":"<ul> <li>Updated NVIDIA containers defaults to 22.09</li> <li> <p>Model Navigator Export API:</p> <ul> <li>new: cast int64 input data to int32 in runner for Torch-TensorRT</li> <li>new: cast 64-bit data samples to 32-bit values for TensorRT</li> <li>new: verbose flag for logging export and conversion commands to console</li> <li>new: debug flag to enable debug mode for export and conversion commands</li> <li>change: logs from commands are streamed to console during command run</li> <li>change: package load omit the log files and autogenerated scripts</li> </ul> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.42.2</li> <li>GraphSurgeon: 0.3.19</li> <li>Triton Model Analyzer 1.20.0</li> <li>tf2onnx: v1.12.1</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#035","title":"0.3.5","text":"<ul> <li>Updated NVIDIA containers defaults to 22.08</li> <li> <p>Model Navigator Export API:</p> <ul> <li>new: TRTExec runner use <code>use_cuda_graph=True</code> by default</li> <li>new: log warning instead of raising error when dataloader dump inputs with <code>nan</code> or <code>inf</code> values</li> <li>new: enabled logging for command input parameters</li> <li>fix: invalid use of Polygraphy TRT profile when trt_dynamic_axes is passed to export function</li> </ul> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.38.0</li> <li>GraphSurgeon: 0.3.19</li> <li>Triton Model Analyzer 1.19.0</li> <li>tf2onnx: v1.12.1</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#034","title":"0.3.4","text":"<ul> <li>Updated NVIDIA containers defaults to 22.07</li> <li>Model Navigator OTIS:<ul> <li>deprecated: <code>TF32</code> precision for TensorRT from CLI options - will be removed in future versions</li> <li>fix: Tensorflow module was imported when obtaining model signature during conversion</li> </ul> </li> <li> <p>Model Navigator Export API:</p> <ul> <li>new: Support for building framework containers with Model Navigator installed</li> <li>new: Example for loading Navigator Package for reproducing the results</li> <li>new: Create reproducing script for correctness and performance steps</li> <li>new: TrtexecRunner for correctness and performance tests   with trtexec tool</li> <li>new: Use TF32 support by default for models with FP32 precision</li> <li>new: Reset conversion parameters to defaults when using <code>load</code> for package</li> <li>new: Testing all options for JAX export enable_xla and jit_compile parameters</li> <li>change: Profiling stability improvements</li> <li>change: Rename of <code>onnx_runtimes</code> export function parameters to <code>runtimes</code></li> <li>deprecated: <code>TF32</code> precision for TensorRT from available options - will be removed in future versions</li> <li>fix: Do not save TF-TRT models to the .nav package</li> <li>fix: Do not save TF-TRT models from the .nav package</li> <li>fix: Correctly load .nav packages when <code>_input_names</code> or <code>_output_names</code> specified</li> <li>fix: Adjust TF and TF-TRT model signatures to match <code>input_names</code></li> <li>fix: Save ONNX opset for CLI configuration inside package</li> <li>fix: Reproduction scripts were missing for failing paths</li> </ul> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.38.0</li> <li>GraphSurgeon: 0.3.19</li> <li>Triton Model Analyzer 1.17.0</li> <li>tf2onnx: v1.11.1</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#033","title":"0.3.3","text":"<ul> <li> <p>Model Navigator Export API:</p> <ul> <li>new: Improved handling inputs and outputs metadata</li> <li>new: Navigator Package version updated to 0.1.3</li> <li>new: Backward compatibility with previous versions of Navigator Package</li> <li>fix: Dynamic shapes for output shapes were read incorrectly</li> </ul> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.36.2</li> <li>GraphSurgeon: 0.3.19</li> <li>Triton Model Analyzer 1.17.0</li> <li>tf2onnx: v1.11.1</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#032","title":"0.3.2","text":"<ul> <li>Updated NVIDIA containers defaults to 22.06</li> <li>Model Navigator OTIS:<ul> <li>new: Perf Analyzer profiling data use base64 format for content</li> <li>fix: Signature for TensorRT model when has <code>uint64</code> or <code>int64</code> input and/or outputs defined</li> </ul> </li> <li> <p>Model Navigator Export API:</p> <ul> <li>new: Updated navigator package format to 0.1.1</li> <li>new: Added Model Navigator version to status file</li> <li>new: Add atol and rtol configuration to CLI config for model</li> <li>new: Added experimental support for JAX models</li> <li>new: In case of export or conversion failures prepare minimal scripts to reproduce errors</li> <li>fix: Conversion parameters are not stored in Navigator Package for CLI execution</li> </ul> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.36.2</li> <li>GraphSurgeon: 0.3.19</li> <li>Triton Model Analyzer 1.17.0</li> <li>tf2onnx: v1.11.1</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#031","title":"0.3.1","text":"<ul> <li>Updated NVIDIA containers defaults to 22.05</li> <li>Model Navigator OTIS:<ul> <li>fix: Saving paths inside the Triton package status file</li> <li>fix: Empty list of gpus cause the process run on CPU only</li> <li>fix: Reading content from zipped Navigator Package</li> <li>fix: When no GPU or target device set to CPU <code>optimize</code> avoid running unsupported conversions in CLI</li> <li>new: Converter accept passing target device kind to selected CPU or GPU supported conversions</li> <li>new: Added support for OpenVINO accelerator for ONNXRuntime</li> <li>new: Added option <code>--config-search-early-exit-enable</code> for Model Analyzer early exit support   in manual profiling mode</li> <li>new: Added option <code>--model-config-name</code> to the <code>select</code> command.   It allows to pick a particular model configuration for deployment from the set of all configurations   generated by Triton Model Analyzer, even if it's not the best performing one.</li> <li>removed: The <code>--tensorrt-strict-types</code> option has been removed due to deprecation of the functionality   in upstream libraries.</li> </ul> </li> <li> <p>Model Navigator Export API:</p> <ul> <li>new: Added dynamic shapes support and trt dynamic shapes support for TensorFlow2 export</li> <li>new: Improved per format logging</li> <li>new: PyTorch to Torch-TRT precision selection added</li> <li>new: Advanced profiling (measurement windows, configurable batch sizes)</li> </ul> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.36.2</li> <li>GraphSurgeon: 0.3.19</li> <li>Triton Model Analyzer 1.16.0</li> <li>tf2onnx: v1.10.1</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#030","title":"0.3.0","text":"<ul> <li>Updated NVIDIA containers defaults to 22.04</li> <li>Model Navigator Export API<ul> <li>Support for exporting models from TensorFlow2 and PyTorch source code to supported target formats</li> <li>Support for conversion from ONNX to supported target formats</li> <li>Support for exporting HuggingFace models</li> <li>Conversion, Correctness and performance tests for exported models</li> <li>Definition of package structure for storing all exported models and additional metadata</li> </ul> </li> <li>Model Navigator OTIS:<ul> <li>change: <code>run</code> command has been deprecated and may be removed in a future release</li> <li>new: <code>optimize</code> command replace <code>run</code> and produces an output <code>*.triton.nav</code> package</li> <li>new: <code>select</code> selects the best-performing configuration from <code>*.triton.nav</code> package and create a   Triton Inference Server model repository</li> <li>new: Added support for using shared memory option for Perf Analyzer</li> </ul> </li> <li> <p>Remove wkhtmltopdf package dependency</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.35.1</li> <li>GraphSurgeon: 0.3.14</li> <li>Triton Model Analyzer 1.14.0</li> <li>tf2onnx: v1.9.3</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#027","title":"0.2.7","text":"<ul> <li>Updated NVIDIA containers defaults to 22.02</li> <li>Removed support for Python 3.7</li> <li>Triton Model configuration related:<ul> <li>Support dynamic batching without setting preferred batch size value</li> </ul> </li> <li> <p>Profiling related:</p> <ul> <li>Deprecated <code>--config-search-max-preferred-batch-size</code> flag as is no longer supported in Triton Model Analyzer</li> </ul> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.35.1</li> <li>GraphSurgeon: 0.3.14</li> <li>Triton Model Analyzer 1.8.2</li> <li>tf2onnx: v1.9.3</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#026","title":"0.2.6","text":"<ul> <li>Updated NVIDIA containers defaults to 22.01</li> <li>Removed support for Python 3.6 due to EOL</li> <li>Conversion related:<ul> <li>Added support for Torch-TensorRT conversion</li> </ul> </li> <li> <p>Fixes and improvements</p> <ul> <li>Processes inside containers started by Model Navigator now run without root privileges</li> <li>Fix for volume mounts while running Triton Inference Server in container from other container</li> <li>Fix for conversion of models without file extension on input and output paths</li> <li>Fix using <code>--model-format</code> argument when input and output files have no extension</li> </ul> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.35.1</li> <li>GraphSurgeon: 0.3.14</li> <li>Triton Model Analyzer 1.8.2</li> <li>tf2onnx: v1.9.3</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> <li> <p>Known issues and limitations</p> <ul> <li>missing support for stateful models (ex. time-series one)</li> <li>no verification of conversion results for conversions: TF -&gt; ONNX, TF-&gt;TF-TRT, TorchScript -&gt; ONNX</li> <li>possible to define a single profile for TensorRT</li> <li>no custom ops support</li> <li>Triton Inference Server stays in the background when the profile   process is interrupted by the user</li> <li>TF-TRT conversion lost outputs shapes info</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#025","title":"0.2.5","text":"<ul> <li>Updated NVIDIA containers defaults to 21.12</li> <li>Conversion related:<ul> <li>[Experimental] TF-TRT - fixed default dataset profile generation</li> </ul> </li> <li> <p>Configuration Model on Triton related</p> <ul> <li>Fixed name for onnxruntime backend in Triton model deployment configuration</li> </ul> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.33.1</li> <li>GraphSurgeon: 0.3.14</li> <li>Triton Model Analyzer 1.8.2</li> <li>tf2onnx: v1.9.3</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> <li> <p>Known issues and limitations</p> <ul> <li>missing support for stateful models (ex. time-series one)</li> <li>no verification of conversion results for conversions: TF -&gt; ONNX, TF-&gt;TF-TRT, TorchScript -&gt; ONNX</li> <li>possible to define a single profile for TensorRT</li> <li>no custom ops support</li> <li>Triton Inference Server stays in the background when the profile   process is interrupted by the user</li> <li>TF-TRT conversion lost outputs shapes info</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#024-2021-12-07","title":"0.2.4 (2021-12-07)","text":"<ul> <li>Updated NVIDIA containers defaults to 21.10</li> <li>Fixed generating profiling data when <code>dtypes</code> are not passed</li> <li>Conversion related:<ul> <li>[Experimental] Added support for TF-TRT conversion</li> </ul> </li> <li>Configuration Model on Triton related<ul> <li>Added possibility to select batching mode - default, dynamic and disabled options supported</li> </ul> </li> <li>Install dependencies from pip packages instead of wheels for Polygraphy and Triton Model Analyzer</li> <li> <p>fixes and improvements</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.33.1</li> <li>GraphSurgeon: 0.3.14</li> <li>Triton Model Analyzer 1.8.2</li> <li>tf2onnx: v1.9.3</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> <li> <p>Known issues and limitations</p> <ul> <li>missing support for stateful models (ex. time-series one)</li> <li>no verification of conversion results for conversions: TF -&gt; ONNX, TF-&gt;TF-TRT, TorchScript -&gt; ONNX</li> <li>possible to define a single profile for TensorRT</li> <li>no custom ops support</li> <li>Triton Inference Server stays in the background when the profile   process is interrupted by the user</li> <li>TF-TRT conversion lost outputs shapes info</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#023-2021-11-10","title":"0.2.3 (2021-11-10)","text":"<ul> <li>Updated NVIDIA containers defaults to 21.09</li> <li>Improved naming of arguments specific for TensorRT conversion and acceleration with backward compatibility</li> <li>Use pip package for Triton Model Analyzer installation with minimal version 1.8.0</li> <li>Fixed <code>model_repository</code> path to be not relative to <code>&lt;navigator_workspace&gt;</code> dir</li> <li>Handle exit codes correctly from CLI commands</li> <li>Support for use device ids for <code>--gpus</code> argument</li> <li>Conversion related<ul> <li>Added support for precision modes to support multiple precisions during conversion to TensorRT</li> <li>Added <code>--tensorrt-sparse-weights</code> flag for sparse weight optimization for TensorRT</li> <li>Added <code>--tensorrt-strict-types</code> flag forcing it to choose tactics based on the layer precision for TensorRT</li> <li>Added <code>--tensorrt-explicit-precision</code> flag enabling explicit precision mode</li> <li>Fixed nan values appearing in relative tolerance during conversion to TensorRT</li> </ul> </li> <li>Configuration Model on Triton related<ul> <li>Removed default value for <code>engine_count_per_device</code></li> <li>Added possibility to define Triton Custom Backend parameters with <code>triton_backend_parameters</code> command</li> <li>Added possibility to define max workspace size for TensorRT backend accelerator using   argument <code>tensorrt_max_workspace_size</code></li> </ul> </li> <li>Profiling related<ul> <li>Added <code>config_search</code> prefix to all profiling parameters (BREAKING CHANGE)</li> <li>Added <code>config_search_max_preferred_batch_size</code> parameter</li> <li>Added <code>config_search_backend_parameters</code> parameter</li> </ul> </li> <li> <p>fixes and improvements</p> </li> <li> <p>Versions of used external components:</p> <ul> <li>Polygraphy: 0.32.0</li> <li>GraphSurgeon: 0.3.13</li> <li>tf2onnx: v1.9.2 (support for ONNX opset 14,   tf 1.15 and 2.6)</li> <li>Triton Model Analyzer 1.8.2</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> <li> <p>Known issues and limitations</p> <ul> <li>missing support for stateful models (ex. time-series one)</li> <li>missing support for models without batching support</li> <li>no verification of conversion results for conversions: TF -&gt; ONNX, TorchScript -&gt; ONNX</li> <li>possible to define a single profile for TensorRT</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#022-2021-09-06","title":"0.2.2 (2021-09-06)","text":"<ul> <li> <p>Updated NVIDIA containers defaults to 21.08</p> </li> <li> <p>Versions of used external components:</p> <ul> <li>Triton Model Analyzer: 1.7.0</li> <li>Triton Inference Server Client: 2.13.0</li> <li>Polygraphy: 0.31.1</li> <li>GraphSurgeon: 0.3.11</li> <li>tf2onnx: v1.9.1 (support for ONNX opset 14,   tf 1.15 and 2.5)</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> <li> <p>Known issues and limitations</p> <ul> <li>missing support for stateful models (ex. time-series one)</li> <li>missing support for models without batching support</li> <li>no verification of conversion results for conversions: TF -&gt; ONNX, TorchScript -&gt; ONNX</li> <li>possible to define a single profile for TensorRT</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#021-2021-08-17","title":"0.2.1 (2021-08-17)","text":"<ul> <li>Fixed triton-model-config error when tensorrt_capture_cuda_graph flag is not passed</li> <li>Dump Conversion Comparator inputs and outputs into JSON files</li> <li>Added information in logs on the tolerance parameters values to pass the conversion verification</li> <li>Use <code>count_windows</code> mode as default option for Perf Analyzer</li> <li>Added possibility to define custom docker images</li> <li> <p>Bugfixes</p> </li> <li> <p>Versions of used external components:</p> <ul> <li>Triton Model Analyzer: 1.6.0</li> <li>Triton Inference Server Client: 2.12.0</li> <li>Polygraphy: 0.31.1</li> <li>GraphSurgeon: 0.3.11</li> <li>tf2onnx: v1.9.1 (support for ONNX opset 14,   tf 1.15 and 2.5)</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> <li> <p>Known issues and limitations</p> <ul> <li>missing support for stateful models (ex. time-series one)</li> <li>missing support for models without batching support</li> <li>no verification of conversion results for conversions: TF -&gt; ONNX, TorchScript -&gt; ONNX</li> <li>possible to define a single profile for TensorRT</li> <li>TensorRT backend acceleration not supported for ONNX Runtime in Triton Inference Server ver. 21.07</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#020-2021-07-05","title":"0.2.0 (2021-07-05)","text":"<ul> <li> <p>comprehensive refactor of command-line API in order to provide more gradual   pipeline steps execution</p> </li> <li> <p>Versions of used external components:</p> <ul> <li>Triton Model Analyzer: 21.05</li> <li>tf2onnx: v1.8.5 (support for ONNX opset 13,   tf 1.15 and 2.5)</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> <li> <p>Known issues and limitations</p> <ul> <li>missing support for stateful models (ex. time-series one)</li> <li>missing support for models without batching support</li> <li>no verification of conversion results for conversions: TF -&gt; ONNX, TorchScript -&gt; ONNX</li> <li>issues with TorchScript -&gt; ONNX conversion due   to issue in PyTorch 1.8<ul> <li>affected NVIDIA PyTorch containers: 20.12, 21.02, 21.03</li> <li>workaround: use PyTorch containers newer than 21.03</li> </ul> </li> <li>possible to define a single profile for TensorRT</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#011-2021-04-12","title":"0.1.1 (2021-04-12)","text":"<ul> <li>documentation update</li> </ul>"},{"location":"CHANGELOG/#010-2021-04-09","title":"0.1.0 (2021-04-09)","text":"<ul> <li> <p>Release of main components:</p> <ul> <li>Model Converter - converts the model to a set of variants optimized for inference or to be later optimized by   Triton Inference Server backend.</li> <li>Model Repo Builder - setup Triton Inference Server Model Repository, including its configuration.</li> <li>Model Analyzer - select optimal Triton Inference Server configuration based on models compute and memory   requirements,   available computation infrastructure, and model application constraints.</li> <li>Helm Chart Generator - deploy Triton Inference Server and model with optimal configuration to cloud.</li> </ul> </li> <li> <p>Versions of used external components:</p> <ul> <li>Triton Model Analyzer: 21.03+616e8a30</li> <li>tf2onnx: v1.8.4 (support for ONNX opset 13, tf 1.15   and 2.4)</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   Refer to its support matrix   for a detailed summary.</li> </ul> </li> <li> <p>Known issues</p> <ul> <li>missing support for stateful models (ex. time-series one)</li> <li>missing support for models without batching support</li> <li>no verification of conversion results for conversions: TF -&gt; ONNX, TorchScript -&gt; ONNX</li> <li>issues with TorchScript -&gt; ONNX conversion due   to issue in PyTorch 1.8<ul> <li>affected NVIDIA PyTorch containers: 20.12, 21.03</li> <li>workaround: use containers different from above</li> </ul> </li> <li>Triton Inference Server stays in the background when the profile process is interrupted by the user</li> </ul> </li> </ul>"},{"location":"CONTRIBUTING/","title":"Contributing","text":""},{"location":"CONTRIBUTING/#contributing","title":"Contributing","text":"<p>Contributions are welcome, and they are much appreciated! Every little helps, and we will always give credit.</p>"},{"location":"CONTRIBUTING/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"CONTRIBUTING/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/triton-inference-server/model_navigator/issues.</p> <p>If you are reporting a bug, include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"CONTRIBUTING/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"CONTRIBUTING/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"CONTRIBUTING/#write-documentation","title":"Write Documentation","text":"<p>The Triton Model Navigator could always use more documentation, whether as part of the official Triton Model Navigator docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"CONTRIBUTING/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/triton-inference-server/model_navigator/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible to make it easier to implement.</li> </ul>"},{"location":"CONTRIBUTING/#sign-your-work","title":"Sign your Work","text":"<p>We require that all contributors \"sign-off\" on their commits. This certifies that the contribution is your original work, or you have the rights to submit it under the same license or a compatible license.</p> <p>Any contribution which contains commits that are not Signed-Off will not be accepted.</p> <p>To sign off on a commit, you simply use the <code>--signoff</code> (or <code>-s</code>) option when committing your changes: <pre><code>$ git commit -s -m \"Add cool feature.\n</code></pre></p> <p>This will append the following to your commit message:</p> <pre><code>Signed-off-by: Your Name &lt;your@email.com&gt;\n</code></pre> <p>By doing this, you certify the below:</p> <pre><code>Developer Certificate of Origin\nVersion 1.1\n\nCopyright (C) 2004, 2006 The Linux Foundation and its contributors.\n1 Letterman Drive\nSuite D4700\nSan Francisco, CA, 94129\n\nEveryone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.\n\nDeveloper's Certificate of Origin 1.1\n\nBy making a contribution to this project, I certify that:\n\n(a) The contribution was created in whole or in part by me and I have the right to submit it under the open source license indicated in the file; or\n\n(b) The contribution is based upon previous work that, to the best of my knowledge, is covered under an appropriate open source license and I have the right under that license to submit that work with modifications, whether created in whole or in part by me, under the same open source license (unless I am permitted to submit under a different license), as indicated in the file; or\n\n(c) The contribution was provided directly to me by some other person who certified (a), (b) or (c) and I have not modified it.\n\n(d) I understand and agree that this project and the contribution are public and that a record of the contribution (including all personal information I submit with it, including my sign-off) is maintained indefinitely and may be redistributed consistent with this project or the open source license(s) involved.\n</code></pre>"},{"location":"CONTRIBUTING/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up the <code>Triton Model Navigator</code> for local development.</p> <ol> <li>Fork the <code>Triton Model Navigator</code> repo on GitHub.</li> <li>Clone your fork locally:</li> </ol> <pre><code>$ git clone git@github.com:your_name_here/model-navigator.git\n</code></pre> <ol> <li>Install your local copy into a virtualenv. Assuming you have virtualenvwrapper installed, this is how you set up your fork for local development:</li> </ol> <pre><code>$ mkvirtualenv model_navigator\n$ cd model_navigator/\n$ make install-dev\n</code></pre> <ol> <li>Create a branch for local development:</li> </ol> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> <ol> <li>When you're done making changes, check that your changes pass linters and the    tests, including testing other Python versions with tox:</li> </ol> <pre><code>$ make lint  # will run i.a. flake8 and pytype linters\n$ make test  # will run a test with on your current virtualenv\n$ make test-fw  # will run a framework test inside framework container\n</code></pre> <ol> <li>Commit your changes and push your branch to GitHub:</li> </ol> <pre><code>$ git add .\n$ git commit -s -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> <ol> <li>Submit a pull request through the GitHub website.</li> </ol>"},{"location":"CONTRIBUTING/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, you should update the docs. Put    your new functionality into a function with a docstring, and add the    feature to the list in README.md.</li> </ol>"},{"location":"CONTRIBUTING/#tips","title":"Tips","text":"<p>To run a subset of tests:</p> <pre><code>$ pytest tests.test_model_navigator\n</code></pre>"},{"location":"CONTRIBUTING/#releasing","title":"Releasing","text":"<p>As a reminder for the maintainers on how to deploy - make sure all your changes are committed (including an entry in CHANGELOG.md) into the master branch. Then run:</p> <pre><code>$ bump2version patch # possible: major / minor / patch\n$ git push\n$ git push --tags\n</code></pre>"},{"location":"CONTRIBUTING/#documentation","title":"Documentation","text":"<p>Add/update docstrings as defined in Google Style Guide.</p>"},{"location":"CONTRIBUTING/#contributor-license-agreement-cla","title":"Contributor License Agreement (CLA)","text":"<p>Triton requires that all contributors (or their corporate entity) send a signed copy of the Contributor License Agreement to triton-cla@nvidia.com. NOTE: Contributors with no company affiliation can fill <code>N/A</code> in the <code>Corporation Name</code> and <code>Corporation Address</code> fields.</p>"},{"location":"LICENSE/","title":"License","text":"<pre><code>                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n</code></pre> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol>"},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#examples","title":"Examples","text":"<p>We provide step-by-step examples that demonstrate how to use various features of Model Navigator. For the sake of readability and accessibility, we use a simple <code>torch.nn.Linear</code> model as an example. These examples illustrate how to optimize, test and deploy the model on the PyTriton and Triton Inference Server.</p>"},{"location":"examples/#step-by-step-examples","title":"Step-by-step examples","text":"<ol> <li>Optimize model</li> <li>Optimize model and verify model</li> <li>Optimize model and save package</li> <li>Load and optimize package</li> <li>Optimize and server model on PyTriton</li> <li>Optimize and serve model on Triton Inference Server</li> <li>Optimize model and use for offline inference</li> <li>Optimize PyTorch HiFi-GAN QAT model</li> <li>Custom configuration for optimize</li> </ol>"},{"location":"examples/#example-models","title":"Example models","text":"<p>Inside example/models directory you can find ready to use example models in various frameworks.</p> <p><code>Python</code>: - Identity Model</p> <p><code>PyTorch</code>:</p> <ul> <li>Linear Model</li> <li>ResNet50</li> <li>BERT</li> </ul> <p><code>TensorFlow</code>:</p> <ul> <li>Linear Model</li> <li>EfficientNet</li> <li>BERT</li> </ul> <p><code>JAX</code>:</p> <ul> <li>Linear Model</li> <li>GPT-2</li> </ul> <p><code>ONNX</code>:</p> <ul> <li>Identity Model</li> </ul>"},{"location":"how_it_works/","title":"How it works?","text":""},{"location":"how_it_works/#how-it-works","title":"How it works?","text":"<p>The Model Navigator optimize process encompasses several crucial steps aimed at improving the performance of deep learning models and converting them into the most optimal formats. Model Navigator supports various frameworks, including TensorFlow 2, PyTorch, ONNX, and JAX.</p> <p>To initiate the multi-step conversion and optimization process in <code>Model Navigator</code>, users only need to provide the model and dataloader. However, for further customization, additional parameters and <code>custom_configs</code> can be used to tailor the optimization process to specific requirements. The optimization process consists of the following steps:</p> <ol> <li> <p>Model export: The source deep learning model, created using one of the supported frameworks, is exported to one of the intermediaries formats: TorchScript, SavedModel, ONNX.</p> </li> <li> <p>Model conversion: The exported model is then converted into a target representation with goal of achiving best possible performance, it includes: TorchTensorRT, TensorFlowTensorRT, ONNX, TensorRT.</p> </li> <li> <p>Correctness test: To ensure the correctness of the produced models, Model Navigator performs a series of correctness tests. These tests callculates absolute and relative tolerance values for source and converted models.</p> </li> <li> <p>Model profiling: Model Navigator conducts performance profiling of the converted models. This process uses <code>Navigator Runners</code> to perform inference and measure its time. The profiler aims to find the maximum throughput for each model and calculates its latency. This information can then be used to retrieve the best runners and provide you with performance details of the optimal configuration:</p> </li> </ol> <pre><code>2023-04-27 14:24:46 INFO     Navigator:\nStrategy: MaxThroughputStrategy\n  Latency: 81.6086 [ms]\n  Throughput: 1568.1343 [infer/sec]\n  Runner: TensorRT\n  Model: trt-fp16/model.plan\n2023-04-27 14:24:46 INFO     Navigator:\nStrategy: MinLatencyStrategy\n  Latency: 3.2270 [ms]\n  Throughput: 309.5315 [infer/sec]\n  Runner: TensorRT\n  Model: trt-fp16/model.plan\n</code></pre> <ol> <li> <p>Verification: Once the profiling is complete, Model Navigator performs verification tests to validate the metrics provided by the user in <code>verify_func</code> against all converted models.</p> </li> <li> <p>Inference deployment: Optimized models can be seamlessly deployed to PyTriton or Triton Inference Server. The Model Navigator offers convenient functionalities to assist in obtaining an inference runner that can be utilized as an inference callback for PyTriton, or for generating a <code>model_repository</code> for Triton Inference Server.</p> </li> </ol> <p>By going through the Optimize process with Model Navigator, deep learning models can be optimized and converted into the most suitable formats for deployment, with NVIDIA TensorRT often providing the optimal solution to achieve the best performance.</p> <p>NVIDIA TensorRT can be used for applications deployed to the data center, as well as embedded and automotive environments. It powers key NVIDIA solutions such as NVIDIA TAO, NVIDIA DRIVE\u2122, NVIDIA Clara\u2122, and NVIDIA Jetpack\u2122. TensorRT is also integrated with application-specific SDKs, such as NVIDIA DeepStream, NVIDIA Riva, NVIDIA Merlin\u2122, NVIDIA Maxine\u2122, NVIDIA Morpheus, and NVIDIA Broadcast Engine to provide developers with a unified path to deploy intelligent video analytics, speech AI, recommender systems, video conference, AI based cybersecurity, and streaming apps in production.</p> <p>You can use those default TensorRT compute plans for your deployment to get very good performance for NVIDIA hardware.</p> <p>You can also apply quantization for some selected models to get better performance like in HiFiGAN example. This model uses quantization aware training so accuracy is very good but many other models can use post-training quantization by just enabling INT8 flag in optimize function. It can reduce accuracy so you must validate quantized model in such case.</p> <p>Model Navigator can build for your quantized model, when flag <code>INT8</code> is used:</p> <pre><code>package = nav.torch.optimize(\n    model=model,\n    dataloader=dataloader,\n    custom_configs=[\n            nav.TensorRTConfig(precision=nav.api.config.TensorRTPrecision.INT8),\n    ],\n)\n</code></pre> <p>The optimization is executed in Navigator workspace, which by default is <code>navigator_workspace</code> folder.</p> <pre><code>nav.package.save(package=package, path=\"pack_your_model.nav\")\n</code></pre> <p>The function <code>nav.package.save</code> creates Navigator Package. It is a folder in ZIP file with model optimized for inference and optimization results and logs.</p> <p>Navigator package contains: * <code>navigator.log</code> - detailed log from optimization. You can inspect it to find details about errors. * <code>status.yaml</code> - results in easy to parse form, which you can use to integrate Model Navigator in your automation tools. * tensors for inference validation - these are <code>model_input</code> and <code>model_output</code> folders, which provide data to verify model again at different inference solutions. * folders for converted and exported formats - these folders contain format logs and checkpoints for binary formats.</p> <p>TensorRT is binary format so it can be just loaded by library without any included python source code. If you use INT8 precision flag, then <code>trt-int8</code> folder is created, which contains <code>model.plan</code> file with quantized checkpoint for TensorRT.</p> <p>Navigator package can be used to obtain runner and configuration for PyTriton:</p> <pre><code>pytriton_adapter = nav.pytriton.PyTritonAdapter(package=package, strategy=nav.MaxThroughputStrategy())\nrunner = pytriton_adapter.runner\nrunner.activate()\nconfig = pytriton_adapter.config\n</code></pre> <p>or to generate <code>model_repository</code> for Triton Inference Server:</p> <pre><code>nav.triton.model_repository.add_model_from_package(\nmodel_repository_path=pathlib.Path(\"model_repository\"),\nmodel_name=\"model\",\npackage=package,\nstrategy=nav.MaxThroughputStrategy(),\n)\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation","title":"Installation","text":"<p>This section describe how to install the tool. We assume you are comfortable with Python programming language and familiar with Machine Learning models.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>The following prerequisites must be fulfilled to use Triton Model Navigator</p> <ul> <li>Installed Python <code>3.8+</code></li> <li>Installed NVIDIA TensorRT for TensorRT models export.</li> </ul> <p>We recommend to use NGC Containers for PyTorch and TensorFlow which provide have all necessary dependencies:</p> <ul> <li>PyTorch</li> <li>TensorFlow</li> </ul> <p>The library can be installed in:</p> <ul> <li>system environment</li> <li>virtualenv</li> <li>Docker</li> </ul> <p>The NVIDIA optimized Docker images for Python frameworks could be obtained from NVIDIA NGC Catalog.</p> <p>For using NVIDIA optimized Docker images we recommend to install NVIDIA Container Toolkit to run model inference on NVIDIA GPU.</p>"},{"location":"installation/#installation_1","title":"Installation","text":"<p>The package can be installed from <code>pypi.org</code> using extra index url:</p> <pre><code>pip install -U --extra-index-url https://pypi.ngc.nvidia.com triton-model-navigator[&lt;extras,&gt;]\n</code></pre> <p>or with nvidia-pyindex:</p> <pre><code>pip install nvidia-pyindex\npip install -U triton-model-navigator[&lt;extras,&gt;]\n</code></pre> <p>To install Triton Model Navigator from source use pip command:</p> <pre><code>$ pip install --extra-index-url https://pypi.ngc.nvidia.com .[&lt;extras,&gt;]\n</code></pre> <p>Extras:</p> <ul> <li><code>tensorflow</code> - Model Navigator with dependencies for TensorFlow2</li> <li><code>jax</code> - Model Navigator with dependencies for JAX</li> </ul> <p>For using with PyTorch no extras are needed.</p>"},{"location":"installation/#building-the-wheel","title":"Building the wheel","text":"<p>The Triton Model Navigator can be built as wheel. On that purpose the Makefile provide necessary commands.</p> <p>The first is required to install necessary packages to perform build. <pre><code>make install-dev\n</code></pre></p> <p>Once the environment contain required packages run: <pre><code>make dist\n</code></pre></p> <p>The wheel is going to be generated in <code>dist</code> catalog.</p>"},{"location":"known_issues/","title":"Known Issues","text":""},{"location":"known_issues/#known-issues-and-limitations","title":"Known Issues and Limitations","text":"<ul> <li>Source model running in Python can cause OOM issue when GPU memory is larger than CPU RAM memory</li> <li>Verify command could potentially experience CUDA OOM errors while trying to run inference on two models at the same time.</li> </ul>"},{"location":"quick_start/","title":"Quick start","text":""},{"location":"quick_start/#quick-start","title":"Quick Start","text":"<p>Using Model Navigator is as simply as calling <code>optimize</code> with <code>model</code> and <code>dataloader</code>: The <code>optimize</code> function will save all the artifacts it generates in the <code>navigator_workspace</code>.</p> <p>Note: The <code>dataloader</code> is utilized to determine the maximum and minimum shapes of the inputs utilized during model conversions. The <code>Model Navigator</code> employs a single sample from the <code>dataloader</code>, which is then repeated to generate synthetic batches for profiling purposes. Correctness tests are conducted on a subset of the <code>dataloader</code> samples, while verification tests are executed on the entire <code>dataloader</code>.</p> <pre><code>import torch\nimport model_navigator as nav\nnav.torch.optimize(\nmodel=torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True).eval(),\ndataloader=[torch.randn(1, 3, 256, 256) for _ in range(10)],\n)\n</code></pre> <p>The code snippet below demonstrates the usage of the <code>PyTritonAdapter</code> to retrieve the <code>runner</code> and other necessary information. The <code>runner</code> serves as an abstraction that connects the model checkpoint with its runtime, making the inference process more accessible and straightforward. Following that, it initiates the PyTriton server using the provided parameters.</p> <pre><code>pytriton_adapter = nav.pytriton.PyTritonAdapter(package=package, strategy=nav.MaxThroughputStrategy())\nrunner = pytriton_adapter.runner\nrunner.activate()\n@batch\ndef infer_func(**inputs):\nreturn runner.infer(inputs)\nwith Triton() as triton:\ntriton.bind(\nmodel_name=\"resnet50\",\ninfer_func=infer_func,\ninputs=pytriton_adapter.inputs,\noutputs=pytriton_adapter.outputs,\nconfig=pytriton_adapter.config,\n)\ntriton.serve()\n</code></pre> <p>Alternatively, Model Navigator can generate <code>model_repository</code> that can be served on the Triton Inference Server:</p> <pre><code>nav.triton.model_repository.add_model_from_package(\nmodel_repository_path=pathlib.Path(\"model_repository\"),\nmodel_name=\"resnet50\",\npackage=package,\nstrategy=nav.MaxThroughputStrategy(),\n)\n</code></pre> <p>For more information on additional frameworks and <code>optimize</code> function parameters, please refer to the API documentation and examples.</p>"},{"location":"support_matrix/","title":"Support matrix","text":""},{"location":"support_matrix/#support-matrix","title":"Support Matrix","text":"<p>Please find below information about tested models, used environment and libraries.</p>"},{"location":"support_matrix/#verified-models","title":"Verified Models","text":"<p>We have verified that the NVIDIA Model Navigator Optimize API works correctly for the following models.</p> Source Model NVIDIA DeepLearningExamples ResNet50 PyT NVIDIA DeepLearningExamples EfficientNet PyT NVIDIA DeepLearningExamples EfficientNet TF2 NVIDIA DeepLearningExamples BERT TF2 HuggingFace GPT2 Jax HuggingFace GPT2 PyT HuggingFace GPT2 TF2 HuggingFace DistilBERT PyT HuggingFace DistilGPT2 TF2"},{"location":"support_matrix/#third-party-packages","title":"Third-Party Packages","text":"<p>A set of component versions are imposed by the used NGC container. During testing we have used <code>23.03</code> container version that contains:</p> <ul> <li>PyTorch 2.0.0a0+1767026</li> <li>TensorFlow 2.11.0</li> <li>TensorRT 8.5.3.1</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.44.2</li> <li>GraphSurgeon: 0.3.26</li> <li> <p>tf2onnx v1.14.0</p> </li> <li> <p>Refer to the containers support matrix for a detailed summary for each version.</p> </li> </ul>"},{"location":"api/config/","title":"Config","text":"<p>Classes, enums and types used to configure Model Navigator.</p>"},{"location":"api/config/#model_navigator.api.config","title":"<code>model_navigator.api.config</code>","text":"<p>Definition of enums and classes representing configuration for Model Navigator.</p>"},{"location":"api/config/#model_navigator.api.config.CustomConfig","title":"<code>CustomConfig</code>","text":"<p>         Bases: <code>abc.ABC</code></p> <p>Base class used for custom configs. Input for Model Navigator <code>optimize</code> method.</p>"},{"location":"api/config/#model_navigator.api.config.CustomConfig.defaults","title":"<code>defaults()</code>","text":"<p>Update parameters to defaults.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def defaults(self) -&gt; None:\n\"\"\"Update parameters to defaults.\"\"\"\nreturn None\n</code></pre>"},{"location":"api/config/#model_navigator.api.config.CustomConfig.from_dict","title":"<code>from_dict(config_dict)</code>  <code>classmethod</code>","text":"<p>Instantiate CustomConfig from a dictionary.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: Dict[str, Any]) -&gt; \"CustomConfig\":\n\"\"\"Instantiate CustomConfig from a dictionary.\"\"\"\nreturn cls(**config_dict)\n</code></pre>"},{"location":"api/config/#model_navigator.api.config.CustomConfig.name","title":"<code>name()</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Name of the CustomConfig.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\n@abc.abstractmethod\ndef name(cls) -&gt; str:\n\"\"\"Name of the CustomConfig.\"\"\"\nraise NotImplementedError()\n</code></pre>"},{"location":"api/config/#model_navigator.api.config.CustomConfigForFormat","title":"<code>CustomConfigForFormat</code>","text":"<p>         Bases: <code>DataObject</code>, <code>CustomConfig</code></p> <p>Abstract base class used for custom configs representing particular format.</p>"},{"location":"api/config/#model_navigator.api.config.CustomConfigForFormat.format","title":"<code>format: Format</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Format represented by CustomConfig.</p>"},{"location":"api/config/#model_navigator.api.config.DeviceKind","title":"<code>DeviceKind</code>","text":"<p>         Bases: <code>Enum</code></p> <p>Supported types of devices.</p> <p>Parameters:</p> Name Type Description Default <code>CPU</code> <code>str</code> <p>Select CPU device.</p> required <code>GPU</code> <code>str</code> <p>Select GPU with CUDA support.</p> required"},{"location":"api/config/#model_navigator.api.config.Format","title":"<code>Format</code>","text":"<p>         Bases: <code>Enum</code></p> <p>All model formats supported by Model Navigator 'optimize' function.</p> <p>Parameters:</p> Name Type Description Default <code>PYTHON</code> <code>str</code> <p>Format indicating any model defined in Python.</p> required <code>TORCH</code> <code>str</code> <p>Format indicating PyTorch model.</p> required <code>TENSORFLOW</code> <code>str</code> <p>Format indicating TensorFlow model.</p> required <code>JAX</code> <code>str</code> <p>Format indicating JAX model.</p> required <code>TORCHSCRIPT</code> <code>str</code> <p>Format indicating TorchScript model.</p> required <code>TF_SAVEDMODEL</code> <code>str</code> <p>Format indicating TensorFlow SavedModel.</p> required <code>TF_TRT</code> <code>str</code> <p>Format indicating TensorFlow TensorRT model.</p> required <code>TORCH_TRT</code> <code>str</code> <p>Format indicating PyTorch TensorRT model.</p> required <code>ONNX</code> <code>str</code> <p>Format indicating ONNX model.</p> required <code>TENSORRT</code> <code>str</code> <p>Format indicating TensorRT model.</p> required"},{"location":"api/config/#model_navigator.api.config.JitType","title":"<code>JitType</code>","text":"<p>         Bases: <code>Enum</code></p> <p>TorchScript export paramter.</p> <p>Used for selecting the type of TorchScript export.</p> <p>Parameters:</p> Name Type Description Default <code>TRACE</code> <code>str</code> <p>Use tracing during export.</p> required <code>SCRIPT</code> <code>str</code> <p>Use scripting during export.</p> required"},{"location":"api/config/#model_navigator.api.config.MeasurementMode","title":"<code>MeasurementMode</code>","text":"<p>         Bases: <code>Enum</code></p> <p>Profiler measurement mode.</p> <p>Parameters:</p> Name Type Description Default <code>TIME_WINDOWS</code> <code>str</code> <p>mode run measurement windows with fixed time length.</p> required <code>COUNT_WINDOWS</code> <code>str</code> <p>mode run measurement windows with fixed number of requests.</p> required"},{"location":"api/config/#model_navigator.api.config.OnnxConfig","title":"<code>OnnxConfig</code>  <code>dataclass</code>","text":"<p>         Bases: <code>CustomConfigForFormat</code></p> <p>ONNX custom config used for ONNX export and conversion.</p> <p>Parameters:</p> Name Type Description Default <code>opset</code> <code>Optional[int]</code> <p>ONNX opset used for conversion.</p> <code>DEFAULT_ONNX_OPSET</code> <code>dynamic_axes</code> <code>Optional[Dict[str, Union[Dict[int, str], List[int]]]]</code> <p>Dynamic axes for ONNX conversion.</p> <code>None</code> <code>onnx_extended_conversion</code> <code>bool</code> <p>Enables additional conversions from TorchScript to ONNX.</p> <code>False</code>"},{"location":"api/config/#model_navigator.api.config.OnnxConfig.format","title":"<code>format: Format</code>  <code>property</code>","text":"<p>Returns Format.ONNX.</p> <p>Returns:</p> Type Description <code>Format</code> <p>Format.ONNX</p>"},{"location":"api/config/#model_navigator.api.config.OnnxConfig.name","title":"<code>name()</code>  <code>classmethod</code>","text":"<p>Name of the config.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n\"\"\"Name of the config.\"\"\"\nreturn \"Onnx\"\n</code></pre>"},{"location":"api/config/#model_navigator.api.config.ProfilerConfig","title":"<code>ProfilerConfig</code>  <code>dataclass</code>","text":"<p>         Bases: <code>DataObject</code></p> <p>Profiler configuration.</p> <p>For each batch size profiler will run measurments in windows. Depending on the measurement mode, each window will have fixed time length (MeasurementMode.TIME_WINDOWS) or fixed number of requests (MeasurementMode.COUNT_WINDOWS). Batch sizes are profiled in the ascending order.</p> <p>Profiler will run multiple trials and will stop when the measurements are stable (within <code>stability_percentage</code> from the mean) within three consecutive windows. If the measurements are not stable after <code>max_trials</code> trials, the profiler will stop with an error. Profiler will also stop profiling when the throughput does not increase at least by <code>throughput_cutoff_threshold</code>.</p> <p>Parameters:</p> Name Type Description Default <code>run_profiling</code> <code>bool</code> <p>If True, run profiling, otherwise skip profiling.</p> <code>True</code> <code>batch_sizes</code> <code>Optional[List[Union[int, None]]]</code> <p>List of batch sizes to profile. None means that the model does not support batching.</p> <code>None</code> <code>measurement_mode</code> <code>MeasurementMode</code> <p>Measurement mode.</p> <code>MeasurementMode.COUNT_WINDOWS</code> <code>measurement_interval</code> <code>Optional[float]</code> <p>Measurement interval in milliseconds. Used only in MeasurementMode.TIME_WINDOWS mode.</p> <code>5000</code> <code>measurement_request_count</code> <code>Optional[int]</code> <p>Number of requests to measure in each window. Used only in MeasurementMode.COUNT_WINDOWS mode.</p> <code>50</code> <code>stability_percentage</code> <code>float</code> <p>Allowed percentage of variation from the mean in three consecutive windows.</p> <code>10.0</code> <code>max_trials</code> <code>int</code> <p>Maximum number of window trials.</p> <code>10</code> <code>throughput_cutoff_threshold</code> <code>float</code> <p>Minimum throughput increase to continue profiling.</p> <code>DEFAULT_PROFILING_THROUGHPUT_CUTOFF_THRESHOLD</code>"},{"location":"api/config/#model_navigator.api.config.ProfilerConfig.from_dict","title":"<code>from_dict(profiler_config_dict)</code>  <code>classmethod</code>","text":"<p>Instantiate ProfilerConfig class from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>profiler_config_dict</code> <code>Mapping</code> <p>Data dictionary.</p> required <p>Returns:</p> Type Description <code>ProfilerConfig</code> <p>ProfilerConfig</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\ndef from_dict(cls, profiler_config_dict: Mapping) -&gt; \"ProfilerConfig\":\n\"\"\"Instantiate ProfilerConfig class from a dictionary.\n    Args:\n        profiler_config_dict (Mapping): Data dictionary.\n    Returns:\n        ProfilerConfig\n    \"\"\"\nreturn cls(\nrun_profiling=profiler_config_dict.get(\"run_profiling\", True),\nbatch_sizes=profiler_config_dict.get(\"batch_sizes\"),\nmeasurement_interval=profiler_config_dict.get(\"measurement_interval\"),\nmeasurement_mode=MeasurementMode(\nprofiler_config_dict.get(\"measurement_mode\", MeasurementMode.TIME_WINDOWS)\n),\nmeasurement_request_count=profiler_config_dict.get(\"measurement_request_count\"),\nstability_percentage=profiler_config_dict.get(\"stability_percentage\", 10.0),\nmax_trials=profiler_config_dict.get(\"max_trials\", 10),\nthroughput_cutoff_threshold=profiler_config_dict.get(\"throughput_cutoff_threshold\", -2),\n)\n</code></pre>"},{"location":"api/config/#model_navigator.api.config.ShapeTuple","title":"<code>ShapeTuple</code>  <code>dataclass</code>","text":"<p>         Bases: <code>DataObject</code></p> <p>Represents a set of shapes for a single binding in a profile.</p> <p>Each element of the tuple represents a shape for a single dimension of the binding.</p> <p>Parameters:</p> Name Type Description Default <code>min</code> <code>Tuple[int]</code> <p>The minimum shape that the profile will support.</p> required <code>opt</code> <code>Tuple[int]</code> <p>The shape for which TensorRT will optimize the engine.</p> required <code>max</code> <code>Tuple[int]</code> <p>The maximum shape that the profile will support.</p> required"},{"location":"api/config/#model_navigator.api.config.ShapeTuple.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over shapes.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def __iter__(self):\n\"\"\"Iterate over shapes.\"\"\"\nyield from [self.min, self.opt, self.max]\n</code></pre>"},{"location":"api/config/#model_navigator.api.config.ShapeTuple.__repr__","title":"<code>__repr__()</code>","text":"<p>Representation.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def __repr__(self):\n\"\"\"Representation.\"\"\"\nreturn type(self).__name__ + self.__str__()\n</code></pre>"},{"location":"api/config/#model_navigator.api.config.ShapeTuple.__str__","title":"<code>__str__()</code>","text":"<p>String representation.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def __str__(self):\n\"\"\"String representation.\"\"\"\nreturn f\"(min={self.min}, opt={self.opt}, max={self.max})\"\n</code></pre>"},{"location":"api/config/#model_navigator.api.config.SizedIterable","title":"<code>SizedIterable</code>","text":"<p>         Bases: <code>Protocol</code></p> <p>Protocol representing sized iterable. Used by dataloader.</p>"},{"location":"api/config/#model_navigator.api.config.SizedIterable.__iter__","title":"<code>__iter__()</code>","text":"<p>Magic method iter.</p> <p>Returns:</p> Type Description <code>Iterator</code> <p>Iterator to next item.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def __iter__(self) -&gt; Iterator:\n\"\"\"Magic method __iter__.\n    Returns:\n        Iterator to next item.\n    \"\"\"\n...\n</code></pre>"},{"location":"api/config/#model_navigator.api.config.SizedIterable.__len__","title":"<code>__len__()</code>","text":"<p>Magic method len.</p> <p>Returns:</p> Type Description <code>int</code> <p>Length of size iterable.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def __len__(self) -&gt; int:\n\"\"\"Magic method __len__.\n    Returns:\n        Length of size iterable.\n    \"\"\"\n...\n</code></pre>"},{"location":"api/config/#model_navigator.api.config.TensorFlowConfig","title":"<code>TensorFlowConfig</code>  <code>dataclass</code>","text":"<p>         Bases: <code>CustomConfigForFormat</code></p> <p>TensorFlow custom config used for SavedModel export.</p> <p>Parameters:</p> Name Type Description Default <code>jit_compile</code> <code>Tuple[Optional[bool], ...]</code> <p>Enable or Disable jit_compile flag for tf.function wrapper for Jax infer function.</p> <code>(None)</code> <code>enable_xla</code> <code>Tuple[Optional[bool], ...]</code> <p>Enable or Disable enable_xla flag for jax2tf converter.</p> <code>(None)</code>"},{"location":"api/config/#model_navigator.api.config.TensorFlowConfig.format","title":"<code>format: Format</code>  <code>property</code>","text":"<p>Returns Format.TF_SAVEDMODEL.</p> <p>Returns:</p> Type Description <code>Format</code> <p>Format.TF_SAVEDMODEL</p>"},{"location":"api/config/#model_navigator.api.config.TensorFlowConfig.defaults","title":"<code>defaults()</code>","text":"<p>Update parameters to defaults.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def defaults(self) -&gt; None:\n\"\"\"Update parameters to defaults.\"\"\"\nself.jit_compile = (None,)\nself.enable_xla = (None,)\n</code></pre>"},{"location":"api/config/#model_navigator.api.config.TensorFlowConfig.name","title":"<code>name()</code>  <code>classmethod</code>","text":"<p>Name of the config.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n\"\"\"Name of the config.\"\"\"\nreturn \"TensorFlow\"\n</code></pre>"},{"location":"api/config/#model_navigator.api.config.TensorFlowTensorRTConfig","title":"<code>TensorFlowTensorRTConfig</code>  <code>dataclass</code>","text":"<p>         Bases: <code>CustomConfigForFormat</code></p> <p>TensorFlow TensorRT custom config used for TensorRT SavedModel export.</p> <p>Parameters:</p> Name Type Description Default <code>precision</code> <code>Union[Union[str, TensorRTPrecision], Tuple[Union[str, TensorRTPrecision], ...]]</code> <p>TensorRT precision.</p> <code>DEFAULT_TENSORRT_PRECISION</code> <code>max_workspace_size</code> <code>Optional[int]</code> <p>Max workspace size used by converter.</p> <code>DEFAULT_MAX_WORKSPACE_SIZE</code> <code>minimum_segment_size</code> <code>int</code> <p>Min size of subgraph.</p> <code>DEFAULT_MIN_SEGMENT_SIZE</code> <code>trt_profile</code> <code>Optional[TensorRTProfile]</code> <p>TensorRT profile.</p> <code>None</code>"},{"location":"api/config/#model_navigator.api.config.TensorFlowTensorRTConfig.format","title":"<code>format: Format</code>  <code>property</code>","text":"<p>Returns Format.TF_TRT.</p> <p>Returns:</p> Type Description <code>Format</code> <p>Format.TF_TRT</p>"},{"location":"api/config/#model_navigator.api.config.TensorFlowTensorRTConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Parse dataclass enums.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n\"\"\"Parse dataclass enums.\"\"\"\nprecision = (self.precision,) if not isinstance(self.precision, (list, tuple)) else self.precision\nself.precision = tuple(TensorRTPrecision(p) for p in precision)\n</code></pre>"},{"location":"api/config/#model_navigator.api.config.TensorFlowTensorRTConfig.defaults","title":"<code>defaults()</code>","text":"<p>Update parameters to defaults.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def defaults(self) -&gt; None:\n\"\"\"Update parameters to defaults.\"\"\"\nself.precision = tuple(TensorRTPrecision(p) for p in DEFAULT_TENSORRT_PRECISION)\nself.max_workspace_size = DEFAULT_MAX_WORKSPACE_SIZE\nself.minimum_segment_size = DEFAULT_MIN_SEGMENT_SIZE\nself.trt_profile = None\n</code></pre>"},{"location":"api/config/#model_navigator.api.config.TensorFlowTensorRTConfig.from_dict","title":"<code>from_dict(config_dict)</code>  <code>classmethod</code>","text":"<p>Instantiate TensorFlowTensorRTConfig from  adictionary.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: Dict[str, Any]) -&gt; \"TensorFlowTensorRTConfig\":\n\"\"\"Instantiate TensorFlowTensorRTConfig from  adictionary.\"\"\"\nif config_dict.get(\"trt_profile\") is not None and not isinstance(config_dict[\"trt_profile\"], TensorRTProfile):\nconfig_dict[\"trt_profile\"] = TensorRTProfile.from_dict(config_dict[\"trt_profile\"])\nreturn cls(**config_dict)\n</code></pre>"},{"location":"api/config/#model_navigator.api.config.TensorFlowTensorRTConfig.name","title":"<code>name()</code>  <code>classmethod</code>","text":"<p>Name of the config.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n\"\"\"Name of the config.\"\"\"\nreturn \"TensorFlowTensorRT\"\n</code></pre>"},{"location":"api/config/#model_navigator.api.config.TensorRTCompatibilityLevel","title":"<code>TensorRTCompatibilityLevel</code>","text":"<p>         Bases: <code>Enum</code></p> <p>Compatibility level for TensorRT.</p> <p>Parameters:</p> Name Type Description Default <code>AMPERE_PLUS</code> <code>str</code> <p>Support AMPERE plus architecture</p> required"},{"location":"api/config/#model_navigator.api.config.TensorRTConfig","title":"<code>TensorRTConfig</code>  <code>dataclass</code>","text":"<p>         Bases: <code>CustomConfigForFormat</code></p> <p>TensorRT custom config used for TensorRT conversion.</p> <p>Parameters:</p> Name Type Description Default <code>precision</code> <code>Union[Union[str, TensorRTPrecision], Tuple[Union[str, TensorRTPrecision], ...]]</code> <p>TensorRT precision.</p> <code>DEFAULT_TENSORRT_PRECISION</code> <code>max_workspace_size</code> <code>Optional[int]</code> <p>Max workspace size used by converter.</p> <code>DEFAULT_MAX_WORKSPACE_SIZE</code> <code>trt_profile</code> <code>Optional[TensorRTProfile]</code> <p>TensorRT profile.</p> <code>None</code> <code>optimization_level</code> <code>Optional[int]</code> <p>Optimization level for TensorRT conversion. Allowed values are fom 0 to 5. Where default is                 3 based on TensorRT API documentation.</p> <code>None</code>"},{"location":"api/config/#model_navigator.api.config.TensorRTConfig.format","title":"<code>format: Format</code>  <code>property</code>","text":"<p>Returns Format.TENSORRT.</p> <p>Returns:</p> Type Description <code>Format</code> <p>Format.TENSORRT</p>"},{"location":"api/config/#model_navigator.api.config.TensorRTConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Parse dataclass enums.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n\"\"\"Parse dataclass enums.\"\"\"\nself.precision_mode = TensorRTPrecisionMode(self.precision_mode)\nprecision = (self.precision,) if not isinstance(self.precision, (list, tuple)) else self.precision\nself.precision = tuple(TensorRTPrecision(p) for p in precision)\nif self.optimization_level is not None and (self.optimization_level &lt; 0 or self.optimization_level &gt; 5):\nraise ModelNavigatorConfigurationError(\nf\"TensorRT `optimization_level` must be between 0 and 5. Provided value: {self.optimization_level}.\"\n)\n</code></pre>"},{"location":"api/config/#model_navigator.api.config.TensorRTConfig.defaults","title":"<code>defaults()</code>","text":"<p>Update parameters to defaults.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def defaults(self) -&gt; None:\n\"\"\"Update parameters to defaults.\"\"\"\nself.precision = tuple(TensorRTPrecision(p) for p in DEFAULT_TENSORRT_PRECISION)\nself.precision_mode = DEFAULT_TENSORRT_PRECISION_MODE\nself.trt_profile = None\nself.max_workspace_size = DEFAULT_MAX_WORKSPACE_SIZE\nself.optimization_level = None\nself.compatibility_level = None\n</code></pre>"},{"location":"api/config/#model_navigator.api.config.TensorRTConfig.from_dict","title":"<code>from_dict(config_dict)</code>  <code>classmethod</code>","text":"<p>Instantiate TensorRTConfig from  adictionary.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: Dict[str, Any]) -&gt; \"TensorRTConfig\":\n\"\"\"Instantiate TensorRTConfig from  adictionary.\"\"\"\nif config_dict.get(\"trt_profile\") is not None and not isinstance(config_dict[\"trt_profile\"], TensorRTProfile):\nconfig_dict[\"trt_profile\"] = TensorRTProfile.from_dict(config_dict[\"trt_profile\"])\nreturn cls(**config_dict)\n</code></pre>"},{"location":"api/config/#model_navigator.api.config.TensorRTConfig.name","title":"<code>name()</code>  <code>classmethod</code>","text":"<p>Name of the config.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n\"\"\"Name of the config.\"\"\"\nreturn \"TensorRT\"\n</code></pre>"},{"location":"api/config/#model_navigator.api.config.TensorRTPrecision","title":"<code>TensorRTPrecision</code>","text":"<p>         Bases: <code>Enum</code></p> <p>Precisions supported during TensorRT conversions.</p> <p>Parameters:</p> Name Type Description Default <code>INT8</code> <code>str</code> <p>8-bit integer precision.</p> required <code>FP16</code> <code>str</code> <p>16-bit floating point precision.</p> required <code>FP32</code> <code>str</code> <p>32-bit floating point precision.</p> required"},{"location":"api/config/#model_navigator.api.config.TensorRTPrecisionMode","title":"<code>TensorRTPrecisionMode</code>","text":"<p>         Bases: <code>Enum</code></p> <p>Precision modes for TensorRT conversions.</p> <p>Parameters:</p> Name Type Description Default <code>HIERARCHY</code> <code>str</code> <p>Use TensorRT precision hierarchy starting from highest to lowest.</p> required <code>SINGLE</code> <code>str</code> <p>Use single precision.</p> required <code>MIXED</code> <code>str</code> <p>Use mixed precision.</p> required"},{"location":"api/config/#model_navigator.api.config.TensorRTProfile","title":"<code>TensorRTProfile</code>","text":"<p>         Bases: <code>Dict[str, ShapeTuple]</code></p> <p>Single optimization profile that can be used to build an engine.</p> <p>More specifically, it is an <code>Dict[str, ShapeTuple]</code> which maps binding names to a set of min/opt/max shapes.</p>"},{"location":"api/config/#model_navigator.api.config.TensorRTProfile.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Retrieves the shapes registered for a given input name.</p> <p>Returns:</p> Name Type Description <code>ShapeTuple</code> <pre><code>A named tuple including ``min``, ``opt``, and ``max`` members for the shapes\ncorresponding to the input.\n</code></pre> Source code in <code>model_navigator/api/config.py</code> <pre><code>def __getitem__(self, key):\n\"\"\"Retrieves the shapes registered for a given input name.\n    Returns:\n        ShapeTuple:\n                A named tuple including ``min``, ``opt``, and ``max`` members for the shapes\n                corresponding to the input.\n    \"\"\"\nif key not in self:\nLOGGER.error(f\"Binding: {key} does not have shapes set in this profile\")\nreturn super().__getitem__(key)\n</code></pre>"},{"location":"api/config/#model_navigator.api.config.TensorRTProfile.__repr__","title":"<code>__repr__()</code>","text":"<p>Representation.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def __repr__(self):\n\"\"\"Representation.\"\"\"\nret = \"TensorRTProfile()\"\nfor name, (min, opt, max) in self.items():\nret += f\".add('{name}', min={min}, opt={opt}, max={max})\"\nreturn ret\n</code></pre>"},{"location":"api/config/#model_navigator.api.config.TensorRTProfile.__str__","title":"<code>__str__()</code>","text":"<p>String representation.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def __str__(self):\n\"\"\"String representation.\"\"\"\nelems = []\nfor name, (min, opt, max) in self.items():\nelems.append(f\"{name} [min={min}, opt={opt}, max={max}]\")\nsep = \",\\n \"\nreturn \"{\" + sep.join(elems) + \"}\"\n</code></pre>"},{"location":"api/config/#model_navigator.api.config.TensorRTProfile.add","title":"<code>add(name, min, opt, max)</code>","text":"<p>A convenience function to add shapes for a single binding.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the binding.</p> required <code>min</code> <code>Tuple[int]</code> <p>The minimum shape that the profile will support.</p> required <code>opt</code> <code>Tuple[int]</code> <p>The shape for which TensorRT will optimize the engine.</p> required <code>max</code> <code>Tuple[int]</code> <p>The maximum shape that the profile will support.</p> required <p>Returns:</p> Name Type Description <code>Profile</code> <p>self, which allows this function to be easily chained to add multiple bindings, e.g., TensorRTProfile().add(...).add(...)</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def add(self, name, min, opt, max):\n\"\"\"A convenience function to add shapes for a single binding.\n    Args:\n        name (str): The name of the binding.\n        min (Tuple[int]): The minimum shape that the profile will support.\n        opt (Tuple[int]): The shape for which TensorRT will optimize the engine.\n        max (Tuple[int]): The maximum shape that the profile will support.\n    Returns:\n        Profile:\n            self, which allows this function to be easily chained to add multiple bindings,\n            e.g., TensorRTProfile().add(...).add(...)\n    \"\"\"\nself[name] = ShapeTuple(min, opt, max)\nreturn self\n</code></pre>"},{"location":"api/config/#model_navigator.api.config.TensorRTProfile.from_dict","title":"<code>from_dict(profile_dict)</code>  <code>classmethod</code>","text":"<p>Create a TensorRTProfile from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>profile_dict</code> <code>Dict[str, Dict[str, Tuple[int, ...]]]</code> <p>A dictionary mapping binding names to a dictionary containing <code>min</code>, <code>opt</code>, and <code>max</code> keys.</p> required <p>Returns:</p> Name Type Description <code>TensorRTProfile</code> <p>A TensorRTProfile object.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\ndef from_dict(cls, profile_dict: Dict[str, Dict[str, Tuple[int, ...]]]):\n\"\"\"Create a TensorRTProfile from a dictionary.\n    Args:\n        profile_dict (Dict[str, Dict[str, Tuple[int, ...]]]):\n            A dictionary mapping binding names to a dictionary containing ``min``, ``opt``, and\n            ``max`` keys.\n    Returns:\n        TensorRTProfile:\n            A TensorRTProfile object.\n    \"\"\"\nreturn cls({name: ShapeTuple(**shapes) for name, shapes in profile_dict.items()})\n</code></pre>"},{"location":"api/config/#model_navigator.api.config.TorchConfig","title":"<code>TorchConfig</code>  <code>dataclass</code>","text":"<p>         Bases: <code>CustomConfigForFormat</code></p> <p>Torch custom config used for TorchScript export.</p> <p>Parameters:</p> Name Type Description Default <code>jit_type</code> <code>Union[Union[str, JitType], Tuple[Union[str, JitType], ...]]</code> <p>Type of TorchScript export.</p> <code>(JitType.SCRIPT, JitType.TRACE)</code> <code>strict</code> <code>bool</code> <p>Enable or Disable strict flag for tracer used in TorchScript export, default: True.</p> <code>True</code>"},{"location":"api/config/#model_navigator.api.config.TorchConfig.format","title":"<code>format: Format</code>  <code>property</code>","text":"<p>Returns Format.TORCHSCRIPT.</p> <p>Returns:</p> Type Description <code>Format</code> <p>Format.TORCHSCRIPT</p>"},{"location":"api/config/#model_navigator.api.config.TorchConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Parse dataclass enums.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n\"\"\"Parse dataclass enums.\"\"\"\njit_type = (self.jit_type,) if not isinstance(self.jit_type, (list, tuple)) else self.jit_type\nself.jit_type = tuple(JitType(j) for j in jit_type)\n</code></pre>"},{"location":"api/config/#model_navigator.api.config.TorchConfig.defaults","title":"<code>defaults()</code>","text":"<p>Update parameters to defaults.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def defaults(self) -&gt; None:\n\"\"\"Update parameters to defaults.\"\"\"\nself.jit_type = (JitType.SCRIPT, JitType.TRACE)\nself.strict = True\n</code></pre>"},{"location":"api/config/#model_navigator.api.config.TorchConfig.name","title":"<code>name()</code>  <code>classmethod</code>","text":"<p>Name of the config.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n\"\"\"Name of the config.\"\"\"\nreturn \"Torch\"\n</code></pre>"},{"location":"api/config/#model_navigator.api.config.TorchTensorRTConfig","title":"<code>TorchTensorRTConfig</code>  <code>dataclass</code>","text":"<p>         Bases: <code>CustomConfigForFormat</code></p> <p>Torch custom config used for TensorRT TorchScript conversion.</p> <p>Parameters:</p> Name Type Description Default <code>precision</code> <code>Union[Union[str, TensorRTPrecision], Tuple[Union[str, TensorRTPrecision], ...]]</code> <p>TensorRT precision.</p> <code>DEFAULT_TENSORRT_PRECISION</code> <code>max_workspace_size</code> <code>Optional[int]</code> <p>Max workspace size used by converter.</p> <code>DEFAULT_MAX_WORKSPACE_SIZE</code> <code>trt_profile</code> <code>Optional[TensorRTProfile]</code> <p>TensorRT profile.</p> <code>None</code>"},{"location":"api/config/#model_navigator.api.config.TorchTensorRTConfig.format","title":"<code>format: Format</code>  <code>property</code>","text":"<p>Returns Format.TORCH_TRT.</p> <p>Returns:</p> Type Description <code>Format</code> <p>Format.TORCH_TRT</p>"},{"location":"api/config/#model_navigator.api.config.TorchTensorRTConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Parse dataclass enums.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n\"\"\"Parse dataclass enums.\"\"\"\nprecision = (self.precision,) if not isinstance(self.precision, (list, tuple)) else self.precision\nself.precision = tuple(TensorRTPrecision(p) for p in precision)\nself.precision_mode = TensorRTPrecisionMode(self.precision_mode)\n</code></pre>"},{"location":"api/config/#model_navigator.api.config.TorchTensorRTConfig.defaults","title":"<code>defaults()</code>","text":"<p>Update parameters to defaults.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def defaults(self) -&gt; None:\n\"\"\"Update parameters to defaults.\"\"\"\nself.precision = tuple(TensorRTPrecision(p) for p in DEFAULT_TENSORRT_PRECISION)\nself.precision_mode = DEFAULT_TENSORRT_PRECISION_MODE\nself.trt_profile = None\nself.max_workspace_size = DEFAULT_MAX_WORKSPACE_SIZE\n</code></pre>"},{"location":"api/config/#model_navigator.api.config.TorchTensorRTConfig.from_dict","title":"<code>from_dict(config_dict)</code>  <code>classmethod</code>","text":"<p>Instantiate TorchTensorRTConfig from  adictionary.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: Dict[str, Any]) -&gt; \"TorchTensorRTConfig\":\n\"\"\"Instantiate TorchTensorRTConfig from  adictionary.\"\"\"\nif config_dict.get(\"trt_profile\") is not None and not isinstance(config_dict[\"trt_profile\"], TensorRTProfile):\nconfig_dict[\"trt_profile\"] = TensorRTProfile.from_dict(config_dict[\"trt_profile\"])\nreturn cls(**config_dict)\n</code></pre>"},{"location":"api/config/#model_navigator.api.config.TorchTensorRTConfig.name","title":"<code>name()</code>  <code>classmethod</code>","text":"<p>Name of the config.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n\"\"\"Name of the config.\"\"\"\nreturn \"TorchTensorRT\"\n</code></pre>"},{"location":"api/config/#model_navigator.api.config.map_custom_configs","title":"<code>map_custom_configs(custom_configs)</code>","text":"<p>Map custom configs from list to dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>custom_configs</code> <code>Optional[Sequence[CustomConfig]]</code> <p>List of custom configs passed to API method</p> required <p>Returns:</p> Type Description <code>Dict</code> <p>Mapped configs to dictionary</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def map_custom_configs(custom_configs: Optional[Sequence[CustomConfig]]) -&gt; Dict:\n\"\"\"Map custom configs from list to dictionary.\n    Args:\n        custom_configs: List of custom configs passed to API method\n    Returns:\n        Mapped configs to dictionary\n    \"\"\"\nif not custom_configs:\nreturn {}\nreturn {config.name(): config for config in custom_configs}\n</code></pre>"},{"location":"api/config/#model_navigator.api.MaxThroughputAndMinLatencyStrategy","title":"<code>model_navigator.api.MaxThroughputAndMinLatencyStrategy</code>","text":"<p>         Bases: <code>RuntimeSearchStrategy</code></p> <p>Get runtime with the highest throughput and the lowest latency.</p>"},{"location":"api/config/#model_navigator.api.MaxThroughputStrategy","title":"<code>model_navigator.api.MaxThroughputStrategy</code>","text":"<p>         Bases: <code>RuntimeSearchStrategy</code></p> <p>Get runtime with the highest throughput.</p>"},{"location":"api/config/#model_navigator.api.MinLatencyStrategy","title":"<code>model_navigator.api.MinLatencyStrategy</code>","text":"<p>         Bases: <code>RuntimeSearchStrategy</code></p> <p>Get runtime with the lowest latency.</p>"},{"location":"api/jax/","title":"JAX","text":""},{"location":"api/jax/#model_navigator.api.jax","title":"<code>model_navigator.api.jax</code>","text":"<p>JAX optimize API.</p>"},{"location":"api/jax/#model_navigator.api.jax.optimize","title":"<code>optimize(model, model_params, dataloader, sample_count=DEFAULT_SAMPLE_COUNT, batching=True, target_formats=None, target_device=DeviceKind.CUDA, runners=None, profiler_config=None, workspace=None, verbose=False, debug=False, verify_func=None, custom_configs=None)</code>","text":"<p>Entry point for JAX optimize.</p> <p>Perform export, conversion, correctness testing, profiling and model verification.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Callable</code> <p>JAX forward function</p> required <code>model_params</code> <code>Any</code> <p>JAX model parameters (weights)</p> required <code>dataloader</code> <code>SizedDataLoader</code> <p>Sized iterable with data that will be feed to the model</p> required <code>sample_count</code> <code>int</code> <p>Limits how many samples will be used from dataloader</p> <code>DEFAULT_SAMPLE_COUNT</code> <code>batching</code> <code>Optional[bool]</code> <p>Enable or disable batching on first (index 0) dimension of the model</p> <code>True</code> <code>target_formats</code> <code>Optional[Union[Union[str, Format], Tuple[Union[str, Format], ...]]]</code> <p>Target model formats for optimize process</p> <code>None</code> <code>target_device</code> <code>Optional[DeviceKind]</code> <p>Target device for optimize process, default is CUDA</p> <code>DeviceKind.CUDA</code> <code>runners</code> <code>Optional[Union[Union[str, Type[NavigatorRunner]], Tuple[Union[str, Type[NavigatorRunner]], ...]]]</code> <p>Use only runners provided as paramter</p> <code>None</code> <code>profiler_config</code> <code>Optional[ProfilerConfig]</code> <p>Profiling config</p> <code>None</code> <code>workspace</code> <code>Optional[Path]</code> <p>Workspace where packages will be extracted</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Enable verbose logging</p> <code>False</code> <code>debug</code> <code>bool</code> <p>Enable debug logging from commands</p> <code>False</code> <code>verify_func</code> <code>Optional[VerifyFunction]</code> <p>Function for additional model verifcation</p> <code>None</code> <code>custom_configs</code> <code>Optional[Sequence[CustomConfig]]</code> <p>Sequence of CustomConfigs used to control produced artifacts</p> <code>None</code> <p>Returns:</p> Type Description <code>Package</code> <p>Package descriptor representing created package.</p> Source code in <code>model_navigator/api/jax.py</code> <pre><code>def optimize(\nmodel: Callable,\nmodel_params: Any,\ndataloader: SizedDataLoader,\nsample_count: int = DEFAULT_SAMPLE_COUNT,\nbatching: Optional[bool] = True,\ntarget_formats: Optional[Union[Union[str, Format], Tuple[Union[str, Format], ...]]] = None,\ntarget_device: Optional[DeviceKind] = DeviceKind.CUDA,\nrunners: Optional[Union[Union[str, Type[NavigatorRunner]], Tuple[Union[str, Type[NavigatorRunner]], ...]]] = None,\nprofiler_config: Optional[ProfilerConfig] = None,\nworkspace: Optional[Path] = None,\nverbose: bool = False,\ndebug: bool = False,\nverify_func: Optional[VerifyFunction] = None,\ncustom_configs: Optional[Sequence[CustomConfig]] = None,\n) -&gt; Package:\n\"\"\"Entry point for JAX optimize.\n    Perform export, conversion, correctness testing, profiling and model verification.\n    Args:\n        model: JAX forward function\n        model_params: JAX model parameters (weights)\n        dataloader: Sized iterable with data that will be feed to the model\n        sample_count: Limits how many samples will be used from dataloader\n        batching: Enable or disable batching on first (index 0) dimension of the model\n        target_formats: Target model formats for optimize process\n        target_device: Target device for optimize process, default is CUDA\n        runners: Use only runners provided as paramter\n        profiler_config: Profiling config\n        workspace: Workspace where packages will be extracted\n        verbose: Enable verbose logging\n        debug: Enable debug logging from commands\n        verify_func: Function for additional model verifcation\n        custom_configs: Sequence of CustomConfigs used to control produced artifacts\n    Returns:\n        Package descriptor representing created package.\n    \"\"\"\nif target_device == DeviceKind.CPU and any(\ndevice.device_type == \"GPU\" for device in tensorflow.config.get_visible_devices()\n):\nraise ModelNavigatorConfigurationError(\n\"\\n\"\n\"    'target_device == nav.DeviceKind.CPU' is not supported for TensorFlow2 \"\n\"(exported from JAX) when GPU is available.\\n\"\n\"    To optimize model for CPU, disable GPU with: \"\n\"'tf.config.set_visible_devices([], 'GPU')' directly after importing TensorFlow.\\n\"\n)\nif isinstance(model, str):\nmodel = Path(model)\nif workspace is None:\nworkspace = get_default_workspace()\nif target_formats is None:\ntarget_formats = DEFAULT_JAX_TARGET_FORMATS\nsample = next(iter(dataloader))\nforward_kw_names = tuple(sample.keys()) if isinstance(sample, Mapping) else None\nif runners is None:\nrunners = default_runners(device_kind=target_device)\nif profiler_config is None:\nprofiler_config = ProfilerConfig()\ntarget_formats_enums = enums.parse(target_formats, Format)\nrunner_names = enums.parse(runners, lambda runner: runner if isinstance(runner, str) else runner.name())\nif Format.JAX not in target_formats_enums:\ntarget_formats_enums = (Format.JAX,) + target_formats_enums\nconfig = CommonConfig(\nFramework.JAX,\nmodel=JaxModel(model=model, params=model_params),\ndataloader=dataloader,\nforward_kw_names=forward_kw_names,\nworkspace=workspace,\ntarget_formats=target_formats_enums,\ntarget_device=target_device,\nsample_count=sample_count,\nbatch_dim=0 if batching else None,\nrunner_names=runner_names,\nprofiler_config=profiler_config,\nverbose=verbose,\ndebug=debug,\nverify_func=verify_func,\ncustom_configs=map_custom_configs(custom_configs=custom_configs),\n)\nmodels_config = ModelConfigBuilder.generate_model_config(\nframework=Framework.JAX,\ntarget_formats=target_formats_enums,\ncustom_configs=custom_configs,\n)\nbuilders = [\npreprocessing_builder,\njax_export_builder,\nfind_device_max_batch_size_builder,\ntensorflow_conversion_builder,\ncorrectness_builder,\n]\nif profiler_config.run_profiling:\nbuilders.append(profiling_builder)\nbuilders.append(verify_builder)\npackage = PipelineManager.run(\npipeline_builders=builders,\nconfig=config,\nmodels_config=models_config,\n)\nreturn package\n</code></pre>"},{"location":"api/onnx/","title":"ONNX","text":""},{"location":"api/onnx/#model_navigator.api.onnx","title":"<code>model_navigator.api.onnx</code>","text":"<p>ONNX optimize API.</p>"},{"location":"api/onnx/#model_navigator.api.onnx.optimize","title":"<code>optimize(model, dataloader, sample_count=DEFAULT_SAMPLE_COUNT, batching=True, target_formats=None, target_device=DeviceKind.CUDA, runners=None, profiler_config=None, workspace=None, verbose=False, debug=False, verify_func=None, custom_configs=None)</code>","text":"<p>Entrypoint for ONNX optimize.</p> <p>Perform conversion, correctness testing, profiling and model verification.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[Path, str]</code> <p>ONNX model path or string</p> required <code>dataloader</code> <code>SizedDataLoader</code> <p>Sized iterable with data that will be feed to the model</p> required <code>sample_count</code> <code>int</code> <p>Limits how many samples will be used from dataloader</p> <code>DEFAULT_SAMPLE_COUNT</code> <code>batching</code> <code>Optional[bool]</code> <p>Enable or disable batching on first (index 0) dimension of the model</p> <code>True</code> <code>target_formats</code> <code>Optional[Union[Union[str, Format], Tuple[Union[str, Format], ...]]]</code> <p>Target model formats for optimize process</p> <code>None</code> <code>target_device</code> <code>Optional[DeviceKind]</code> <p>Target device for optimize process, default is CUDA</p> <code>DeviceKind.CUDA</code> <code>runners</code> <code>Optional[Union[Union[str, Type[NavigatorRunner]], Tuple[Union[str, Type[NavigatorRunner]], ...]]]</code> <p>Use only runners provided as parameter</p> <code>None</code> <code>profiler_config</code> <code>Optional[ProfilerConfig]</code> <p>Profiling config</p> <code>None</code> <code>workspace</code> <code>Optional[Path]</code> <p>Workspace where packages will be extracted</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Enable verbose logging</p> <code>False</code> <code>debug</code> <code>bool</code> <p>Enable debug logging from commands</p> <code>False</code> <code>verify_func</code> <code>Optional[VerifyFunction]</code> <p>Function for additional model verification</p> <code>None</code> <code>custom_configs</code> <code>Optional[Sequence[CustomConfig]]</code> <p>Sequence of CustomConfigs used to control produced artifacts</p> <code>None</code> <p>Returns:</p> Type Description <code>Package</code> <p>Package descriptor representing created package.</p> Source code in <code>model_navigator/api/onnx.py</code> <pre><code>def optimize(\nmodel: Union[Path, str],\ndataloader: SizedDataLoader,\nsample_count: int = DEFAULT_SAMPLE_COUNT,\nbatching: Optional[bool] = True,\ntarget_formats: Optional[Union[Union[str, Format], Tuple[Union[str, Format], ...]]] = None,\ntarget_device: Optional[DeviceKind] = DeviceKind.CUDA,\nrunners: Optional[Union[Union[str, Type[NavigatorRunner]], Tuple[Union[str, Type[NavigatorRunner]], ...]]] = None,\nprofiler_config: Optional[ProfilerConfig] = None,\nworkspace: Optional[Path] = None,\nverbose: bool = False,\ndebug: bool = False,\nverify_func: Optional[VerifyFunction] = None,\ncustom_configs: Optional[Sequence[CustomConfig]] = None,\n) -&gt; Package:\n\"\"\"Entrypoint for ONNX optimize.\n    Perform conversion, correctness testing, profiling and model verification.\n    Args:\n        model: ONNX model path or string\n        dataloader: Sized iterable with data that will be feed to the model\n        sample_count: Limits how many samples will be used from dataloader\n        batching: Enable or disable batching on first (index 0) dimension of the model\n        target_formats: Target model formats for optimize process\n        target_device: Target device for optimize process, default is CUDA\n        runners: Use only runners provided as parameter\n        profiler_config: Profiling config\n        workspace: Workspace where packages will be extracted\n        verbose: Enable verbose logging\n        debug: Enable debug logging from commands\n        verify_func: Function for additional model verification\n        custom_configs: Sequence of CustomConfigs used to control produced artifacts\n    Returns:\n        Package descriptor representing created package.\n    \"\"\"\nif isinstance(model, str):\nmodel = Path(model)\nif workspace is None:\nworkspace = get_default_workspace()\nif target_formats is None:\ntarget_formats = DEFAULT_ONNX_TARGET_FORMATS\nif runners is None:\nrunners = default_runners(device_kind=target_device)\nif profiler_config is None:\nprofiler_config = ProfilerConfig()\ntarget_formats_enums = enums.parse(target_formats, Format)\nrunner_names = enums.parse(runners, lambda runner: runner if isinstance(runner, str) else runner.name())\nif Format.ONNX not in target_formats_enums:\ntarget_formats_enums = (Format.ONNX,) + target_formats_enums\nconfig = CommonConfig(\nFramework.ONNX,\nmodel=model,\ndataloader=dataloader,\nworkspace=workspace,\ntarget_formats=target_formats_enums,\ntarget_device=target_device,\nsample_count=sample_count,\nbatch_dim=0 if batching else None,\nrunner_names=runner_names,\nprofiler_config=profiler_config,\nverbose=verbose,\ndebug=debug,\nverify_func=verify_func,\ncustom_configs=map_custom_configs(custom_configs=custom_configs),\n)\nmodels_config = ModelConfigBuilder.generate_model_config(\nframework=Framework.ONNX,\ntarget_formats=target_formats_enums,\ncustom_configs=custom_configs,\n)\nbuilders = [\npreprocessing_builder,\nonnx_export_builder,\nfind_device_max_batch_size_builder,\nonnx_conversion_builder,\ncorrectness_builder,\n]\nif profiler_config.run_profiling:\nbuilders.append(profiling_builder)\nbuilders.append(verify_builder)\npackage = PipelineManager.run(\npipeline_builders=builders,\nconfig=config,\nmodels_config=models_config,\n)\nreturn package\n</code></pre>"},{"location":"api/package/","title":"Package","text":""},{"location":"api/package/#model_navigator.api.package","title":"<code>model_navigator.api.package</code>","text":"<p>Package operations related API.</p>"},{"location":"api/package/#model_navigator.api.package.get_best_model_status","title":"<code>get_best_model_status(package, strategy=None, include_source=True)</code>","text":"<p>Returns ModelStatus of best model for given strategy.</p> <p>If model with given strategy cannot be found, search is repeated with MaxThroughputStrategy. If there is no model match given strategy or MaxThroughputStrategy, function returns None.</p> <p>Parameters:</p> Name Type Description Default <code>package</code> <code>Package</code> <p>A package object to be searched for best model.</p> required <code>strategy</code> <code>Optional[RuntimeSearchStrategy]</code> <p>Strategy for finding the best model. Defaults to <code>MaxThroughputAndMinLatencyStrategy</code></p> <code>None</code> <code>include_source</code> <code>bool</code> <p>Flag if Python based model has to be included in analysis</p> <code>True</code> <p>Returns:</p> Type Description <code>Optional[ModelStatus]</code> <p>ModelStatus of best model for given strategy or None.</p> Source code in <code>model_navigator/api/package.py</code> <pre><code>def get_best_model_status(\npackage: Package,\nstrategy: Optional[RuntimeSearchStrategy] = None,\ninclude_source: bool = True,\n) -&gt; Optional[ModelStatus]:\n\"\"\"Returns ModelStatus of best model for given strategy.\n    If model with given strategy cannot be found, search is repeated with MaxThroughputStrategy.\n    If there is no model match given strategy or MaxThroughputStrategy, function returns None.\n    Args:\n        package: A package object to be searched for best model.\n        strategy: Strategy for finding the best model. Defaults to `MaxThroughputAndMinLatencyStrategy`\n        include_source: Flag if Python based model has to be included in analysis\n    Returns:\n        ModelStatus of best model for given strategy or None.\n    \"\"\"\nreturn package.get_best_model_status(strategy=strategy, include_source=include_source)\n</code></pre>"},{"location":"api/package/#model_navigator.api.package.load","title":"<code>load(path, workspace=None)</code>","text":"<p>Load package from provided path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>The location of package to load</p> required <code>workspace</code> <code>Optional[Union[str, Path]]</code> <p>Workspace where packages will be extracted</p> <code>None</code> <p>Returns:</p> Type Description <code>Package</code> <p>Package.</p> Source code in <code>model_navigator/api/package.py</code> <pre><code>def load(\npath: Union[str, Path],\nworkspace: Optional[Union[str, Path]] = None,\n) -&gt; Package:\n\"\"\"Load package from provided path.\n    Args:\n        path: The location of package to load\n        workspace: Workspace where packages will be extracted\n    Returns:\n        Package.\n    \"\"\"\nreturn Package.load(path=path, workspace=workspace)\n</code></pre>"},{"location":"api/package/#model_navigator.api.package.optimize","title":"<code>optimize(package, target_formats=None, target_device=DeviceKind.CUDA, runners=None, profiler_config=None, verbose=False, debug=False, verify_func=None, custom_configs=None, defaults=True)</code>","text":"<p>Generate target formats and run correctness and profiling tests for available runners.</p> <p>Parameters:</p> Name Type Description Default <code>package</code> <code>Package</code> <p>Package to optimize.</p> required <code>target_formats</code> <code>Optional[Union[Union[str, Format], Tuple[Union[str, Format], ...]]]</code> <p>Formats to generate and profile. Defaults to target formats from the package.</p> <code>None</code> <code>target_device</code> <code>Optional[DeviceKind]</code> <p>Target device for optimize process, default is CUDA</p> <code>DeviceKind.CUDA</code> <code>runners</code> <code>Optional[Union[Union[str, Type[NavigatorRunner]], Tuple[Union[str, Type[NavigatorRunner]], ...]]]</code> <p>Runners to run correctness tests and profiling on. Defaults to runners from the package.</p> <code>None</code> <code>profiler_config</code> <code>Optional[ProfilerConfig]</code> <p>Configuration of the profiler. Defaults to config from the package.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>If True enable verbose logging. Defaults to False.</p> <code>False</code> <code>debug</code> <code>bool</code> <p>If True print debugging logs. Defaults to False.</p> <code>False</code> <code>verify_func</code> <code>Optional[VerifyFunction]</code> <p>Function used for verifying generated models. Defaults to None.</p> <code>None</code> <code>custom_configs</code> <code>Optional[List[CustomConfig]]</code> <p>Custom formats configuration. Defaults to None.</p> <code>None</code> <code>defaults</code> <code>bool</code> <p>reset configuration of custom configs to defaults</p> <code>True</code> <p>Returns:</p> Type Description <code>Package</code> <p>Optimized package</p> Source code in <code>model_navigator/api/package.py</code> <pre><code>def optimize(\npackage: Package,\ntarget_formats: Optional[Union[Union[str, Format], Tuple[Union[str, Format], ...]]] = None,\ntarget_device: Optional[DeviceKind] = DeviceKind.CUDA,\nrunners: Optional[Union[Union[str, Type[NavigatorRunner]], Tuple[Union[str, Type[NavigatorRunner]], ...]]] = None,\nprofiler_config: Optional[ProfilerConfig] = None,\nverbose: bool = False,\ndebug: bool = False,\nverify_func: Optional[VerifyFunction] = None,\ncustom_configs: Optional[List[CustomConfig]] = None,\ndefaults: bool = True,\n) -&gt; Package:\n\"\"\"Generate target formats and run correctness and profiling tests for available runners.\n    Args:\n        package: Package to optimize.\n        target_formats: Formats to generate and profile. Defaults to target formats from the package.\n        target_device: Target device for optimize process, default is CUDA\n        runners: Runners to run correctness tests and profiling on. Defaults to runners from the package.\n        profiler_config: Configuration of the profiler. Defaults to config from the package.\n        verbose: If True enable verbose logging. Defaults to False.\n        debug: If True print debugging logs. Defaults to False.\n        verify_func: Function used for verifying generated models. Defaults to None.\n        custom_configs: Custom formats configuration. Defaults to None.\n        defaults: reset configuration of custom configs to defaults\n    Returns:\n        Optimized package\n    \"\"\"\nif package.is_empty() and package.model is None:\nraise ModelNavigatorEmptyPackageError(\n\"Package is empty and source model is not loaded. Unable to run optimize.\"\n)\nconfig = package.config\nif target_formats is None:\ntarget_formats = DEFAULT_TARGET_FORMATS[package.framework]\nif package.framework == Framework.TORCH and config.batch_dim is not None:\ntarget_formats, custom_configs = update_allowed_batching_parameters(\ntarget_formats=target_formats,\ncustom_configs=custom_configs,\n)\nif runners is None:\nrunners = default_runners(device_kind=target_device)\nif profiler_config is None:\nprofiler_config = ProfilerConfig()\nis_source_available = package.model is not None\n_update_config(\nconfig=config,\nis_source_available=is_source_available,\ntarget_formats=target_formats,\nrunners=runners,\nprofiler_config=profiler_config,\nverbose=verbose,\ndebug=debug,\nverify_func=verify_func,\ncustom_configs=custom_configs,\ndefaults=defaults,\ntarget_device=target_device,\n)\nbuilders = _get_builders(\nframework=package.framework,\nrun_profiling=config.profiler_config.run_profiling,\n)\nmodel_configs = _get_model_configs(\nconfig=config,\ncustom_configs=list(config.custom_configs.values()),\n)\noptimized_package = PipelineManager.run(\npipeline_builders=builders,\nconfig=config,\nmodels_config=model_configs,\npackage=package,\n)\nreturn optimized_package\n</code></pre>"},{"location":"api/package/#model_navigator.api.package.save","title":"<code>save(package, path, keep_workspace=True, override=False, save_data=True)</code>","text":"<p>Save export results into the .nav package at given path.</p> <p>Parameters:</p> Name Type Description Default <code>package</code> <code>Package</code> <p>A package object to prepare the package</p> required <code>path</code> <code>Union[str, Path]</code> <p>A path to file where the package has to be saved</p> required <code>keep_workspace</code> <code>bool</code> <p>flag to remove the working directory after saving the package</p> <code>True</code> <code>override</code> <code>bool</code> <p>flag to override existing package in provided path</p> <code>False</code> <code>save_data</code> <code>bool</code> <p>disable saving samples from the dataloader</p> <code>True</code> Source code in <code>model_navigator/api/package.py</code> <pre><code>def save(\npackage: Package,\npath: Union[str, Path],\nkeep_workspace: bool = True,\noverride: bool = False,\nsave_data: bool = True,\n) -&gt; None:\n\"\"\"Save export results into the .nav package at given path.\n    Args:\n        package: A package object to prepare the package\n        path: A path to file where the package has to be saved\n        keep_workspace: flag to remove the working directory after saving the package\n        override: flag to override existing package in provided path\n        save_data: disable saving samples from the dataloader\n    \"\"\"\npackage.save(\npath=path,\nkeep_workspace=keep_workspace,\noverride=override,\nsave_data=save_data,\n)\n</code></pre>"},{"location":"api/package/#model_navigator.api.package.set_verified","title":"<code>set_verified(package, model_key, runner_name)</code>","text":"<p>Set verified status for model and runner.</p> <p>Parameters:</p> Name Type Description Default <code>package</code> <code>Package</code> <p>Package.</p> required <code>model_key</code> <code>str</code> <p>Unique key of the model.</p> required <code>runner_name</code> <code>str</code> <p>Name of the runner.</p> required <p>Raises:</p> Type Description <code>ModelNavigatorNotFoundError</code> <p>When model and runner not found.</p> Source code in <code>model_navigator/api/package.py</code> <pre><code>def set_verified(\npackage: Package,\nmodel_key: str,\nrunner_name: str,\n) -&gt; None:\n\"\"\"Set verified status for model and runner.\n    Args:\n        package (Package): Package.\n        model_key (str): Unique key of the model.\n        runner_name (str): Name of the runner.\n    Raises:\n        ModelNavigatorNotFoundError: When model and runner not found.\n    \"\"\"\ntry:\nrunner_results = package.status.models_status[model_key].runners_status[runner_name]\nexcept KeyError:\nraise ModelNavigatorNotFoundError(f\"Model {model_key} and runner {runner_name} not found.\")\nrunner_results.status[VerifyModel.__name__] = CommandStatus.OK\n</code></pre>"},{"location":"api/python/","title":"Python","text":""},{"location":"api/python/#model_navigator.api.python","title":"<code>model_navigator.api.python</code>","text":"<p>Python optimize API.</p>"},{"location":"api/python/#model_navigator.api.python.optimize","title":"<code>optimize(model, dataloader, sample_count=DEFAULT_SAMPLE_COUNT, batching=True, target_device=DeviceKind.CPU, runners=None, profiler_config=None, workspace=None, verbose=False, debug=False, verify_func=None, custom_configs=None)</code>","text":"<p>Entrypoint for Python model optimize.</p> <p>Perform correctness testing, profiling and model verification.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Callable</code> <p>Model inference function</p> required <code>dataloader</code> <code>SizedDataLoader</code> <p>Sized iterable with data that will be feed to the model</p> required <code>sample_count</code> <code>int</code> <p>Limits how many samples will be used from dataloader</p> <code>DEFAULT_SAMPLE_COUNT</code> <code>batching</code> <code>Optional[bool]</code> <p>Enable or disable batching on first (index 0) dimension of the model</p> <code>True</code> <code>target_device</code> <code>Optional[DeviceKind]</code> <p>Target device for optimize process, default is CPU</p> <code>DeviceKind.CPU</code> <code>runners</code> <code>Optional[Union[Union[str, Type[NavigatorRunner]], Tuple[Union[str, Type[NavigatorRunner]], ...]]]</code> <p>Use only runners provided as paramter</p> <code>None</code> <code>profiler_config</code> <code>Optional[ProfilerConfig]</code> <p>Profiling config</p> <code>None</code> <code>workspace</code> <code>Optional[Path]</code> <p>Workspace where packages will be extracted</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Enable verbose logging</p> <code>False</code> <code>debug</code> <code>bool</code> <p>Enable debug logging from commands</p> <code>False</code> <code>verify_func</code> <code>Optional[VerifyFunction]</code> <p>Function for additional model verifcation</p> <code>None</code> <code>custom_configs</code> <code>Optional[Sequence[CustomConfig]]</code> <p>Sequence of CustomConfigs used to control produced artifacts</p> <code>None</code> <p>Returns:</p> Type Description <code>Package</code> <p>Package descriptor representing created package.</p> Source code in <code>model_navigator/api/python.py</code> <pre><code>def optimize(\nmodel: Callable,\ndataloader: SizedDataLoader,\nsample_count: int = DEFAULT_SAMPLE_COUNT,\nbatching: Optional[bool] = True,\ntarget_device: Optional[DeviceKind] = DeviceKind.CPU,\nrunners: Optional[Union[Union[str, Type[NavigatorRunner]], Tuple[Union[str, Type[NavigatorRunner]], ...]]] = None,\nprofiler_config: Optional[ProfilerConfig] = None,\nworkspace: Optional[Path] = None,\nverbose: bool = False,\ndebug: bool = False,\nverify_func: Optional[VerifyFunction] = None,\ncustom_configs: Optional[Sequence[CustomConfig]] = None,\n) -&gt; Package:\n\"\"\"Entrypoint for Python model optimize.\n    Perform correctness testing, profiling and model verification.\n    Args:\n        model: Model inference function\n        dataloader: Sized iterable with data that will be feed to the model\n        sample_count: Limits how many samples will be used from dataloader\n        batching: Enable or disable batching on first (index 0) dimension of the model\n        target_device: Target device for optimize process, default is CPU\n        runners: Use only runners provided as paramter\n        profiler_config: Profiling config\n        workspace: Workspace where packages will be extracted\n        verbose: Enable verbose logging\n        debug: Enable debug logging from commands\n        verify_func: Function for additional model verifcation\n        custom_configs: Sequence of CustomConfigs used to control produced artifacts\n    Returns:\n        Package descriptor representing created package.\n    \"\"\"\nif isinstance(model, str):\nmodel = Path(model)\nif workspace is None:\nworkspace = get_default_workspace()\nsample = next(iter(dataloader))\nforward_kw_names = tuple(sample.keys()) if isinstance(sample, Mapping) else None\nif runners is None:\nrunners = default_runners(device_kind=target_device)\nif profiler_config is None:\nprofiler_config = ProfilerConfig()\nrunner_names = enums.parse(runners, lambda runner: runner if isinstance(runner, str) else runner.name())\ntarget_formats = DEFAULT_NONE_FRAMEWORK_TARGET_FORMATS\nconfig = CommonConfig(\nFramework.NONE,\nmodel=model,\ndataloader=dataloader,\nforward_kw_names=forward_kw_names,\nworkspace=workspace,\ntarget_formats=target_formats,\ntarget_device=target_device,\nsample_count=sample_count,\nbatch_dim=0 if batching else None,\nrunner_names=runner_names,\nprofiler_config=profiler_config,\nverbose=verbose,\ndebug=debug,\nverify_func=verify_func,\ncustom_configs=map_custom_configs(custom_configs=custom_configs),\n)\nmodels_config = ModelConfigBuilder.generate_model_config(\nframework=Framework.NONE,\ntarget_formats=target_formats,\ncustom_configs=[],\n)\nbuilders = [preprocessing_builder, correctness_builder]\nif profiler_config.run_profiling:\nbuilders.append(profiling_builder)\nbuilders.append(verify_builder)\npackage = PipelineManager.run(\npipeline_builders=builders,\nconfig=config,\nmodels_config=models_config,\n)\nreturn package\n</code></pre>"},{"location":"api/pytriton/","title":"PyTriton","text":""},{"location":"api/pytriton/#model_navigator.api.pytriton","title":"<code>model_navigator.api.pytriton</code>","text":"<p>Public API definition for PyTriton related functionality.</p>"},{"location":"api/pytriton/#model_navigator.api.pytriton.DynamicBatcher","title":"<code>DynamicBatcher</code>  <code>dataclass</code>","text":"<p>Dynamic batcher configuration.</p> <p>More in Triton Inference Server documentation</p> <p>Parameters:</p> Name Type Description Default <code>max_queue_delay_microseconds</code> <code>int</code> <p>The maximum time, in microseconds, a request will be delayed in                           the scheduling queue to wait for additional requests for batching.</p> <code>0</code> <code>preferred_batch_size</code> <code>Optional[list]</code> <p>Preferred batch sizes for dynamic batching.</p> <code>None</code> <code>preserve_ordering</code> <code>bool</code> <p>Should the dynamic batcher preserve the ordering of responses to                 match the order of requests received by the scheduler.</p> <code>False</code> <code>priority_levels</code> <code>int</code> <p>The number of priority levels to be enabled for the model.</p> <code>0</code> <code>default_priority_level</code> <code>int</code> <p>The priority level used for requests that don't specify their priority.</p> <code>0</code> <code>default_queue_policy</code> <code>Optional[QueuePolicy]</code> <p>The default queue policy used for requests.</p> <code>None</code> <code>priority_queue_policy</code> <code>Optional[Dict[int, QueuePolicy]]</code> <p>Specify the queue policy for the priority level.</p> <code>None</code>"},{"location":"api/pytriton/#model_navigator.api.pytriton.ModelConfig","title":"<code>ModelConfig</code>  <code>dataclass</code>","text":"<p>Additional model configuration for running model through Triton Inference Server.</p> <p>Parameters:</p> Name Type Description Default <code>batching</code> <code>bool</code> <p>Flag to enable/disable batching for model.</p> <code>True</code> <code>max_batch_size</code> <code>int</code> <p>The maximal batch size that would be handled by model.</p> <code>4</code> <code>batcher</code> <code>DynamicBatcher</code> <p>Configuration of Dynamic Batching for the model.</p> <code>DynamicBatcher()</code> <code>response_cache</code> <code>bool</code> <p>Flag to enable/disable response cache for the model</p> <code>False</code>"},{"location":"api/pytriton/#model_navigator.api.pytriton.PyTritonAdapter","title":"<code>PyTritonAdapter(package, strategy=None)</code>","text":"<p>Provides model and configuration for PyTrtion deployment.</p> <p>Initialize PyTritonAdapter.</p> <p>Parameters:</p> Name Type Description Default <code>package</code> <code>Package</code> <p>A package object to be searched for best possible model.</p> required <code>strategy</code> <code>Optional[RuntimeSearchStrategy]</code> <p>Strategy for finding the best model. Defaults to <code>MaxThroughputAndMinLatencyStrategy</code></p> <code>None</code> Source code in <code>model_navigator/api/pytriton.py</code> <pre><code>def __init__(self, package: Package, strategy: Optional[RuntimeSearchStrategy] = None):\n\"\"\"Initialize PyTritonAdapter.\n    Args:\n        package: A package object to be searched for best possible model.\n        strategy: Strategy for finding the best model. Defaults to `MaxThroughputAndMinLatencyStrategy`\n    \"\"\"\nself._package = package\nself._strategy = MaxThroughputAndMinLatencyStrategy() if strategy is None else strategy\nself._runner = self._package.get_runner(strategy=self._strategy)\nself._batching = self._package.status.config.get(\"batch_dim\", None) == 0\n</code></pre>"},{"location":"api/pytriton/#model_navigator.api.pytriton.PyTritonAdapter.batching","title":"<code>batching: bool</code>  <code>property</code>","text":"<p>Returns status of batching support by the runner.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if runner supports batching, False otherwise.</p>"},{"location":"api/pytriton/#model_navigator.api.pytriton.PyTritonAdapter.config","title":"<code>config: ModelConfig</code>  <code>property</code>","text":"<p>Returns config for pytriton.</p> <p>Returns:</p> Type Description <code>ModelConfig</code> <p>ModelConfig with configuration for PyTrtion bind method.</p>"},{"location":"api/pytriton/#model_navigator.api.pytriton.PyTritonAdapter.inputs","title":"<code>inputs: List[Tensor]</code>  <code>property</code>","text":"<p>Returns inputs configuration.</p> <p>Returns:</p> Type Description <code>List[Tensor]</code> <p>List with Tensor objects describing inputs configuration of runner</p>"},{"location":"api/pytriton/#model_navigator.api.pytriton.PyTritonAdapter.outputs","title":"<code>outputs: List[Tensor]</code>  <code>property</code>","text":"<p>Returns outputs configuration.</p> <p>Returns:</p> Type Description <code>List[Tensor]</code> <p>List with Tensor objects describing outpus configuration of runner</p>"},{"location":"api/pytriton/#model_navigator.api.pytriton.PyTritonAdapter.runner","title":"<code>runner: NavigatorRunner</code>  <code>property</code>","text":"<p>Returns runner.</p> <pre><code>Runner must be activated before use with activate() method.\n</code></pre> <p>Returns:</p> Type Description <code>NavigatorRunner</code> <p>Model Navigator runner.</p>"},{"location":"api/pytriton/#model_navigator.api.pytriton.QueuePolicy","title":"<code>QueuePolicy</code>  <code>dataclass</code>","text":"<p>Model queue policy configuration.</p> <p>More in Triton Inference Server documentation</p> <p>Parameters:</p> Name Type Description Default <code>timeout_action</code> <code>TimeoutAction</code> <p>The action applied to timed-out request.</p> <code>TimeoutAction.REJECT</code> <code>default_timeout_microseconds</code> <code>int</code> <p>The default timeout for every request, in microseconds.</p> <code>0</code> <code>allow_timeout_override</code> <code>bool</code> <p>Whether individual request can override the default timeout value.</p> <code>False</code> <code>max_queue_size</code> <code>int</code> <p>The maximum queue size for holding requests.</p> <code>0</code>"},{"location":"api/pytriton/#model_navigator.api.pytriton.Tensor","title":"<code>Tensor</code>  <code>dataclass</code>","text":"<p>Model input and output definition for Triton deployment.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>tuple</code> <p>Shape of the input/output tensor.</p> required <code>dtype</code> <code>Union[np.dtype, Type[np.dtype], Type[object]]</code> <p>Data type of the input/output tensor.</p> required <code>name</code> <code>Optional[str]</code> <p>Name of the input/output of model.</p> <code>None</code> <code>optional</code> <code>Optional[bool]</code> <p>Flag to mark if input is optional.</p> <code>False</code>"},{"location":"api/pytriton/#model_navigator.api.pytriton.TimeoutAction","title":"<code>TimeoutAction</code>","text":"<p>         Bases: <code>enum.Enum</code></p> <p>Timeout action definition for timeout_action QueuePolicy field.</p> <p>Parameters:</p> Name Type Description Default <code>REJECT</code> <code>str</code> <p>Reject the request and return error message accordingly.</p> required <code>DELAY</code> <code>str</code> <p>Delay the request until all other requests at the same (or higher) priority levels that have not reached their timeouts are processed.</p> required"},{"location":"api/tensorflow/","title":"TensorFlow 2","text":""},{"location":"api/tensorflow/#model_navigator.api.tensorflow","title":"<code>model_navigator.api.tensorflow</code>","text":"<p>TensorFlow optimize API.</p>"},{"location":"api/tensorflow/#model_navigator.api.tensorflow.optimize","title":"<code>optimize(model, dataloader, sample_count=DEFAULT_SAMPLE_COUNT, batching=True, input_names=None, output_names=None, target_formats=None, target_device=DeviceKind.CUDA, runners=None, profiler_config=None, workspace=None, verbose=False, debug=False, verify_func=None, custom_configs=None)</code>","text":"<p>Entrypoint for TensorFlow2 optimize.</p> <p>Perform export, conversion, correctness testing, profiling and model verification.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>tensorflow.keras.Model</code> <p>TensorFlow2 model object</p> required <code>dataloader</code> <code>SizedDataLoader</code> <p>Sized iterable with data that will be feed to the model</p> required <code>sample_count</code> <code>int</code> <p>Limits how many samples will be used from dataloader</p> <code>DEFAULT_SAMPLE_COUNT</code> <code>batching</code> <code>Optional[bool]</code> <p>Enable or disable batching on first (index 0) dimension of the model</p> <code>True</code> <code>input_names</code> <code>Optional[Tuple[str, ...]]</code> <p>Model input names</p> <code>None</code> <code>output_names</code> <code>Optional[Tuple[str, ...]]</code> <p>Model output names</p> <code>None</code> <code>target_formats</code> <code>Optional[Union[Union[str, Format], Tuple[Union[str, Format], ...]]]</code> <p>Target model formats for optimize process</p> <code>None</code> <code>target_device</code> <code>Optional[DeviceKind]</code> <p>Target device for optimize process, default is CUDA</p> <code>DeviceKind.CUDA</code> <code>runners</code> <code>Optional[Union[Union[str, Type[NavigatorRunner]], Tuple[Union[str, Type[NavigatorRunner]], ...]]]</code> <p>Use only runners provided as paramter</p> <code>None</code> <code>profiler_config</code> <code>Optional[ProfilerConfig]</code> <p>Profiling config</p> <code>None</code> <code>workspace</code> <code>Optional[Path]</code> <p>Workspace where packages will be extracted</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Enable verbose logging</p> <code>False</code> <code>debug</code> <code>bool</code> <p>Enable debug logging from commands</p> <code>False</code> <code>verify_func</code> <code>Optional[VerifyFunction]</code> <p>Function for additional model verifcation</p> <code>None</code> <code>custom_configs</code> <code>Optional[Sequence[CustomConfig]]</code> <p>Sequence of CustomConfigs used to control produced artifacts</p> <code>None</code> <p>Returns:</p> Type Description <code>Package</code> <p>Package descriptor representing created package.</p> Source code in <code>model_navigator/api/tensorflow.py</code> <pre><code>def optimize(\nmodel: tensorflow.keras.Model,\ndataloader: SizedDataLoader,\nsample_count: int = DEFAULT_SAMPLE_COUNT,\nbatching: Optional[bool] = True,\ninput_names: Optional[Tuple[str, ...]] = None,\noutput_names: Optional[Tuple[str, ...]] = None,\ntarget_formats: Optional[Union[Union[str, Format], Tuple[Union[str, Format], ...]]] = None,\ntarget_device: Optional[DeviceKind] = DeviceKind.CUDA,\nrunners: Optional[Union[Union[str, Type[NavigatorRunner]], Tuple[Union[str, Type[NavigatorRunner]], ...]]] = None,\nprofiler_config: Optional[ProfilerConfig] = None,\nworkspace: Optional[Path] = None,\nverbose: bool = False,\ndebug: bool = False,\nverify_func: Optional[VerifyFunction] = None,\ncustom_configs: Optional[Sequence[CustomConfig]] = None,\n) -&gt; Package:\n\"\"\"Entrypoint for TensorFlow2 optimize.\n    Perform export, conversion, correctness testing, profiling and model verification.\n    Args:\n        model: TensorFlow2 model object\n        dataloader: Sized iterable with data that will be feed to the model\n        sample_count: Limits how many samples will be used from dataloader\n        batching: Enable or disable batching on first (index 0) dimension of the model\n        input_names: Model input names\n        output_names: Model output names\n        target_formats: Target model formats for optimize process\n        target_device: Target device for optimize process, default is CUDA\n        runners: Use only runners provided as paramter\n        profiler_config: Profiling config\n        workspace: Workspace where packages will be extracted\n        verbose: Enable verbose logging\n        debug: Enable debug logging from commands\n        verify_func: Function for additional model verifcation\n        custom_configs: Sequence of CustomConfigs used to control produced artifacts\n    Returns:\n        Package descriptor representing created package.\n    \"\"\"\nif target_device == DeviceKind.CPU and any(\ndevice.device_type == \"GPU\" for device in tensorflow.config.get_visible_devices()\n):\nraise ModelNavigatorConfigurationError(\n\"\\n\"\n\"    'target_device == nav.DeviceKind.CPU' is not supported for TensorFlow2 when GPU is available.\\n\"\n\"    To optimize model for CPU, disable GPU with: \"\n\"'tf.config.set_visible_devices([], 'GPU')' directly after importing TensorFlow.\\n\"\n)\nif workspace is None:\nworkspace = get_default_workspace()\nif target_formats is None:\ntarget_formats = DEFAULT_TENSORFLOW_TARGET_FORMATS\nif runners is None:\nrunners = default_runners(device_kind=target_device)\nforward_kw_names = None\nsample = next(iter(dataloader))\nif isinstance(sample, Mapping):\nforward_kw_names = tuple(sample.keys())\ntarget_formats_enums = enums.parse(target_formats, Format)\nrunner_names = enums.parse(runners, lambda runner: runner if isinstance(runner, str) else runner.name())\nif profiler_config is None:\nprofiler_config = ProfilerConfig()\nif Format.TENSORFLOW not in target_formats_enums:\ntarget_formats_enums = (Format.TENSORFLOW,) + target_formats_enums\nconfig = CommonConfig(\nFramework.TENSORFLOW,\nmodel=model,\ndataloader=dataloader,\ntarget_formats=target_formats_enums,\ntarget_device=target_device,\nworkspace=workspace,\nsample_count=sample_count,\n_input_names=input_names,\n_output_names=output_names,\nbatch_dim=0 if batching else None,\nrunner_names=runner_names,\nprofiler_config=profiler_config,\nforward_kw_names=forward_kw_names,\nverbose=verbose,\ndebug=debug,\nverify_func=verify_func,\ncustom_configs=map_custom_configs(custom_configs=custom_configs),\n)\nmodels_config = ModelConfigBuilder.generate_model_config(\nframework=Framework.TENSORFLOW,\ntarget_formats=target_formats_enums,\ncustom_configs=custom_configs,\n)\nbuilders = [\npreprocessing_builder,\ntensorflow_export_builder,\nfind_device_max_batch_size_builder,\ntensorflow_conversion_builder,\ncorrectness_builder,\n]\nif profiler_config.run_profiling:\nbuilders.append(profiling_builder)\nbuilders.append(verify_builder)\npackage = PipelineManager.run(\npipeline_builders=builders,\nconfig=config,\nmodels_config=models_config,\n)\nreturn package\n</code></pre>"},{"location":"api/torch/","title":"PyTorch","text":""},{"location":"api/torch/#model_navigator.api.torch","title":"<code>model_navigator.api.torch</code>","text":"<p>Torch optimize API.</p>"},{"location":"api/torch/#model_navigator.api.torch.optimize","title":"<code>optimize(model, dataloader, sample_count=DEFAULT_SAMPLE_COUNT, batching=True, input_names=None, output_names=None, target_formats=None, target_device=DeviceKind.CUDA, runners=None, profiler_config=None, workspace=None, verbose=False, debug=False, verify_func=None, custom_configs=None)</code>","text":"<p>Entrypoint for Torch optimize.</p> <p>Perform export, conversion, correctness testing, profiling and model verification.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>torch.nn.Module</code> <p>PyTorch model object</p> required <code>dataloader</code> <code>SizedDataLoader</code> <p>Sized iterable with data that will be feed to the model</p> required <code>sample_count</code> <code>Optional[int]</code> <p>Limits how many samples will be used from dataloader</p> <code>DEFAULT_SAMPLE_COUNT</code> <code>batching</code> <code>Optional[bool]</code> <p>Enable or disable batching on first (index 0) dimension of the model</p> <code>True</code> <code>input_names</code> <code>Optional[Tuple[str, ...]]</code> <p>Model input names</p> <code>None</code> <code>output_names</code> <code>Optional[Tuple[str, ...]]</code> <p>Model output names</p> <code>None</code> <code>target_formats</code> <code>Optional[Union[Union[str, Format], Tuple[Union[str, Format], ...]]]</code> <p>Target model formats for optimize process</p> <code>None</code> <code>target_device</code> <code>Optional[DeviceKind]</code> <p>Target device for optimize process, default is CUDA</p> <code>DeviceKind.CUDA</code> <code>runners</code> <code>Optional[Union[Union[str, Type[NavigatorRunner]], Tuple[Union[str, Type[NavigatorRunner]], ...]]]</code> <p>Use only runners provided as parameter</p> <code>None</code> <code>profiler_config</code> <code>Optional[ProfilerConfig]</code> <p>Profiling config</p> <code>None</code> <code>workspace</code> <code>Optional[Path]</code> <p>Workspace where packages will be extracted</p> <code>None</code> <code>verbose</code> <code>Optional[bool]</code> <p>Enable verbose logging</p> <code>False</code> <code>debug</code> <code>Optional[bool]</code> <p>Enable debug logging from commands</p> <code>False</code> <code>verify_func</code> <code>Optional[VerifyFunction]</code> <p>Function for additional model verification</p> <code>None</code> <code>custom_configs</code> <code>Optional[Sequence[CustomConfig]]</code> <p>Sequence of CustomConfigs used to control produced artifacts</p> <code>None</code> <p>Returns:</p> Type Description <code>Package</code> <p>Package descriptor representing created package.</p> Source code in <code>model_navigator/api/torch.py</code> <pre><code>def optimize(\nmodel: torch.nn.Module,\ndataloader: SizedDataLoader,\nsample_count: Optional[int] = DEFAULT_SAMPLE_COUNT,\nbatching: Optional[bool] = True,\ninput_names: Optional[Tuple[str, ...]] = None,\noutput_names: Optional[Tuple[str, ...]] = None,\ntarget_formats: Optional[Union[Union[str, Format], Tuple[Union[str, Format], ...]]] = None,\ntarget_device: Optional[DeviceKind] = DeviceKind.CUDA,\nrunners: Optional[Union[Union[str, Type[NavigatorRunner]], Tuple[Union[str, Type[NavigatorRunner]], ...]]] = None,\nprofiler_config: Optional[ProfilerConfig] = None,\nworkspace: Optional[Path] = None,\nverbose: Optional[bool] = False,\ndebug: Optional[bool] = False,\nverify_func: Optional[VerifyFunction] = None,\ncustom_configs: Optional[Sequence[CustomConfig]] = None,\n) -&gt; Package:\n\"\"\"Entrypoint for Torch optimize.\n    Perform export, conversion, correctness testing, profiling and model verification.\n    Args:\n        model: PyTorch model object\n        dataloader: Sized iterable with data that will be feed to the model\n        sample_count: Limits how many samples will be used from dataloader\n        batching: Enable or disable batching on first (index 0) dimension of the model\n        input_names: Model input names\n        output_names: Model output names\n        target_formats: Target model formats for optimize process\n        target_device: Target device for optimize process, default is CUDA\n        runners: Use only runners provided as parameter\n        profiler_config: Profiling config\n        workspace: Workspace where packages will be extracted\n        verbose: Enable verbose logging\n        debug: Enable debug logging from commands\n        verify_func: Function for additional model verification\n        custom_configs: Sequence of CustomConfigs used to control produced artifacts\n    Returns:\n        Package descriptor representing created package.\n    \"\"\"\nif workspace is None:\nworkspace = get_default_workspace()\nif target_formats is None:\ntarget_formats = DEFAULT_TORCH_TARGET_FORMATS\nif batching:\ntarget_formats, custom_configs = update_allowed_batching_parameters(\ntarget_formats=target_formats,\ncustom_configs=custom_configs,\n)\nLOGGER.info(f\"Using default target formats: {[tf.name for tf in target_formats]}\")\nsample = next(iter(dataloader))\nif isinstance(sample, Mapping):\nforward_kw_names = tuple(sample.keys())\nelse:\nforward_kw_names = None\nif runners is None:\nrunners = default_runners(device_kind=target_device)\ntarget_formats = enums.parse(target_formats, Format)\nrunner_names = enums.parse(runners, lambda runner: runner if isinstance(runner, str) else runner.name())\nif profiler_config is None:\nprofiler_config = ProfilerConfig()\nif Format.TORCH not in target_formats:\ntarget_formats = (Format.TORCH,) + target_formats\nconfig = CommonConfig(\nframework=Framework.TORCH,\nmodel=model,\ndataloader=dataloader,\ntarget_formats=target_formats,\nworkspace=workspace,\nsample_count=sample_count,\n_input_names=input_names,\n_output_names=output_names,\ntarget_device=target_device,\nforward_kw_names=forward_kw_names,\nbatch_dim=0 if batching else None,\nrunner_names=runner_names,\nprofiler_config=profiler_config,\nverbose=verbose,\ndebug=debug,\nverify_func=verify_func,\ncustom_configs=map_custom_configs(custom_configs=custom_configs),\n)\nmodels_config = ModelConfigBuilder.generate_model_config(\nframework=Framework.TORCH,\ntarget_formats=target_formats,\ncustom_configs=custom_configs,\n)\nbuilders = [\npreprocessing_builder,\ntorch_export_builder,\nfind_device_max_batch_size_builder,\ntorch_conversion_builder,\ncorrectness_builder,\n]\nif profiler_config.run_profiling:\nbuilders.append(profiling_builder)\nbuilders.append(verify_builder)\npackage = PipelineManager.run(\npipeline_builders=builders,\nconfig=config,\nmodels_config=models_config,\n)\nreturn package\n</code></pre>"},{"location":"api/utilities/","title":"PyTriton","text":""},{"location":"api/utilities/#model_navigator.api.utilities","title":"<code>model_navigator.api.utilities</code>","text":"<p>Public utilities for the Model Navigator API.</p>"},{"location":"api/utilities/#model_navigator.api.utilities.UnpackedDataloader","title":"<code>UnpackedDataloader(dataloader, unpack_fn)</code>","text":"<p>A wrapper around a SizedDataLoader that applies a function to each sample.</p> <p>Parameters:</p> Name Type Description Default <code>dataloader</code> <code>SizedDataLoader</code> <p>A SizedDataLoader.</p> required <code>unpack_fn</code> <code>Callable</code> <p>A function that takes a sample and returns a new sample.</p> required <p>Returns:</p> Type Description <p>An iterator over the samples in the dataloader with the unpack_fn applied.</p> Example <p>dataloader = [1, 2, 3] unpacked_dataloader = UnpackedDataloader(dataloader, lambda x: x + 1)</p> <p>Initialize the UnpackedDataloader.</p> Source code in <code>model_navigator/api/utilities.py</code> <pre><code>def __init__(self, dataloader: SizedDataLoader, unpack_fn: Callable):\n\"\"\"Initialize the UnpackedDataloader.\"\"\"\nself._dataloader = dataloader\nself._unpack_fn = unpack_fn\n</code></pre>"},{"location":"api/utilities/#model_navigator.api.utilities.UnpackedDataloader--unpacked_dataloader-is-now-2-3-4","title":"unpacked_dataloader is now [2, 3, 4]","text":""},{"location":"api/utilities/#model_navigator.api.utilities.UnpackedDataloader.__iter__","title":"<code>__iter__()</code>","text":"<p>Return an iterator over the samples in the dataloader with the unpack_fn applied.</p> Source code in <code>model_navigator/api/utilities.py</code> <pre><code>def __iter__(self):\n\"\"\"Return an iterator over the samples in the dataloader with the unpack_fn applied.\"\"\"\nfor sample in self._dataloader:\nyield self._unpack_fn(sample)\n</code></pre>"},{"location":"api/utilities/#model_navigator.api.utilities.UnpackedDataloader.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of samples in the dataloader.</p> Source code in <code>model_navigator/api/utilities.py</code> <pre><code>def __len__(self):\n\"\"\"Return the number of samples in the dataloader.\"\"\"\nreturn len(self._dataloader)\n</code></pre>"},{"location":"triton/accelerators/","title":"Accelerators","text":""},{"location":"triton/accelerators/#accelerators","title":"Accelerators","text":""},{"location":"triton/accelerators/#model_navigator.api.triton.AutoMixedPrecisionAccelerator","title":"<code>model_navigator.api.triton.AutoMixedPrecisionAccelerator</code>  <code>dataclass</code>","text":"<p>Auto-mixed-precision accelerator for TensorFlow. Enable automatic FP16 precision.</p> <p>Currently empty - no arguments required.</p>"},{"location":"triton/accelerators/#model_navigator.api.triton.GPUIOAccelerator","title":"<code>model_navigator.api.triton.GPUIOAccelerator</code>  <code>dataclass</code>","text":"<p>GPU IO accelerator for TensorFlow.</p> <p>Currently empty - no arguments required.</p>"},{"location":"triton/accelerators/#model_navigator.api.triton.OpenVINOAccelerator","title":"<code>model_navigator.api.triton.OpenVINOAccelerator</code>  <code>dataclass</code>","text":"<p>OpenVINO optimization.</p> <p>Currently empty - no arguments required.</p>"},{"location":"triton/accelerators/#model_navigator.api.triton.OpenVINOAccelerator","title":"<code>model_navigator.api.triton.OpenVINOAccelerator</code>  <code>dataclass</code>","text":"<p>OpenVINO optimization.</p> <p>Currently empty - no arguments required.</p>"},{"location":"triton/accelerators/#model_navigator.api.triton.TensorRTAccelerator","title":"<code>model_navigator.api.triton.TensorRTAccelerator</code>  <code>dataclass</code>","text":"<p>TensorRT accelerator configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> Name Type Description Default <code>precision</code> <code>TensorRTOptPrecision</code> <p>The precision used for optimization</p> <code>TensorRTOptPrecision.FP32</code> <code>max_workspace_size</code> <code>Optional[int]</code> <p>The maximum GPU memory the model can use temporarily during execution</p> <code>None</code> <code>max_cached_engines</code> <code>Optional[int]</code> <p>The maximum number of cached TensorRT engines in dynamic TensorRT ops</p> <code>None</code> <code>minimum_segment_size</code> <code>Optional[int]</code> <p>The smallest model subgraph that will be considered for optimization by TensorRT</p> <code>None</code>"},{"location":"triton/accelerators/#model_navigator.api.triton.TensorRTOptPrecision","title":"<code>model_navigator.api.triton.TensorRTOptPrecision</code>","text":"<p>         Bases: <code>enum.Enum</code></p> <p>TensorRT optimization allowed precision.</p> <p>Parameters:</p> Name Type Description Default <code>FP16</code> <p>fp16 precision</p> required <code>FP32</code> <p>fp32 precision</p> required"},{"location":"triton/dynamic_batcher/","title":"Dynamic Batcher","text":""},{"location":"triton/dynamic_batcher/#dynamic-batcher","title":"Dynamic Batcher","text":""},{"location":"triton/dynamic_batcher/#model_navigator.api.triton.DynamicBatcher","title":"<code>model_navigator.api.triton.DynamicBatcher</code>  <code>dataclass</code>","text":"<p>Dynamic batching configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> Name Type Description Default <code>max_queue_delay_microseconds</code> <code>int</code> <p>The maximum time, in microseconds, a request will be delayed in                           the scheduling queue to wait for additional requests for batching.</p> <code>0</code> <code>preferred_batch_size</code> <code>Optional[list]</code> <p>Preferred batch sizes for dynamic batching.</p> <code>None</code> <code>preserve_ordering</code> <p>Should the dynamic batcher preserve the ordering of responses to                 match the order of requests received by the scheduler.</p> <code>False</code> <code>priority_levels</code> <code>int</code> <p>The number of priority levels to be enabled for the model.</p> <code>0</code> <code>default_priority_level</code> <code>int</code> <p>The priority level used for requests that don't specify their priority.</p> <code>0</code> <code>default_queue_policy</code> <code>Optional[QueuePolicy]</code> <p>The default queue policy used for requests.</p> <code>None</code> <code>priority_queue_policy</code> <code>Optional[Dict[int, QueuePolicy]]</code> <p>Specify the queue policy for the priority level.</p> <code>None</code>"},{"location":"triton/dynamic_batcher/#model_navigator.triton.specialized_configs.common.DynamicBatcher.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/common.py</code> <pre><code>def __post_init__(self):\n\"\"\"Validate the configuration for early error handling.\"\"\"\nif self.default_priority_level &gt; self.priority_levels:\nraise ModelNavigatorWrongParameterError(\n\"The `default_priority_level` must be between 1 and \" f\"{self.priority_levels}.\"\n)\nif self.priority_queue_policy:\nif not self.priority_levels:\nraise ModelNavigatorWrongParameterError(\n\"Provide the `priority_levels` if you want to define `priority_queue_policy` \"\n\"for Dynamic Batching.\"\n)\nfor priority in self.priority_queue_policy.keys():\nif priority &lt; 0 or priority &gt; self.priority_levels:\nraise ModelNavigatorWrongParameterError(\nf\"Invalid `priority`={priority} provided. The value must be between \"\nf\"1 and {self.priority_levels}.\"\n)\n</code></pre>"},{"location":"triton/dynamic_batcher/#model_navigator.api.triton.QueuePolicy","title":"<code>model_navigator.api.triton.QueuePolicy</code>  <code>dataclass</code>","text":"<p>Model queue policy configuration.</p> <p>Used for <code>default_queue_policy</code> and <code>priority_queue_policy</code> fields in DynamicBatcher configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> Name Type Description Default <code>timeout_action</code> <code>TimeoutAction</code> <p>The action applied to timed-out request.</p> <code>TimeoutAction.REJECT</code> <code>default_timeout_microseconds</code> <code>int</code> <p>The default timeout for every request, in microseconds.</p> <code>0</code> <code>allow_timeout_override</code> <code>bool</code> <p>Whether individual request can override the default timeout value.</p> <code>False</code> <code>max_queue_size</code> <code>int</code> <p>The maximum queue size for holding requests.</p> <code>0</code>"},{"location":"triton/dynamic_batcher/#model_navigator.api.triton.TimeoutAction","title":"<code>model_navigator.api.triton.TimeoutAction</code>","text":"<p>         Bases: <code>enum.Enum</code></p> <p>Timeout action definition for timeout_action QueuePolicy field.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> Name Type Description Default <code>REJECT</code> <p>\"REJECT\"</p> required <code>DELAY</code> <p>\"DELAY\"</p> required"},{"location":"triton/inputs_and_outputs/","title":"Inputs and Outputs","text":""},{"location":"triton/inputs_and_outputs/#model-inputs-and-outputs","title":"Model Inputs and Outputs","text":""},{"location":"triton/inputs_and_outputs/#model_navigator.api.triton.InputTensorSpec","title":"<code>model_navigator.api.triton.InputTensorSpec</code>  <code>dataclass</code>","text":"<p>         Bases: <code>BaseTensorSpec</code></p> <p>Stores specification of single input tensor.</p> <p>This includes name, shape, dtype and more parameters available for input tensor in Triton Inference Server:</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> Name Type Description Default <code>optional</code> <code>bool</code> <p>Flag marking the input is optional for the model execution</p> <code>False</code> <code>format</code> <code>Optional[InputTensorFormat]</code> <p>The format of the input.</p> <code>None</code> <code>allow_ragged_batch</code> <code>bool</code> <p>Flag marking the input is allowed to be \"ragged\" in a dynamically created batch.</p> <code>False</code>"},{"location":"triton/inputs_and_outputs/#model_navigator.api.triton.InputTensorFormat","title":"<code>model_navigator.api.triton.InputTensorFormat</code>","text":"<p>         Bases: <code>enum.Enum</code></p> <p>Format for input tensor.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> Name Type Description Default <code>FORMAT_NONE</code> <p>0</p> required <code>FORMAT_NHWC</code> <p>1</p> required <code>FORMAT_NCHW</code> <p>2</p> required"},{"location":"triton/inputs_and_outputs/#model_navigator.api.triton.OutputTensorSpec","title":"<code>model_navigator.api.triton.OutputTensorSpec</code>  <code>dataclass</code>","text":"<p>         Bases: <code>BaseTensorSpec</code></p> <p>Stores specification of single output tensor.</p> <p>This includes name, shape, dtype and more parameters available for output tensor in Triton Inference Server:</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> Name Type Description Default <code>label_filename</code> <code>Optional[str]</code> <p>The label file associated with this output.</p> <code>None</code>"},{"location":"triton/instance_groups/","title":"Instance Group","text":""},{"location":"triton/instance_groups/#model-instance-group","title":"Model Instance Group","text":""},{"location":"triton/instance_groups/#model_navigator.api.triton.InstanceGroup","title":"<code>model_navigator.api.triton.InstanceGroup</code>  <code>dataclass</code>","text":"<p>Configuration for model instance group.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> Name Type Description Default <code>kind</code> <code>Optional[DeviceKind]</code> <p>Kind of this instance group.</p> <code>None</code> <code>count</code> <code>Optional[int]</code> <p>For a group assigned to GPU, the number of instances created for    each GPU listed in 'gpus'. For a group assigned to CPU the number    of instances created.</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>Optional name of this group of instances.</p> <code>None</code> <code>gpus</code> <code>List[int]</code> <p>GPU(s) where instances should be available.</p> <code>dataclasses.field(default_factory=lambda : [])</code> <code>passive</code> <code>bool</code> <p>Whether the instances within this instance group will be accepting      inference requests from the scheduler.</p> <code>False</code> <code>host_policy</code> <code>Optional[str]</code> <p>The host policy name that the instance to be associated with.</p> <code>None</code> <code>profile</code> <code>List[str]</code> <p>For TensorRT models containing multiple optimization profile, this      parameter specifies a set of optimization profiles available to this      instance group.</p> <code>dataclasses.field(default_factory=lambda : [])</code>"},{"location":"triton/instance_groups/#model_navigator.triton.specialized_configs.common.InstanceGroup.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/common.py</code> <pre><code>def __post_init__(self) -&gt; None:\n\"\"\"Validate the configuration for early error handling.\"\"\"\nif self.count is not None and self.count &lt; 1:\nraise ModelNavigatorWrongParameterError(\"The `count` must be greater or equal 1.\")\nif self.kind not in [None, DeviceKind.KIND_GPU, DeviceKind.KIND_AUTO] and len(self.gpus) &gt; 0:\nraise ModelNavigatorWrongParameterError(\nf\"`gpus` cannot be set when device is not {DeviceKind.KIND_GPU} or {DeviceKind.KIND_AUTO}\"\n)\n</code></pre>"},{"location":"triton/instance_groups/#model_navigator.api.triton.DeviceKind","title":"<code>model_navigator.api.triton.DeviceKind</code>","text":"<p>         Bases: <code>enum.Enum</code></p> <p>Device kind for model deployment.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> Name Type Description Default <code>KIND_AUTO</code> <p>\"KIND_AUTO\"</p> required <code>KIND_CPU</code> <p>\"KIND_CPU\"</p> required <code>KIND_GPU</code> <p>\"KIND_GPU\"</p> required"},{"location":"triton/model_store_api/","title":"Python API","text":""},{"location":"triton/model_store_api/#triton-model-store-api","title":"Triton Model Store API","text":""},{"location":"triton/model_store_api/#model_navigator.triton.model_repository.add_model","title":"<code>model_navigator.triton.model_repository.add_model(model_repository_path, model_name, model_path, config, model_version=1)</code>","text":"<p>Generate model deployment inside provided model store path.</p> <p>The config requires specialized configuration to be passed for backend on which model is executed. Example: - ONNX model requires ONNXModelConfig - TensorRT model requires TensorRTModelConfig - TorchScript or Torch-TensorRT models requires PyTorchModelConfig - TensorFlow SavedModel or TensorFlow-TensorRT models requires TensorFlowModelConfig - Python model requires PythonModelConfig</p> <p>Parameters:</p> Name Type Description Default <code>model_repository_path</code> <code>Union[str, pathlib.Path]</code> <p>Path where deployment should be created</p> required <code>model_name</code> <code>str</code> <p>Name under which model is deployed in Triton Inference Server</p> required <code>model_path</code> <code>Union[str, pathlib.Path]</code> <p>Path to model</p> required <code>config</code> <code>Union[ONNXModelConfig, TensorRTModelConfig, PyTorchModelConfig, PythonModelConfig, TensorFlowModelConfig]</code> <p>Specialized configuration of model for backend on which model is executed</p> required <code>model_version</code> <code>int</code> <p>Version of model that is deployed</p> <code>1</code> <p>Returns:</p> Type Description <code>pathlib.Path</code> <p>Path to created model store</p> Source code in <code>model_navigator/triton/model_repository.py</code> <pre><code>def add_model(\nmodel_repository_path: Union[str, pathlib.Path],\nmodel_name: str,\nmodel_path: Union[str, pathlib.Path],\nconfig: Union[\nONNXModelConfig,\nTensorRTModelConfig,\nPyTorchModelConfig,\nPythonModelConfig,\nTensorFlowModelConfig,\n],\nmodel_version: int = 1,\n) -&gt; pathlib.Path:\n\"\"\"Generate model deployment inside provided model store path.\n    The config requires specialized configuration to be passed for backend on which model is executed. Example:\n    - ONNX model requires ONNXModelConfig\n    - TensorRT model requires TensorRTModelConfig\n    - TorchScript or Torch-TensorRT models requires PyTorchModelConfig\n    - TensorFlow SavedModel or TensorFlow-TensorRT models requires TensorFlowModelConfig\n    - Python model requires PythonModelConfig\n    Args:\n        model_repository_path: Path where deployment should be created\n        model_name: Name under which model is deployed in Triton Inference Server\n        model_path: Path to model\n        config: Specialized configuration of model for backend on which model is executed\n        model_version: Version of model that is deployed\n    Returns:\n         Path to created model store\n    \"\"\"\nif isinstance(config, ONNXModelConfig):\nmodel_config = ModelConfigBuilder.from_onnx_config(\nmodel_name=model_name,\nmodel_version=model_version,\nonnx_config=config,\n)\nelif isinstance(config, TensorFlowModelConfig):\nmodel_config = ModelConfigBuilder.from_tensorflow_config(\nmodel_name=model_name,\nmodel_version=model_version,\ntensorflow_config=config,\n)\nelif isinstance(config, PythonModelConfig):\nmodel_config = ModelConfigBuilder.from_python_config(\nmodel_name=model_name,\nmodel_version=model_version,\npython_config=config,\n)\nelif isinstance(config, PyTorchModelConfig):\nmodel_config = ModelConfigBuilder.from_pytorch_config(\nmodel_name=model_name,\nmodel_version=model_version,\npytorch_config=config,\n)\nelif isinstance(config, TensorRTModelConfig):\nmodel_config = ModelConfigBuilder.from_tensorrt_config(\nmodel_name=model_name,\nmodel_version=model_version,\ntensorrt_config=config,\n)\nelse:\nraise ModelNavigatorWrongParameterError(f\"Unsupported model config provided: {config.__class__}\")\ntriton_model_repository = _TritonModelRepository(model_repository_path=pathlib.Path(model_repository_path))\nreturn triton_model_repository.deploy_model(\nmodel_path=pathlib.Path(model_path),\nmodel_config=model_config,\n)\n</code></pre>"},{"location":"triton/model_store_api/#model_navigator.triton.model_repository.add_model_from_package","title":"<code>model_navigator.triton.model_repository.add_model_from_package(model_repository_path, model_name, package, model_version=1, strategy=None, response_cache=False)</code>","text":"<p>Create the Triton Model Store with optimized model and save it to <code>model_repository_path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_repository_path</code> <code>Union[str, pathlib.Path]</code> <p>Path where the model store is located</p> required <code>model_name</code> <code>str</code> <p>Name under which model is deployed in Triton Inference Server</p> required <code>model_version</code> <code>int</code> <p>Version of model that is deployed</p> <code>1</code> <code>package</code> <code>Package</code> <p>Package for which model store is created</p> required <code>strategy</code> <code>Optional[RuntimeSearchStrategy]</code> <p>Strategy for finding the best runtime.       When not set the <code>MaxThroughputAndMinLatencyStrategy</code> is used.</p> <code>None</code> <code>response_cache</code> <code>bool</code> <p>Enable response cache for model</p> <code>False</code> <p>Returns:</p> Type Description <p>Path to created model store</p> Source code in <code>model_navigator/triton/model_repository.py</code> <pre><code>def add_model_from_package(\nmodel_repository_path: Union[str, pathlib.Path],\nmodel_name: str,\npackage: Package,\nmodel_version: int = 1,\nstrategy: Optional[RuntimeSearchStrategy] = None,\nresponse_cache: bool = False,\n):\n\"\"\"Create the Triton Model Store with optimized model and save it to `model_repository_path`.\n    Args:\n        model_repository_path: Path where the model store is located\n        model_name: Name under which model is deployed in Triton Inference Server\n        model_version: Version of model that is deployed\n        package: Package for which model store is created\n        strategy: Strategy for finding the best runtime.\n                  When not set the `MaxThroughputAndMinLatencyStrategy` is used.\n        response_cache: Enable response cache for model\n    Returns:\n        Path to created model store\n    \"\"\"\nif package.is_empty():\nraise ModelNavigatorEmptyPackageError(\"No models available in the package. Triton deployment is not possible.\")\nif package.config.batch_dim not in [0, None]:\nraise ModelNavigatorWrongParameterError(\n\"Only models without batching or batch dimension on first place in shape are supported for Triton.\"\n)\nif strategy is None:\nstrategy = MaxThroughputAndMinLatencyStrategy()\nbatching = package.config.batch_dim == 0\nruntime_result = RuntimeAnalyzer.get_runtime(\nmodels_status=package.status.models_status,\nstrategy=strategy,\nformats=[fmt.value for fmt in TRITON_FORMATS],\nrunners=[runner.name() for runner in TRITON_RUNNERS],\n)\nmax_batch_size = max(\nprofiling_results.batch_size\nfor profiling_results in runtime_result.runner_status.result[Performance.name()][\"profiling_results\"]\n)\nif runtime_result.model_status.model_config.format == Format.ONNX:\nconfig = _onnx_config_from_runtime_result(\nbatching=batching,\nmax_batch_size=max_batch_size,\nresponse_cache=response_cache,\nruntime_result=runtime_result,\n)\nelif runtime_result.model_status.model_config.format in [Format.TF_SAVEDMODEL, Format.TF_TRT]:\nconfig = _tensorflow_config_from_runtime_result(\nbatching=batching,\nmax_batch_size=max_batch_size,\nresponse_cache=response_cache,\nruntime_result=runtime_result,\n)\nelif runtime_result.model_status.model_config.format in [Format.TORCHSCRIPT, Format.TORCH_TRT]:\ninputs = input_tensor_from_metadata(\npackage.status.input_metadata,\nbatching=batching,\n)\noutputs = output_tensor_from_metadata(\npackage.status.output_metadata,\nbatching=batching,\n)\nconfig = _pytorch_config_from_runtime_result(\nbatching=batching,\nmax_batch_size=max_batch_size,\ninputs=inputs,\noutputs=outputs,\nresponse_cache=response_cache,\nruntime_result=runtime_result,\n)\nelif runtime_result.model_status.model_config.format == Format.TENSORRT:\nconfig = _tensorrt_config_from_runtime_result(\nbatching=batching,\nmax_batch_size=max_batch_size,\nresponse_cache=response_cache,\n)\nelse:\nraise ModelNavigatorError(\nf\"Unsupported model format selected: {runtime_result.model_status.model_config.format}\"\n)\nreturn add_model(\nmodel_repository_path=model_repository_path,\nmodel_name=model_name,\nmodel_version=model_version,\nmodel_path=package.workspace / runtime_result.model_status.model_config.path,\nconfig=config,\n)\n</code></pre>"},{"location":"triton/sequence_batcher/","title":"Sequence Batcher","text":""},{"location":"triton/sequence_batcher/#sequence-batcher","title":"Sequence Batcher","text":""},{"location":"triton/sequence_batcher/#model_navigator.api.triton.SequenceBatcher","title":"<code>model_navigator.api.triton.SequenceBatcher</code>  <code>dataclass</code>","text":"<p>Sequence batching configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> Name Type Description Default <code>strategy</code> <code>Optional[Union[SequenceBatcherStrategyDirect, SequenceBatcherStrategyOldest]]</code> <p>The strategy used by the sequence batcher.</p> <code>None</code> <code>max_sequence_idle_microseconds</code> <code>Optional[int]</code> <p>The maximum time, in microseconds, that a sequence is allowed to                             be idle before it is aborted.</p> <code>None</code> <code>control_inputs</code> <code>List[SequenceBatcherControlInput]</code> <p>The model input(s) that the server should use to communicate             sequence start, stop, ready and similar control values to the model.</p> <code>dataclasses.field(default_factory=lambda : [])</code> <code>states</code> <code>List[SequenceBatcherState]</code> <p>The optional state that can be stored in Triton for performing     inference requests on a sequence.</p> <code>dataclasses.field(default_factory=lambda : [])</code>"},{"location":"triton/sequence_batcher/#model_navigator.triton.specialized_configs.common.SequenceBatcher.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/common.py</code> <pre><code>def __post_init__(self):\n\"\"\"Validate the configuration for early error handling.\"\"\"\nif self.strategy and (\nnot isinstance(self.strategy, SequenceBatcherStrategyDirect)\nand not isinstance(self.strategy, SequenceBatcherStrategyOldest)\n):\nraise ModelNavigatorWrongParameterError(\"Unsupported strategy type provided.\")\n</code></pre>"},{"location":"triton/sequence_batcher/#model_navigator.api.triton.SequenceBatcherControl","title":"<code>model_navigator.api.triton.SequenceBatcherControl</code>  <code>dataclass</code>","text":"<p>Sequence Batching control configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> Name Type Description Default <code>kind</code> <code>SequenceBatcherControlKind</code> <p>The kind of this control.</p> required <code>dtype</code> <code>Optional[Union[np.dtype, Type[np.dtype]]]</code> <p>The control's datatype.</p> <code>None</code> <code>int32_false_true</code> <code>List[int]</code> <p>The control's true and false setting is indicated by setting               a value in an int32 tensor.</p> <code>dataclasses.field(default_factory=lambda : [])</code> <code>fp32_false_true</code> <code>List[float]</code> <p>The control's true and false setting is indicated by setting              a value in a fp32 tensor.</p> <code>dataclasses.field(default_factory=lambda : [])</code> <code>bool_false_true</code> <code>List[bool]</code> <p>The control's true and false setting is indicated by setting              a value in a bool tensor.</p> <code>dataclasses.field(default_factory=lambda : [])</code>"},{"location":"triton/sequence_batcher/#model_navigator.triton.specialized_configs.common.SequenceBatcherControl.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/common.py</code> <pre><code>def __post_init__(self):\n\"\"\"Validate the configuration for early error handling.\"\"\"\nif self.kind == SequenceBatcherControlKind.CONTROL_SEQUENCE_CORRID and self.dtype is None:\nraise ModelNavigatorWrongParameterError(f\"The {self.kind} control type requires `dtype` to be specified.\")\nif self.kind == SequenceBatcherControlKind.CONTROL_SEQUENCE_CORRID and any(\n[self.int32_false_true, self.fp32_false_true, self.bool_false_true]\n):\nraise ModelNavigatorWrongParameterError(\nf\"The {self.kind} control type requires `dtype` to be specified only.\"\n)\ncontrols = [\nSequenceBatcherControlKind.CONTROL_SEQUENCE_START,\nSequenceBatcherControlKind.CONTROL_SEQUENCE_END,\nSequenceBatcherControlKind.CONTROL_SEQUENCE_READY,\n]\nif self.kind in controls and self.dtype:\nraise ModelNavigatorWrongParameterError(f\"The {self.kind} control does not support `dtype` parameter.\")\nif self.kind in controls and not (self.int32_false_true or self.fp32_false_true or self.bool_false_true):\nraise ModelNavigatorWrongParameterError(\nf\"The {self.kind} control type requires one of: \"\n\"`int32_false_true`, `fp32_false_true`, `bool_false_true` to be specified.\"\n)\nif self.int32_false_true and len(self.int32_false_true) != 2:\nraise ModelNavigatorWrongParameterError(\n\"The `int32_false_true` field should be two element list with false and true values. Example: [0 , 1]\"\n)\nif self.fp32_false_true and len(self.fp32_false_true) != 2:\nraise ModelNavigatorWrongParameterError(\n\"The `fp32_false_true` field should be two element list with false and true values. Example: [0 , 1]\"\n)\nif self.bool_false_true and len(self.bool_false_true) != 2:\nraise ModelNavigatorWrongParameterError(\n\"The `bool_false_true` field should be two element list with false and true values. \"\n\"Example: [False, True]\"\n)\nif self.dtype:\nself.dtype = cast_dtype(dtype=self.dtype)\nexpect_type(\"dtype\", self.dtype, np.dtype, optional=True)\n</code></pre>"},{"location":"triton/sequence_batcher/#model_navigator.api.triton.SequenceBatcherControlInput","title":"<code>model_navigator.api.triton.SequenceBatcherControlInput</code>  <code>dataclass</code>","text":"<p>Sequence Batching control input configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> Name Type Description Default <code>input_name</code> <code>str</code> <p>The name of the model input.</p> required <code>controls</code> <code>List[SequenceBatcherControl]</code> <p>List of  control value(s) that should be communicated to the       model using this model input.</p> required"},{"location":"triton/sequence_batcher/#model_navigator.api.triton.SequenceBatcherControlKind","title":"<code>model_navigator.api.triton.SequenceBatcherControlKind</code>","text":"<p>         Bases: <code>enum.Enum</code></p> <p>Sequence Batching control options.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> Name Type Description Default <code>CONTROL_SEQUENCE_START</code> <p>\"CONTROL_SEQUENCE_START\"</p> required <code>CONTROL_SEQUENCE_READY</code> <p>\"CONTROL_SEQUENCE_READY\"</p> required <code>CONTROL_SEQUENCE_END</code> <p>\"CONTROL_SEQUENCE_END\"</p> required <code>CONTROL_SEQUENCE_CORRID</code> <p>\"CONTROL_SEQUENCE_CORRID\"</p> required"},{"location":"triton/sequence_batcher/#model_navigator.api.triton.SequenceBatcherInitialState","title":"<code>model_navigator.api.triton.SequenceBatcherInitialState</code>  <code>dataclass</code>","text":"<p>Sequence Batching initial state configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> required <code>shape</code> <code>Tuple[int, ...]</code> <p>The shape of the state tensor, not including the batch dimension.</p> required <code>dtype</code> <code>Optional[Union[np.dtype, Type[np.dtype]]]</code> <p>The data-type of the state.</p> <code>None</code> <code>zero_data</code> <code>Optional[bool]</code> <p>The identifier for using zeros as initial state data.</p> <code>None</code> <code>data_file</code> <code>Optional[str]</code> <p>The file whose content will be used as the initial data for        the state in row-major order.</p> <code>None</code>"},{"location":"triton/sequence_batcher/#model_navigator.triton.specialized_configs.common.SequenceBatcherInitialState.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/common.py</code> <pre><code>def __post_init__(self):\n\"\"\"Validate the configuration for early error handling.\"\"\"\nif not self.zero_data and not self.data_file:\nraise ModelNavigatorWrongParameterError(\"zero_data or data_file has to be defined. None was provided.\")\nif self.zero_data and self.data_file:\nraise ModelNavigatorWrongParameterError(\"zero_data or data_file has to be defined. Both were provided.\")\nif self.dtype:\nself.dtype = cast_dtype(dtype=self.dtype)\nexpect_type(\"name\", self.name, str)\nexpect_type(\"shape\", self.shape, tuple)\nexpect_type(\"dtype\", self.dtype, np.dtype, optional=True)\nis_shape_correct(\"shape\", self.shape)\n</code></pre>"},{"location":"triton/sequence_batcher/#model_navigator.api.triton.SequenceBatcherState","title":"<code>model_navigator.api.triton.SequenceBatcherState</code>  <code>dataclass</code>","text":"<p>Sequence Batching state configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> Name Type Description Default <code>input_name</code> <code>str</code> <p>The name of the model state input.</p> required <code>output_name</code> <code>str</code> <p>The name of the model state output.</p> required <code>dtype</code> <code>Union[np.dtype, Type[np.dtype]]</code> <p>The data-type of the state.</p> required <code>shape</code> <code>Tuple[int, ...]</code> <p>The shape of the state tensor.</p> required <code>initial_states</code> <code>List[SequenceBatcherInitialState]</code> <p>The optional field to specify the list of initial states for the model.</p> <code>dataclasses.field(default_factory=lambda : [])</code>"},{"location":"triton/sequence_batcher/#model_navigator.triton.specialized_configs.common.SequenceBatcherState.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/common.py</code> <pre><code>def __post_init__(self):\n\"\"\"Validate the configuration for early error handling.\"\"\"\nself.dtype = cast_dtype(dtype=self.dtype)\nexpect_type(\"shape\", self.shape, tuple)\nexpect_type(\"dtype\", self.dtype, np.dtype, optional=True)\nis_shape_correct(\"shape\", self.shape)\n</code></pre>"},{"location":"triton/sequence_batcher/#model_navigator.api.triton.SequenceBatcherStrategyDirect","title":"<code>model_navigator.api.triton.SequenceBatcherStrategyDirect</code>  <code>dataclass</code>","text":"<p>Sequence Batching strategy direct configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> Name Type Description Default <code>max_queue_delay_microseconds</code> <code>int</code> <p>The maximum time, in microseconds, a candidate request                           will be delayed in the sequence batch scheduling queue to                           wait for additional requests for batching.</p> <code>0</code> <code>minimum_slot_utilization</code> <code>float</code> <p>The minimum slot utilization that must be satisfied to                       execute the batch before 'max_queue_delay_microseconds' expires.</p> <code>0.0</code>"},{"location":"triton/sequence_batcher/#model_navigator.api.triton.SequenceBatcherStrategyOldest","title":"<code>model_navigator.api.triton.SequenceBatcherStrategyOldest</code>  <code>dataclass</code>","text":"<p>Sequence Batching strategy oldest configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> Name Type Description Default <code>max_candidate_sequences</code> <code>int</code> <p>Maximum number of candidate sequences that the batcher maintains.</p> required <code>preferred_batch_size</code> <code>List[int]</code> <p>Preferred batch sizes for dynamic batching of candidate sequences.</p> <code>dataclasses.field(default_factory=lambda : [])</code> <code>max_queue_delay_microseconds</code> <code>int</code> <p>The maximum time, in microseconds, a candidate request                           will be delayed in the dynamic batch scheduling queue to                           wait for additional requests for batching.</p> <code>0</code>"},{"location":"triton/specialized_configs/","title":"Specialized Configs","text":""},{"location":"triton/specialized_configs/#specialized-configs-for-triton-backends","title":"Specialized Configs for Triton Backends","text":"<p>The Python API provides specialized configuration classes that help provide only available options for the given type of model.</p>"},{"location":"triton/specialized_configs/#model_navigator.api.triton.ONNXModelConfig","title":"<code>model_navigator.api.triton.ONNXModelConfig</code>  <code>dataclass</code>","text":"<p>         Bases: <code>BaseSpecializedModelConfig</code></p> <p>Specialized model config for ONNX backend supported model.</p> <p>Parameters:</p> Name Type Description Default <code>platform</code> <code>Optional[Platform]</code> <p>Override backend parameter with platform.       Possible options: Platform.ONNXRuntimeONNX</p> <code>None</code> <code>optimization</code> <code>Optional[ONNXOptimization]</code> <p>Possible optimization for ONNX models</p> <code>None</code>"},{"location":"triton/specialized_configs/#model_navigator.triton.specialized_configs.onnx_model_config.ONNXModelConfig.backend","title":"<code>backend: Backend</code>  <code>property</code>","text":"<p>Define backend value for config.</p>"},{"location":"triton/specialized_configs/#model_navigator.triton.specialized_configs.onnx_model_config.ONNXModelConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/onnx_model_config.py</code> <pre><code>def __post_init__(self):\n\"\"\"Validate the configuration for early error handling.\"\"\"\nsuper().__post_init__()\nif self.optimization and not isinstance(self.optimization, ONNXOptimization):\nraise ModelNavigatorWrongParameterError(\"Unsupported optimization type provided.\")\nif self.platform and self.platform != Platform.ONNXRuntimeONNX:\nraise ModelNavigatorWrongParameterError(f\"Unsupported platform provided. Use: {Platform.ONNXRuntimeONNX}.\")\n</code></pre>"},{"location":"triton/specialized_configs/#model_navigator.api.triton.ONNXOptimization","title":"<code>model_navigator.api.triton.ONNXOptimization</code>  <code>dataclass</code>","text":"<p>ONNX possible optimizations.</p> <p>Parameters:</p> Name Type Description Default <code>accelerator</code> <code>Union[OpenVINOAccelerator, TensorRTAccelerator]</code> <p>Execution accelerator for model</p> required"},{"location":"triton/specialized_configs/#model_navigator.triton.specialized_configs.onnx_model_config.ONNXOptimization.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/onnx_model_config.py</code> <pre><code>def __post_init__(self):\n\"\"\"Validate the configuration for early error handling.\"\"\"\nif self.accelerator and type(self.accelerator) not in [OpenVINOAccelerator, TensorRTAccelerator]:\nraise ModelNavigatorWrongParameterError(\"Unsupported optimization type provided.\")\n</code></pre>"},{"location":"triton/specialized_configs/#model_navigator.api.triton.PythonModelConfig","title":"<code>model_navigator.api.triton.PythonModelConfig</code>  <code>dataclass</code>","text":"<p>         Bases: <code>BaseSpecializedModelConfig</code></p> <p>Specialized model config for Python backend supported model.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Sequence[InputTensorSpec]</code> <p>Required definition of model inputs</p> <code>dataclasses.field(default_factory=lambda : [])</code> <code>outputs</code> <code>Sequence[OutputTensorSpec]</code> <p>Required definition of model outputs</p> <code>dataclasses.field(default_factory=lambda : [])</code>"},{"location":"triton/specialized_configs/#model_navigator.triton.specialized_configs.python_model_config.PythonModelConfig.backend","title":"<code>backend: Backend</code>  <code>property</code>","text":"<p>Define backend value for config.</p>"},{"location":"triton/specialized_configs/#model_navigator.triton.specialized_configs.python_model_config.PythonModelConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/python_model_config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n\"\"\"Validate the configuration for early error handling.\"\"\"\nsuper().__post_init__()\nassert len(self.inputs) &gt; 0, \"Model inputs definition is required for Python backend.\"\nassert len(self.outputs) &gt; 0, \"Model outputs definition is required for Python backend.\"\n</code></pre>"},{"location":"triton/specialized_configs/#model_navigator.api.triton.PyTorchModelConfig","title":"<code>model_navigator.api.triton.PyTorchModelConfig</code>  <code>dataclass</code>","text":"<p>         Bases: <code>BaseSpecializedModelConfig</code></p> <p>Specialized model config for PyTorch backend supported model.</p> <p>Parameters:</p> Name Type Description Default <code>platform</code> <code>Optional[Platform]</code> <p>Override backend parameter with platform.       Possible options: Platform.PyTorchLibtorch</p> <code>None</code> <code>inputs</code> <code>Sequence[InputTensorSpec]</code> <p>Required definition of model inputs</p> <code>dataclasses.field(default_factory=lambda : [])</code> <code>outputs</code> <code>Sequence[OutputTensorSpec]</code> <p>Required definition of model outputs</p> <code>dataclasses.field(default_factory=lambda : [])</code>"},{"location":"triton/specialized_configs/#model_navigator.triton.specialized_configs.pytorch_model_config.PyTorchModelConfig.backend","title":"<code>backend: Backend</code>  <code>property</code>","text":"<p>Define backend value for config.</p>"},{"location":"triton/specialized_configs/#model_navigator.triton.specialized_configs.pytorch_model_config.PyTorchModelConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/pytorch_model_config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n\"\"\"Validate the configuration for early error handling.\"\"\"\nsuper().__post_init__()\nassert len(self.inputs) &gt; 0, \"Model inputs definition is required for PyTorch backend.\"\nassert len(self.outputs) &gt; 0, \"Model outputs definition is required for PyTorch backend.\"\nif self.platform and self.platform != Platform.PyTorchLibtorch:\nraise ModelNavigatorWrongParameterError(f\"Unsupported platform provided. Use: {Platform.PyTorchLibtorch}.\")\n</code></pre>"},{"location":"triton/specialized_configs/#model_navigator.api.triton.TensorFlowModelConfig","title":"<code>model_navigator.api.triton.TensorFlowModelConfig</code>  <code>dataclass</code>","text":"<p>         Bases: <code>BaseSpecializedModelConfig</code></p> <p>Specialized model config for TensorFlow backend supported model.</p> <p>Parameters:</p> Name Type Description Default <code>platform</code> <code>Optional[Platform]</code> <p>Override backend parameter with platform.       Possible options: Platform.TensorFlowSavedModel, Platform.TensorFlowGraphDef</p> <code>None</code> <code>optimization</code> <code>Optional[TensorFlowOptimization]</code> <p>Possible optimization for TensorFlow models</p> <code>None</code>"},{"location":"triton/specialized_configs/#model_navigator.triton.specialized_configs.tensorflow_model_config.TensorFlowModelConfig.backend","title":"<code>backend: Backend</code>  <code>property</code>","text":"<p>Define backend value for config.</p>"},{"location":"triton/specialized_configs/#model_navigator.triton.specialized_configs.tensorflow_model_config.TensorFlowModelConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/tensorflow_model_config.py</code> <pre><code>def __post_init__(self):\n\"\"\"Validate the configuration for early error handling.\"\"\"\nsuper().__post_init__()\nif self.optimization and not isinstance(self.optimization, TensorFlowOptimization):\nraise ModelNavigatorWrongParameterError(\"Unsupported optimization type provided.\")\nplatforms = [Platform.TensorFlowSavedModel, Platform.TensorFlowGraphDef]\nif self.platform and self.platform not in platforms:\nraise ModelNavigatorWrongParameterError(f\"Unsupported platform provided. Use one of: {platforms}\")\n</code></pre>"},{"location":"triton/specialized_configs/#model_navigator.api.triton.TensorFlowOptimization","title":"<code>model_navigator.api.triton.TensorFlowOptimization</code>  <code>dataclass</code>","text":"<p>TensorFlow possible optimizations.</p> <p>Parameters:</p> Name Type Description Default <code>accelerator</code> <code>Union[AutoMixedPrecisionAccelerator, GPUIOAccelerator, TensorRTAccelerator]</code> <p>Execution accelerator for model</p> required"},{"location":"triton/specialized_configs/#model_navigator.triton.specialized_configs.tensorflow_model_config.TensorFlowOptimization.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/tensorflow_model_config.py</code> <pre><code>def __post_init__(self):\n\"\"\"Validate the configuration for early error handling.\"\"\"\nif self.accelerator and type(self.accelerator) not in [\nAutoMixedPrecisionAccelerator,\nGPUIOAccelerator,\nTensorRTAccelerator,\n]:\nraise ModelNavigatorWrongParameterError(\"Unsupported optimization type provided.\")\n</code></pre>"},{"location":"triton/specialized_configs/#model_navigator.api.triton.TensorRTModelConfig","title":"<code>model_navigator.api.triton.TensorRTModelConfig</code>  <code>dataclass</code>","text":"<p>         Bases: <code>BaseSpecializedModelConfig</code></p> <p>Specialized model config for TensorRT platform supported model.</p> <p>Parameters:</p> Name Type Description Default <code>platform</code> <code>Optional[Platform]</code> <p>Override backend parameter with platform.       Possible options: Platform.TensorRTPlan</p> <code>None</code> <code>optimization</code> <code>Optional[TensorRTOptimization]</code> <p>Possible optimization for TensorRT models</p> <code>None</code>"},{"location":"triton/specialized_configs/#model_navigator.triton.specialized_configs.tensorrt_model_config.TensorRTModelConfig.backend","title":"<code>backend: Backend</code>  <code>property</code>","text":"<p>Define backend value for config.</p>"},{"location":"triton/specialized_configs/#model_navigator.triton.specialized_configs.tensorrt_model_config.TensorRTModelConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/tensorrt_model_config.py</code> <pre><code>def __post_init__(self):\n\"\"\"Validate the configuration for early error handling.\"\"\"\nsuper().__post_init__()\nif self.optimization and not isinstance(self.optimization, TensorRTOptimization):\nraise ModelNavigatorWrongParameterError(\"Unsupported optimization type provided.\")\nif self.platform and self.platform != Platform.TensorRTPlan:\nraise ModelNavigatorWrongParameterError(f\"Unsupported platform provided. Use: {Platform.TensorRTPlan}.\")\n</code></pre>"},{"location":"triton/specialized_configs/#model_navigator.api.triton.TensorRTOptimization","title":"<code>model_navigator.api.triton.TensorRTOptimization</code>  <code>dataclass</code>","text":"<p>TensorRT possible optimizations.</p> <p>Parameters:</p> Name Type Description Default <code>cuda_graphs</code> <code>bool</code> <p>Use CUDA graphs API to capture model operations and execute them more efficiently.</p> <code>False</code> <code>gather_kernel_buffer_threshold</code> <code>Optional[int]</code> <p>The backend may use a gather kernel to gather input data if the                             device has direct access to the source buffer and the destination                             buffer.</p> <code>None</code> <code>eager_batching</code> <code>bool</code> <p>Start preparing the next batch before the model instance is ready for the next inference.</p> <code>False</code>"},{"location":"triton/specialized_configs/#model_navigator.triton.specialized_configs.tensorrt_model_config.TensorRTOptimization.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate the configuration for early error handling.</p> Source code in <code>model_navigator/triton/specialized_configs/tensorrt_model_config.py</code> <pre><code>def __post_init__(self):\n\"\"\"Validate the configuration for early error handling.\"\"\"\nif not self.cuda_graphs and not self.gather_kernel_buffer_threshold and not self.eager_batching:\nraise ModelNavigatorWrongParameterError(\"At least one of the optimization options should be enabled.\")\n</code></pre>"},{"location":"triton/triton_deployment/","title":"Deploying models","text":""},{"location":"triton/triton_deployment/#deploying-a-model-on-the-triton-inference-server","title":"Deploying a model on the Triton Inference Server","text":"<p>Triton Model Navigator provides an API for working with the Triton model repository. Currently, we support adding your own model or a pre-selected model from a Navigator Package.</p> <p>The API only provides possible functionality for the given model's type and only provides offline validation of the provided configuration. In the end, the model with the configuration is created inside the provided model repository path.</p>"},{"location":"triton/triton_deployment/#adding-your-own-model-to-the-triton-model-repository","title":"Adding your own model to the Triton model repository","text":"<p>When you works with an already exported model you can provide a path to where one's model is located. Then you can use one of the specialized APIs that guides you through what options are possible for deployment of tbe selected model type.</p> <p>Example of deploying a TensorRT model:</p> <pre><code>import model_navigator as nav\nnav.triton.model_repository.add_model(\nmodel_repository_path=\"/path/to/triton/model/repository\",\nmodel_path=\"/path/to/model/plan/file\",\nmodel_name=\"NameOfModel\",\nconfig=nav.triton.TensorRTModelConfig(\nmax_batch_size=256,\noptimization=nav.triton.CUDAGraphOptimization(),\nresponse_cache=True,\n)\n)\n</code></pre> <p>The model catalog with the model file and configuration is going to be created inside <code>model_repository_path</code>.</p>"},{"location":"triton/triton_deployment/#adding-model-from-package-to-the-triton-model-repository","title":"Adding model from package to the Triton model repository","text":"<p>When you want to deploy a model from a package created during the <code>optimize</code> process you can use:</p> <pre><code>import model_navigator as nav\nnav.triton.model_repository.add_model_from_package(\nmodel_repository_path=\"/path/to/triton/model/repository\",\nmodel_name=\"NameOfModel\",\npackage=package,\n)\n</code></pre> <p>The model is automatically selected based on profiling results. The default selection options can be adjusted by changing the <code>strategy</code> argument.</p>"},{"location":"triton/triton_deployment/#using-triton-model-analyzer","title":"Using Triton Model Analyzer","text":"<p>A model added to the Triton Inference Server can be further optimized in the target environment using Triton Model Analyzer.</p> <p>Please, follow the documentation to learn more how to use Triton Model Analyzer.</p>"}]}