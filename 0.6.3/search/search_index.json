{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#triton-model-navigator","title":"Triton Model Navigator","text":""},{"location":"#overview","title":"Overview","text":"<p>Model optimization plays a crucial role in unlocking the maximum performance capabilities of the underlying hardware. By applying various transformation techniques, models can be optimized to fully utilize the specific features offered by the hardware architecture to improve the inference performance and cost. Furthermore, in many cases allow for serialization of models, separating them from the source code. The serialization process enhances portability, allowing the models to be seamlessly deployed in production environments. The decoupling of models from the source code also facilitates maintenance, updates, and collaboration among developers. However, this process comprises multiple steps and offers various potential paths, making manual execution complicated and time-consuming.</p> <p>The Triton Model Navigator offers a user-friendly and automated solution for optimizing and deploying machine learning models. Using a single entry point for various supported frameworks, allowing users to start the process of searching for the best deployment option with a single call to the dedicated <code>optimize</code> function. Model Navigator handles model export, conversion, correctness testing, and profiling to select optimal model format and save generated artifacts for inference deployment on the PyTriton or Triton Inference Server.</p> <p>The high-level flowchart below illustrates the process of moving models from source code to deployment optimized formats with the support of the Model Navigator.</p> <p></p>"},{"location":"#support-matrix","title":"Support Matrix","text":"<p>The Model Navigator generates multiple optimized and production-ready models. The table below illustrates the model formats that can be obtained by using the Model Navigator with various frameworks.</p> <p>Table: Supported conversion target formats per each supported Python framework or file.</p> PyTorch TensorFlow 2 JAX ONNX Torch Compile SavedModel SavedModel TensorRT TorchScript Trace TensorRT in TensorFlow TensorRT in TensorFlow TorchScript Script ONNX ONNX Torch-TensorRT TensorRT TensorRT ONNX TensorRT <p>Note: The Model Navigator has the capability to support any Python function as input. However, in this particular case, its role is limited to profiling the function without generating any serialized models.</p> <p>The Model Navigator stores all artifacts within the <code>navigator_workspace</code>. Additionally, it provides an option to save a portable and transferable <code>Navigator Package</code> - an artifact that includes only the models with minimal latency and maximal throughput. This package also includes base formats that can be used to regenerate the <code>TensorRT</code> plan on the target hardware.</p> <p>Table: Model formats that can be generated from saved <code>Navigator Package</code> and from model sources.</p> From model source From Navigator Package SavedModel TorchTensorRT TensorFlowTensorRT TensorFlowTensorRT TorchScript Trace ONNX TorchScript Script TensorRT Torch 2 Compile TorchTensorRT ONNX TensorRT"},{"location":"#what-next","title":"What next?","text":"<p>Learn more about using Model Navigator in quick start where you will find more information about optimizing models and serving inference.</p>"},{"location":"CHANGELOG/","title":"Changelog","text":""},{"location":"CHANGELOG/#changelog","title":"Changelog","text":""},{"location":"CHANGELOG/#063","title":"0.6.3","text":"<ul> <li> <p>fix: Conditional imports of supported frameworks in export commands</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>PyTorch 2.1.0a0+4136153</li> <li>TensorFlow 2.12.0</li> <li>TensorRT 8.6.1</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.47.1</li> <li>GraphSurgeon: 0.3.26</li> <li>tf2onnx v1.14.0</li> <li>Other component versions depend on the used framework containers versions.     See its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#062","title":"0.6.2","text":"<ul> <li>new: Collect information about TensorRT shapes used during conversion</li> <li>fix: Invalid link in documentation</li> <li> <p>change: Improved rendering documentation</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>PyTorch 2.1.0a0+4136153</li> <li>TensorFlow 2.12.0</li> <li>TensorRT 8.6.1</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.47.1</li> <li>GraphSurgeon: 0.3.26</li> <li>tf2onnx v1.14.0</li> <li>Other component versions depend on the used framework containers versions.     See its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#061","title":"0.6.1","text":"<ul> <li> <p>fix: Add model from package to Triton model store with custom configs</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>PyTorch 2.1.0a0+4136153</li> <li>TensorFlow 2.12.0</li> <li>TensorRT 8.6.1</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.47.1</li> <li>GraphSurgeon: 0.3.26</li> <li>tf2onnx v1.14.0</li> <li>Other component versions depend on the used framework containers versions.     See its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#060","title":"0.6.0","text":"<ul> <li>new: Zero-copy runners for Torch, ONNX and TensorRT - omit H2D and D2H memory copy between runners execution</li> <li>new: <code>nav.pacakge.profile</code> API method to profile generated models on provided dataloader</li> <li>change: ProfilerConfig replaced with OptimizationProfile:</li> <li>new: OptimizationProfile impact the conversion for TensorRT</li> <li>new: <code>batch_sizes</code> and <code>max_batch_size</code> limit the max profile in TensorRT conversion</li> <li>new: Allow to provide separate dataloader for profiling - first sample used only</li> <li>new: allow to run <code>nav.package.optimize</code> on empty package - status generation only</li> <li>new: use <code>torch.inference_mode</code> for inference runner when PyTorch 2.x is available</li> <li>fix: Missing <code>model</code> in config when passing package generated during <code>nav.{framework}.optimize</code> directly to <code>nav.package.optimize</code> command</li> <li> <p>Other minor fixes and improvements</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>PyTorch 2.1.0a0+4136153</li> <li>TensorFlow 2.12.0</li> <li>TensorRT 8.6.1</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.47.1</li> <li>GraphSurgeon: 0.3.26</li> <li>tf2onnx v1.14.0</li> <li>Other component versions depend on the used framework containers versions.     See its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#056","title":"0.5.6","text":"<ul> <li>fix: Load samples as sorted to keep valid order</li> <li>fix: Execute conversion when model already exists in path</li> <li> <p>Other minor fixes and improvements</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>PyTorch 2.1.0a0+fe05266f</li> <li>TensorFlow 2.12.0</li> <li>TensorRT 8.6.1</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.47.1</li> <li>GraphSurgeon: 0.3.26</li> <li>tf2onnx v1.14.0</li> <li>Other component versions depend on the used framework containers versions.     See its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#055","title":"0.5.5","text":"<ul> <li>new: Public <code>nav.utilities</code> module with UnpackedDataloader wrapper</li> <li>new: Added support for strict flag in Torch custom config</li> <li>new: Extended TensorRT custom config to support builder optimization level and hardware compatibility flags</li> <li> <p>fix: Invalid optimal shape calculation for odd values in max batch size</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>PyTorch 2.1.0a0+fe05266f</li> <li>TensorFlow 2.12.0</li> <li>TensorRT 8.6.1</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.47.1</li> <li>GraphSurgeon: 0.3.26</li> <li>tf2onnx v1.14.0</li> <li>Other component versions depend on the used framework containers versions.     See its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#054","title":"0.5.4","text":"<ul> <li>new: Custom implementation for ONNX and TensorRT runners</li> <li>new: Use CUDA 12 for JAX in unit tests and functional tests</li> <li>new: Step-by-step examples</li> <li>new: Updated documentation</li> <li>new: TensorRTCUDAGraph runner introduced with support for CUDA graphs</li> <li>fix: Optimal shape not set correctly during adaptive conversion</li> <li>fix: Find max batch size command for JAX</li> <li> <p>fix: Save stdout to logfiles in debug mode</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>PyTorch 2.1.0a0+fe05266f</li> <li>TensorFlow 2.12.0</li> <li>TensorRT 8.6.1</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.47.1</li> <li>GraphSurgeon: 0.3.26</li> <li>tf2onnx v1.14.0</li> <li>Other component versions depend on the used framework containers versions.     See its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#053","title":"0.5.3","text":"<ul> <li> <p>fix: filter outputs using output_metadata in ONNX runners</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>PyTorch 2.0.0a0+1767026</li> <li>TensorFlow 2.11.0</li> <li>TensorRT 8.5.3.1</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.44.2</li> <li>GraphSurgeon: 0.3.26</li> <li>tf2onnx v1.14.0</li> <li>Other component versions depend on the used framework containers versions.     See its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#052","title":"0.5.2","text":"<ul> <li>new: Added Contributor License Agreement (CLA)</li> <li>fix: Added missing --extra-index-url to installation instruction for pypi</li> <li>fix: Updated wheel readme</li> <li>fix: Do not run TorchScript export when only ONNX in target formats and ONNX extended export is disabled</li> <li> <p>fix: Log full traceback for ModelNavigatorUserInputError</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>PyTorch 2.0.0a0+1767026</li> <li>TensorFlow 2.11.0</li> <li>TensorRT 8.5.3.1</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.44.2</li> <li>GraphSurgeon: 0.3.26</li> <li>tf2onnx v1.14.0</li> <li>Other component versions depend on the used framework containers versions.     See its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#051","title":"0.5.1","text":"<ul> <li>fix: Using relative workspace cause error during Onnx to TensorRT conversion</li> <li>fix: Added external weight in package for ONNX format</li> <li> <p>fix: bugfixes for functional tests</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>PyTorch 1.14.0a0+410ce96</li> <li>TensorFlow 2.11.0</li> <li>TensorRT 8.5.3</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.44.2</li> <li>GraphSurgeon: 0.4.6</li> <li>tf2onnx v1.13.0</li> <li>Other component versions depend on the used framework containers versions.     See its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#050","title":"0.5.0","text":"<ul> <li>new: Support for PyTriton deployment</li> <li>new: Support for Python models with python.optimize API</li> <li>new: PyTorch 2 compile CPU and CUDA runners</li> <li>new: Collect conversion max batch size in status</li> <li>new: PyTorch runners with <code>compile</code> support</li> <li>change: Improved handling CUDA and CPU runners</li> <li>change: Reduced finding device max batch size time by running it once as separate pipeline</li> <li> <p>change: Stored find max batch size result in separate filed in status</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>PyTorch 1.14.0a0+410ce96</li> <li>TensorFlow 2.11.0</li> <li>TensorRT 8.5.3</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.44.2</li> <li>GraphSurgeon: 0.4.6</li> <li>tf2onnx v1.13.0</li> <li>Other component versions depend on the used framework containers versions.     See its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#044","title":"0.4.4","text":"<ul> <li> <p>fix: when exporting single input model to saved model, unwrap one element list with inputs</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>PyTorch 1.14.0a0+410ce96</li> <li>TensorFlow 2.11.0</li> <li>TensorRT 8.5.3</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.44.2</li> <li>GraphSurgeon: 0.4.6</li> <li>tf2onnx v1.13.0</li> <li>Other component versions depend on the used framework containers versions.     See its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#043","title":"0.4.3","text":"<ul> <li> <p>fix: in Keras inference use model.predict(tensor) for single input models</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>PyTorch 1.14.0a0+410ce96</li> <li>TensorFlow 2.11.0</li> <li>TensorRT 8.5.3</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.44.2</li> <li>GraphSurgeon: 0.4.6</li> <li>tf2onnx v1.13.0</li> <li>Other component versions depend on the used framework containers versions.     See its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#042","title":"0.4.2","text":"<ul> <li>fix: loading configuration for trt_profile from package</li> <li>fix: missing reproduction scripts and logs inside package</li> <li>fix: invalid model path in reproduction script for ONNX to TRT conversion</li> <li> <p>fix: collecting metadata from ONNX model in main thread during ONNX to TRT conversion</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>PyTorch 1.14.0a0+410ce96</li> <li>TensorFlow 2.11.0</li> <li>TensorRT 8.5.3</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.44.2</li> <li>GraphSurgeon: 0.4.6</li> <li>tf2onnx v1.13.0</li> <li>Other component versions depend on the used framework containers versions.     See its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#041","title":"0.4.1","text":"<ul> <li> <p>fix: when specified use dynamic axes from custom OnnxConfig</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>PyTorch 1.14.0a0+410ce96</li> <li>TensorFlow 2.11.0</li> <li>TensorRT 8.5.2.2</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.43.1</li> <li>GraphSurgeon: 0.4.6</li> <li>tf2onnx v1.13.0</li> <li>Other component versions depend on the used framework containers versions.     See its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#040","title":"0.4.0","text":"<ul> <li>new: <code>optimize</code> method that replace <code>export</code> and perform max batch size search and improved profiling during process</li> <li>new: Introduced custom configs in <code>optimize</code> for better parametrization of export/conversion commands</li> <li>new: Support for adding user runners for model correctness and profiling</li> <li>new: Search for max possible batch size per format during conversion and profiling</li> <li>new: API for creating Triton model store from Navigator Package and user provided models</li> <li>change: Improved status structure for Navigator Package</li> <li>deprecated: Optimize for Triton Inference Server support</li> <li>deprecated: HuggingFace contrib module</li> <li> <p>Bug fixes and other improvements</p> </li> <li> <p>Version of external components used during testing:</p> </li> <li>PyTorch 1.14.0a0+410ce96</li> <li>TensorFlow 2.11.0</li> <li>TensorRT 8.5.2.2</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.43.1</li> <li>GraphSurgeon: 0.4.6</li> <li>tf2onnx v1.13.0</li> <li>Other component versions depend on the used framework containers versions.     See its support matrix     for a detailed summary.</li> </ul>"},{"location":"CHANGELOG/#038","title":"0.3.8","text":"<ul> <li> <p>Updated NVIDIA containers defaults to 22.11</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.42.2</li> <li>GraphSurgeon: 0.3.19</li> <li>Triton Model Analyzer 1.20.0</li> <li>tf2onnx: v1.12.1</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#037","title":"0.3.7","text":"<ul> <li> <p>Updated NVIDIA containers defaults to 22.10</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.42.2</li> <li>GraphSurgeon: 0.3.19</li> <li>Triton Model Analyzer 1.20.0</li> <li>tf2onnx: v1.12.1</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#036","title":"0.3.6","text":"<ul> <li>Updated NVIDIA containers defaults to 22.09</li> <li> <p>Model Navigator Export API:</p> <ul> <li>new: cast int64 input data to int32 in runner for Torch-TensorRT</li> <li>new: cast 64-bit data samples to 32-bit values for TensorRT</li> <li>new: verbose flag for logging export and conversion commands to console</li> <li>new: debug flag to enable debug mode for export and conversion commands</li> <li>change: logs from commands are streamed to console during command run</li> <li>change: package load omit the log files and autogenerated scripts</li> </ul> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.42.2</li> <li>GraphSurgeon: 0.3.19</li> <li>Triton Model Analyzer 1.20.0</li> <li>tf2onnx: v1.12.1</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#035","title":"0.3.5","text":"<ul> <li>Updated NVIDIA containers defaults to 22.08</li> <li> <p>Model Navigator Export API:</p> <ul> <li>new: TRTExec runner use <code>use_cuda_graph=True</code> by default</li> <li>new: log warning instead of raising error when dataloader dump inputs with <code>nan</code> or <code>inf</code> values</li> <li>new: enabled logging for command input parameters</li> <li>fix: invalid use of Polygraphy TRT profile when trt_dynamic_axes is passed to export function</li> </ul> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.38.0</li> <li>GraphSurgeon: 0.3.19</li> <li>Triton Model Analyzer 1.19.0</li> <li>tf2onnx: v1.12.1</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#034","title":"0.3.4","text":"<ul> <li>Updated NVIDIA containers defaults to 22.07</li> <li>Model Navigator OTIS:<ul> <li>deprecated: <code>TF32</code> precision for TensorRT from CLI options - will be removed in future versions</li> <li>fix: Tensorflow module was imported when obtaining model signature during conversion</li> </ul> </li> <li> <p>Model Navigator Export API:</p> <ul> <li>new: Support for building framework containers with Model Navigator installed</li> <li>new: Example for loading Navigator Package for reproducing the results</li> <li>new: Create reproducing script for correctness and performance steps</li> <li>new: TrtexecRunner for correctness and performance tests   with trtexec tool</li> <li>new: Use TF32 support by default for models with FP32 precision</li> <li>new: Reset conversion parameters to defaults when using <code>load</code> for package</li> <li>new: Testing all options for JAX export enable_xla and jit_compile parameters</li> <li>change: Profiling stability improvements</li> <li>change: Rename of <code>onnx_runtimes</code> export function parameters to <code>runtimes</code></li> <li>deprecated: <code>TF32</code> precision for TensorRT from available options - will be removed in future versions</li> <li>fix: Do not save TF-TRT models to the .nav package</li> <li>fix: Do not save TF-TRT models from the .nav package</li> <li>fix: Correctly load .nav packages when <code>_input_names</code> or <code>_output_names</code> specified</li> <li>fix: Adjust TF and TF-TRT model signatures to match <code>input_names</code></li> <li>fix: Save ONNX opset for CLI configuration inside package</li> <li>fix: Reproduction scripts were missing for failing paths</li> </ul> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.38.0</li> <li>GraphSurgeon: 0.3.19</li> <li>Triton Model Analyzer 1.17.0</li> <li>tf2onnx: v1.11.1</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#033","title":"0.3.3","text":"<ul> <li> <p>Model Navigator Export API:</p> <ul> <li>new: Improved handling inputs and outputs metadata</li> <li>new: Navigator Package version updated to 0.1.3</li> <li>new: Backward compatibility with previous versions of Navigator Package</li> <li>fix: Dynamic shapes for output shapes were read incorrectly</li> </ul> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.36.2</li> <li>GraphSurgeon: 0.3.19</li> <li>Triton Model Analyzer 1.17.0</li> <li>tf2onnx: v1.11.1</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#032","title":"0.3.2","text":"<ul> <li>Updated NVIDIA containers defaults to 22.06</li> <li>Model Navigator OTIS:<ul> <li>new: Perf Analyzer profiling data use base64 format for content</li> <li>fix: Signature for TensorRT model when has <code>uint64</code> or <code>int64</code> input and/or outputs defined</li> </ul> </li> <li> <p>Model Navigator Export API:</p> <ul> <li>new: Updated navigator package format to 0.1.1</li> <li>new: Added Model Navigator version to status file</li> <li>new: Add atol and rtol configuration to CLI config for model</li> <li>new: Added experimental support for JAX models</li> <li>new: In case of export or conversion failures prepare minimal scripts to reproduce errors</li> <li>fix: Conversion parameters are not stored in Navigator Package for CLI execution</li> </ul> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.36.2</li> <li>GraphSurgeon: 0.3.19</li> <li>Triton Model Analyzer 1.17.0</li> <li>tf2onnx: v1.11.1</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#031","title":"0.3.1","text":"<ul> <li>Updated NVIDIA containers defaults to 22.05</li> <li>Model Navigator OTIS:<ul> <li>fix: Saving paths inside the Triton package status file</li> <li>fix: Empty list of gpus cause the process run on CPU only</li> <li>fix: Reading content from zipped Navigator Package</li> <li>fix: When no GPU or target device set to CPU <code>optimize</code> avoid running unsupported conversions in CLI</li> <li>new: Converter accept passing target device kind to selected CPU or GPU supported conversions</li> <li>new: Added support for OpenVINO accelerator for ONNXRuntime</li> <li>new: Added option <code>--config-search-early-exit-enable</code> for Model Analyzer early exit support   in manual profiling mode</li> <li>new: Added option <code>--model-config-name</code> to the <code>select</code> command.   It allows to pick a particular model configuration for deployment from the set of all configurations   generated by Triton Model Analyzer, even if it's not the best performing one.</li> <li>removed: The <code>--tensorrt-strict-types</code> option has been removed due to deprecation of the functionality   in upstream libraries.</li> </ul> </li> <li> <p>Model Navigator Export API:</p> <ul> <li>new: Added dynamic shapes support and trt dynamic shapes support for TensorFlow2 export</li> <li>new: Improved per format logging</li> <li>new: PyTorch to Torch-TRT precision selection added</li> <li>new: Advanced profiling (measurement windows, configurable batch sizes)</li> </ul> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.36.2</li> <li>GraphSurgeon: 0.3.19</li> <li>Triton Model Analyzer 1.16.0</li> <li>tf2onnx: v1.10.1</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#030","title":"0.3.0","text":"<ul> <li>Updated NVIDIA containers defaults to 22.04</li> <li>Model Navigator Export API<ul> <li>Support for exporting models from TensorFlow2 and PyTorch source code to supported target formats</li> <li>Support for conversion from ONNX to supported target formats</li> <li>Support for exporting HuggingFace models</li> <li>Conversion, Correctness and performance tests for exported models</li> <li>Definition of package structure for storing all exported models and additional metadata</li> </ul> </li> <li>Model Navigator OTIS:<ul> <li>change: <code>run</code> command has been deprecated and may be removed in a future release</li> <li>new: <code>optimize</code> command replace <code>run</code> and produces an output <code>*.triton.nav</code> package</li> <li>new: <code>select</code> selects the best-performing configuration from <code>*.triton.nav</code> package and create a   Triton Inference Server model repository</li> <li>new: Added support for using shared memory option for Perf Analyzer</li> </ul> </li> <li> <p>Remove wkhtmltopdf package dependency</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.35.1</li> <li>GraphSurgeon: 0.3.14</li> <li>Triton Model Analyzer 1.14.0</li> <li>tf2onnx: v1.9.3</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#027","title":"0.2.7","text":"<ul> <li>Updated NVIDIA containers defaults to 22.02</li> <li>Removed support for Python 3.7</li> <li>Triton Model configuration related:<ul> <li>Support dynamic batching without setting preferred batch size value</li> </ul> </li> <li> <p>Profiling related:</p> <ul> <li>Deprecated <code>--config-search-max-preferred-batch-size</code> flag as is no longer supported in Triton Model Analyzer</li> </ul> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.35.1</li> <li>GraphSurgeon: 0.3.14</li> <li>Triton Model Analyzer 1.8.2</li> <li>tf2onnx: v1.9.3</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#026","title":"0.2.6","text":"<ul> <li>Updated NVIDIA containers defaults to 22.01</li> <li>Removed support for Python 3.6 due to EOL</li> <li>Conversion related:<ul> <li>Added support for Torch-TensorRT conversion</li> </ul> </li> <li> <p>Fixes and improvements</p> <ul> <li>Processes inside containers started by Model Navigator now run without root privileges</li> <li>Fix for volume mounts while running Triton Inference Server in container from other container</li> <li>Fix for conversion of models without file extension on input and output paths</li> <li>Fix using <code>--model-format</code> argument when input and output files have no extension</li> </ul> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.35.1</li> <li>GraphSurgeon: 0.3.14</li> <li>Triton Model Analyzer 1.8.2</li> <li>tf2onnx: v1.9.3</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> <li> <p>Known issues and limitations</p> <ul> <li>missing support for stateful models (ex. time-series one)</li> <li>no verification of conversion results for conversions: TF -&gt; ONNX, TF-&gt;TF-TRT, TorchScript -&gt; ONNX</li> <li>possible to define a single profile for TensorRT</li> <li>no custom ops support</li> <li>Triton Inference Server stays in the background when the profile   process is interrupted by the user</li> <li>TF-TRT conversion lost outputs shapes info</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#025","title":"0.2.5","text":"<ul> <li>Updated NVIDIA containers defaults to 21.12</li> <li>Conversion related:<ul> <li>[Experimental] TF-TRT - fixed default dataset profile generation</li> </ul> </li> <li> <p>Configuration Model on Triton related</p> <ul> <li>Fixed name for onnxruntime backend in Triton model deployment configuration</li> </ul> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.33.1</li> <li>GraphSurgeon: 0.3.14</li> <li>Triton Model Analyzer 1.8.2</li> <li>tf2onnx: v1.9.3</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> <li> <p>Known issues and limitations</p> <ul> <li>missing support for stateful models (ex. time-series one)</li> <li>no verification of conversion results for conversions: TF -&gt; ONNX, TF-&gt;TF-TRT, TorchScript -&gt; ONNX</li> <li>possible to define a single profile for TensorRT</li> <li>no custom ops support</li> <li>Triton Inference Server stays in the background when the profile   process is interrupted by the user</li> <li>TF-TRT conversion lost outputs shapes info</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#024-2021-12-07","title":"0.2.4 (2021-12-07)","text":"<ul> <li>Updated NVIDIA containers defaults to 21.10</li> <li>Fixed generating profiling data when <code>dtypes</code> are not passed</li> <li>Conversion related:<ul> <li>[Experimental] Added support for TF-TRT conversion</li> </ul> </li> <li>Configuration Model on Triton related<ul> <li>Added possibility to select batching mode - default, dynamic and disabled options supported</li> </ul> </li> <li>Install dependencies from pip packages instead of wheels for Polygraphy and Triton Model Analyzer</li> <li> <p>fixes and improvements</p> </li> <li> <p>Version of external components used during testing:</p> <ul> <li>Polygraphy: 0.33.1</li> <li>GraphSurgeon: 0.3.14</li> <li>Triton Model Analyzer 1.8.2</li> <li>tf2onnx: v1.9.3</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> <li> <p>Known issues and limitations</p> <ul> <li>missing support for stateful models (ex. time-series one)</li> <li>no verification of conversion results for conversions: TF -&gt; ONNX, TF-&gt;TF-TRT, TorchScript -&gt; ONNX</li> <li>possible to define a single profile for TensorRT</li> <li>no custom ops support</li> <li>Triton Inference Server stays in the background when the profile   process is interrupted by the user</li> <li>TF-TRT conversion lost outputs shapes info</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#023-2021-11-10","title":"0.2.3 (2021-11-10)","text":"<ul> <li>Updated NVIDIA containers defaults to 21.09</li> <li>Improved naming of arguments specific for TensorRT conversion and acceleration with backward compatibility</li> <li>Use pip package for Triton Model Analyzer installation with minimal version 1.8.0</li> <li>Fixed <code>model_repository</code> path to be not relative to <code>&lt;navigator_workspace&gt;</code> dir</li> <li>Handle exit codes correctly from CLI commands</li> <li>Support for use device ids for <code>--gpus</code> argument</li> <li>Conversion related<ul> <li>Added support for precision modes to support multiple precisions during conversion to TensorRT</li> <li>Added <code>--tensorrt-sparse-weights</code> flag for sparse weight optimization for TensorRT</li> <li>Added <code>--tensorrt-strict-types</code> flag forcing it to choose tactics based on the layer precision for TensorRT</li> <li>Added <code>--tensorrt-explicit-precision</code> flag enabling explicit precision mode</li> <li>Fixed nan values appearing in relative tolerance during conversion to TensorRT</li> </ul> </li> <li>Configuration Model on Triton related<ul> <li>Removed default value for <code>engine_count_per_device</code></li> <li>Added possibility to define Triton Custom Backend parameters with <code>triton_backend_parameters</code> command</li> <li>Added possibility to define max workspace size for TensorRT backend accelerator using   argument <code>tensorrt_max_workspace_size</code></li> </ul> </li> <li>Profiling related<ul> <li>Added <code>config_search</code> prefix to all profiling parameters (BREAKING CHANGE)</li> <li>Added <code>config_search_max_preferred_batch_size</code> parameter</li> <li>Added <code>config_search_backend_parameters</code> parameter</li> </ul> </li> <li> <p>fixes and improvements</p> </li> <li> <p>Versions of used external components:</p> <ul> <li>Polygraphy: 0.32.0</li> <li>GraphSurgeon: 0.3.13</li> <li>tf2onnx: v1.9.2 (support for ONNX opset 14,   tf 1.15 and 2.6)</li> <li>Triton Model Analyzer 1.8.2</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> <li> <p>Known issues and limitations</p> <ul> <li>missing support for stateful models (ex. time-series one)</li> <li>missing support for models without batching support</li> <li>no verification of conversion results for conversions: TF -&gt; ONNX, TorchScript -&gt; ONNX</li> <li>possible to define a single profile for TensorRT</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#022-2021-09-06","title":"0.2.2 (2021-09-06)","text":"<ul> <li> <p>Updated NVIDIA containers defaults to 21.08</p> </li> <li> <p>Versions of used external components:</p> <ul> <li>Triton Model Analyzer: 1.7.0</li> <li>Triton Inference Server Client: 2.13.0</li> <li>Polygraphy: 0.31.1</li> <li>GraphSurgeon: 0.3.11</li> <li>tf2onnx: v1.9.1 (support for ONNX opset 14,   tf 1.15 and 2.5)</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> <li> <p>Known issues and limitations</p> <ul> <li>missing support for stateful models (ex. time-series one)</li> <li>missing support for models without batching support</li> <li>no verification of conversion results for conversions: TF -&gt; ONNX, TorchScript -&gt; ONNX</li> <li>possible to define a single profile for TensorRT</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#021-2021-08-17","title":"0.2.1 (2021-08-17)","text":"<ul> <li>Fixed triton-model-config error when tensorrt_capture_cuda_graph flag is not passed</li> <li>Dump Conversion Comparator inputs and outputs into JSON files</li> <li>Added information in logs on the tolerance parameters values to pass the conversion verification</li> <li>Use <code>count_windows</code> mode as default option for Perf Analyzer</li> <li>Added possibility to define custom docker images</li> <li> <p>Bugfixes</p> </li> <li> <p>Versions of used external components:</p> <ul> <li>Triton Model Analyzer: 1.6.0</li> <li>Triton Inference Server Client: 2.12.0</li> <li>Polygraphy: 0.31.1</li> <li>GraphSurgeon: 0.3.11</li> <li>tf2onnx: v1.9.1 (support for ONNX opset 14,   tf 1.15 and 2.5)</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> <li> <p>Known issues and limitations</p> <ul> <li>missing support for stateful models (ex. time-series one)</li> <li>missing support for models without batching support</li> <li>no verification of conversion results for conversions: TF -&gt; ONNX, TorchScript -&gt; ONNX</li> <li>possible to define a single profile for TensorRT</li> <li>TensorRT backend acceleration not supported for ONNX Runtime in Triton Inference Server ver. 21.07</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#020-2021-07-05","title":"0.2.0 (2021-07-05)","text":"<ul> <li> <p>comprehensive refactor of command-line API in order to provide more gradual   pipeline steps execution</p> </li> <li> <p>Versions of used external components:</p> <ul> <li>Triton Model Analyzer: 21.05</li> <li>tf2onnx: v1.8.5 (support for ONNX opset 13,   tf 1.15 and 2.5)</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   See its support matrix   for a detailed summary.</li> </ul> </li> <li> <p>Known issues and limitations</p> <ul> <li>missing support for stateful models (ex. time-series one)</li> <li>missing support for models without batching support</li> <li>no verification of conversion results for conversions: TF -&gt; ONNX, TorchScript -&gt; ONNX</li> <li>issues with TorchScript -&gt; ONNX conversion due   to issue in PyTorch 1.8<ul> <li>affected NVIDIA PyTorch containers: 20.12, 21.02, 21.03</li> <li>workaround: use PyTorch containers newer than 21.03</li> </ul> </li> <li>possible to define a single profile for TensorRT</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#011-2021-04-12","title":"0.1.1 (2021-04-12)","text":"<ul> <li>documentation update</li> </ul>"},{"location":"CHANGELOG/#010-2021-04-09","title":"0.1.0 (2021-04-09)","text":"<ul> <li> <p>Release of main components:</p> <ul> <li>Model Converter - converts the model to a set of variants optimized for inference or to be later optimized by   Triton Inference Server backend.</li> <li>Model Repo Builder - setup Triton Inference Server Model Repository, including its configuration.</li> <li>Model Analyzer - select optimal Triton Inference Server configuration based on models compute and memory   requirements,   available computation infrastructure, and model application constraints.</li> <li>Helm Chart Generator - deploy Triton Inference Server and model with optimal configuration to cloud.</li> </ul> </li> <li> <p>Versions of used external components:</p> <ul> <li>Triton Model Analyzer: 21.03+616e8a30</li> <li>tf2onnx: v1.8.4 (support for ONNX opset 13, tf 1.15   and 2.4)</li> <li>Other component versions depend on the used framework and Triton Inference Server containers versions.   Refer to its support matrix   for a detailed summary.</li> </ul> </li> <li> <p>Known issues</p> <ul> <li>missing support for stateful models (ex. time-series one)</li> <li>missing support for models without batching support</li> <li>no verification of conversion results for conversions: TF -&gt; ONNX, TorchScript -&gt; ONNX</li> <li>issues with TorchScript -&gt; ONNX conversion due   to issue in PyTorch 1.8<ul> <li>affected NVIDIA PyTorch containers: 20.12, 21.03</li> <li>workaround: use containers different from above</li> </ul> </li> <li>Triton Inference Server stays in the background when the profile process is interrupted by the user</li> </ul> </li> </ul>"},{"location":"CONTRIBUTING/","title":"Contributing","text":""},{"location":"CONTRIBUTING/#contributing","title":"Contributing","text":"<p>Contributions are welcome, and they are much appreciated! Every little helps, and we will always give credit.</p>"},{"location":"CONTRIBUTING/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"CONTRIBUTING/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/triton-inference-server/model_navigator/issues.</p> <p>If you are reporting a bug, include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"CONTRIBUTING/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"CONTRIBUTING/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"CONTRIBUTING/#write-documentation","title":"Write Documentation","text":"<p>The Triton Model Navigator could always use more documentation, whether as part of the official Triton Model Navigator docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"CONTRIBUTING/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/triton-inference-server/model_navigator/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible to make it easier to implement.</li> </ul>"},{"location":"CONTRIBUTING/#sign-your-work","title":"Sign your Work","text":"<p>We require that all contributors \"sign-off\" on their commits. This certifies that the contribution is your original work, or you have the rights to submit it under the same license or a compatible license.</p> <p>Any contribution which contains commits that are not Signed-Off will not be accepted.</p> <p>To sign off on a commit, you simply use the <code>--signoff</code> (or <code>-s</code>) option when committing your changes: <pre><code>$ git commit -s -m \"Add cool feature.\n</code></pre></p> <p>This will append the following to your commit message:</p> <pre><code>Signed-off-by: Your Name &lt;your@email.com&gt;\n</code></pre> <p>By doing this, you certify the below:</p> <pre><code>Developer Certificate of Origin\nVersion 1.1\n\nCopyright (C) 2004, 2006 The Linux Foundation and its contributors.\n1 Letterman Drive\nSuite D4700\nSan Francisco, CA, 94129\n\nEveryone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.\n\nDeveloper's Certificate of Origin 1.1\n\nBy making a contribution to this project, I certify that:\n\n(a) The contribution was created in whole or in part by me and I have the right to submit it under the open source license indicated in the file; or\n\n(b) The contribution is based upon previous work that, to the best of my knowledge, is covered under an appropriate open source license and I have the right under that license to submit that work with modifications, whether created in whole or in part by me, under the same open source license (unless I am permitted to submit under a different license), as indicated in the file; or\n\n(c) The contribution was provided directly to me by some other person who certified (a), (b) or (c) and I have not modified it.\n\n(d) I understand and agree that this project and the contribution are public and that a record of the contribution (including all personal information I submit with it, including my sign-off) is maintained indefinitely and may be redistributed consistent with this project or the open source license(s) involved.\n</code></pre>"},{"location":"CONTRIBUTING/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up the <code>Triton Model Navigator</code> for local development.</p> <ol> <li>Fork the <code>Triton Model Navigator</code> repo on GitHub.</li> <li>Clone your fork locally:</li> </ol> <pre><code>$ git clone git@github.com:your_name_here/model-navigator.git\n</code></pre> <ol> <li>Install your local copy into a virtualenv. Assuming you have virtualenvwrapper installed, this is how you set up your fork for local development:</li> </ol> <pre><code>$ mkvirtualenv model_navigator\n$ cd model_navigator/\n$ make install-dev\n</code></pre> <ol> <li>Create a branch for local development:</li> </ol> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> <ol> <li>When you're done making changes, check that your changes pass linters and the    tests, including testing other Python versions with tox:</li> </ol> <pre><code>$ make lint  # will run i.a. flake8 and pytype linters\n$ make test  # will run a test with on your current virtualenv\n$ make test-fw  # will run a framework test inside framework container\n</code></pre> <ol> <li>Commit your changes and push your branch to GitHub:</li> </ol> <pre><code>$ git add .\n$ git commit -s -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> <ol> <li>Submit a pull request through the GitHub website.</li> </ol>"},{"location":"CONTRIBUTING/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, you should update the docs. Put    your new functionality into a function with a docstring, and add the    feature to the list in README.md.</li> </ol>"},{"location":"CONTRIBUTING/#tips","title":"Tips","text":"<p>To run a subset of tests:</p> <pre><code>$ pytest tests.test_model_navigator\n</code></pre>"},{"location":"CONTRIBUTING/#releasing","title":"Releasing","text":"<p>As a reminder for the maintainers on how to deploy - make sure all your changes are committed (including an entry in CHANGELOG.md) into the master branch. Then run:</p> <pre><code>$ bump2version patch # possible: major / minor / patch\n$ git push\n$ git push --tags\n</code></pre>"},{"location":"CONTRIBUTING/#documentation","title":"Documentation","text":"<p>Add/update docstrings as defined in Google Style Guide.</p>"},{"location":"CONTRIBUTING/#contributor-license-agreement-cla","title":"Contributor License Agreement (CLA)","text":"<p>Triton requires that all contributors (or their corporate entity) send a signed copy of the Contributor License Agreement to triton-cla@nvidia.com. NOTE: Contributors with no company affiliation can fill <code>N/A</code> in the <code>Corporation Name</code> and <code>Corporation Address</code> fields.</p>"},{"location":"LICENSE/","title":"License","text":"<pre><code>                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n</code></pre> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol>"},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#examples","title":"Examples","text":"<p>We provide step-by-step examples that demonstrate how to use various features of Model Navigator. For the sake of readability and accessibility, we use a simple <code>torch.nn.Linear</code> model as an example. These examples illustrate how to optimize, test and deploy the model on the PyTriton and Triton Inference Server.</p>"},{"location":"examples/#step-by-step-examples","title":"Step-by-step examples","text":"<ol> <li>Optimize model</li> <li>Optimize model and verify model</li> <li>Optimize model and save package</li> <li>Load and optimize package</li> <li>Optimize and server model on PyTriton</li> <li>Optimize and serve model on Triton Inference Server</li> <li>Optimize model and use for offline inference</li> <li>Optimize PyTorch HiFi-GAN QAT model</li> <li>Custom configuration for optimize</li> </ol>"},{"location":"examples/#example-models","title":"Example models","text":"<p>Inside example/models directory you can find ready to use example models in various frameworks.</p> <p><code>Python</code>: - Identity Model</p> <p><code>PyTorch</code>:</p> <ul> <li>Linear Model</li> <li>ResNet50</li> <li>BERT</li> </ul> <p><code>TensorFlow</code>:</p> <ul> <li>Linear Model</li> <li>EfficientNet</li> <li>BERT</li> </ul> <p><code>JAX</code>:</p> <ul> <li>Linear Model</li> <li>GPT-2</li> </ul> <p><code>ONNX</code>:</p> <ul> <li>Identity Model</li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation","title":"Installation","text":"<p>This section describes how to install the tool. We assume you are comfortable with Python programming language and familiar with Machine Learning models.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>The following prerequisites must be fulfilled to use Triton Model Navigator</p> <ul> <li>Installed Python <code>3.8+</code></li> <li>Installed NVIDIA TensorRT for TensorRT models export.</li> </ul> <p>We recommend to use NGC Containers for PyTorch and TensorFlow which provide have all necessary dependencies:</p> <ul> <li>PyTorch</li> <li>TensorFlow</li> </ul> <p>The library can be installed in:</p> <ul> <li>system environment</li> <li>virtualenv</li> <li>Docker</li> </ul> <p>The NVIDIA optimized Docker images for Python frameworks could be obtained from NVIDIA NGC Catalog.</p> <p>For using NVIDIA optimized Docker images we recommend to install NVIDIA Container Toolkit to run model inference on NVIDIA GPU.</p>"},{"location":"installation/#installation_1","title":"Installation","text":"<p>The package can be installed from <code>pypi.org</code> using extra index url:</p> <pre><code>pip install -U --extra-index-url https://pypi.ngc.nvidia.com triton-model-navigator[&lt;extras,&gt;]\n</code></pre> <p>or with nvidia-pyindex:</p> <pre><code>pip install nvidia-pyindex\npip install -U triton-model-navigator[&lt;extras,&gt;]\n</code></pre> <p>To install Triton Model Navigator from source use pip command:</p> <pre><code>$ pip install --extra-index-url https://pypi.ngc.nvidia.com .[&lt;extras,&gt;]\n</code></pre> <p>Extras:</p> <ul> <li><code>tensorflow</code> - Model Navigator with dependencies for TensorFlow2</li> <li><code>jax</code> - Model Navigator with dependencies for JAX</li> </ul> <p>For using with PyTorch no extras are needed.</p>"},{"location":"installation/#building-the-wheel","title":"Building the wheel","text":"<p>The Triton Model Navigator can be built as a wheel. For that purpose the <code>Makefile</code> provides necessary commands.</p> <p>The first is required to install necessary packages to perform build. <pre><code>make install-dev\n</code></pre></p> <p>Once the environment contain required packages run: <pre><code>make dist\n</code></pre></p> <p>The wheel is going to be generated in the <code>dist</code> catalog.</p>"},{"location":"known_issues/","title":"Known Issues","text":""},{"location":"known_issues/#known-issues-and-limitations","title":"Known Issues and Limitations","text":"<ul> <li>Source model running in Python can cause OOM issue when GPU memory is larger than CPU RAM memory</li> <li>Verify command could potentially experience CUDA OOM errors while trying to run inference on two models at the same time.</li> </ul>"},{"location":"quick_start/","title":"Quick start","text":""},{"location":"quick_start/#quick-start","title":"Quick Start","text":"<p>These sections provide an overview of optimizing the model, deploying model for serving inference on PyTriton or Triton Inference Server as well as using the Navigator Package. In each section you will find links to learn more about Model Navigator features.</p>"},{"location":"quick_start/#optimize-model","title":"Optimize Model","text":"<p>Optimizing models using Model Navigator is as simple as calling <code>optimize</code> function. The optimization process requires at least:</p> <ul> <li><code>model</code> - a Python object, callable or file path with model to optimize.</li> <li><code>dataloader</code> - a method or class generating input data. The data is utilized to determine the maximum and minimum   shapes   of the model inputs and create output samples that are used during the optimization process.</li> </ul> <p>Here is an example of running <code>optimize</code> on Torch Hub ResNet50 model:</p> <pre><code>import torch\nimport model_navigator as nav\npackage = nav.torch.optimize(\nmodel=torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True).eval(),\ndataloader=[torch.randn(1, 3, 256, 256) for _ in range(10)],\n)\n</code></pre> <p>Once the model has been optimized the created artifacts are stored in <code>navigator_workspace</code> and a Package object is returned from the function. Read more about optimize in documentation</p>"},{"location":"quick_start/#deploy-model-in-pytriton","title":"Deploy model in PyTriton","text":"<p>The PyTriton can be used to serve inference of any optimized format. Model Navigator provide a dedicated <code>PyTritonAdapter</code> to retrieve the <code>runner</code> and other information required to bind a model for serving inference. The <code>runner</code> is an abstraction that connects the model checkpoint with its runtime, making the inference process more accessible and straightforward.</p> <p>Following that, you can initialize the PyTriton server using the adapter information:</p> <pre><code>pytriton_adapter = nav.pytriton.PyTritonAdapter(package=package, strategy=nav.MaxThroughputStrategy())\nrunner = pytriton_adapter.runner\nrunner.activate()\n@batch\ndef infer_func(**inputs):\nreturn runner.infer(inputs)\nwith Triton() as triton:\ntriton.bind(\nmodel_name=\"resnet50\",\ninfer_func=infer_func,\ninputs=pytriton_adapter.inputs,\noutputs=pytriton_adapter.outputs,\nconfig=pytriton_adapter.config,\n)\ntriton.serve()\n</code></pre> <p>Read more about deploying model on PyTriton in documentation</p>"},{"location":"quick_start/#deploy-model-in-triton-inference-server","title":"Deploy model in Triton Inference Server","text":"<p>The optimized model can be also used for serving inference on Triton Inference Server when the serialized format has been created. Model Navigator provide functionality to generate a model deployment configuration directly inside Triton <code>model_repository</code>. The following command will select the model format with the highest throughput and create the Triton deployment in defined path to model repository:</p> <pre><code>nav.triton.model_repository.add_model_from_package(\nmodel_repository_path=pathlib.Path(\"model_repository\"),\nmodel_name=\"resnet50\",\npackage=package,\nstrategy=nav.MaxThroughputStrategy(),\n)\n</code></pre> <p>Once the entry is created, you can simply start Triton Inference Server mounting the defined <code>model_repository_path</code>.</p> <p>Read more about deploying model on Triton Inference Server in documentation</p>"},{"location":"quick_start/#using-navigator-package","title":"Using Navigator Package","text":"<p>The <code>Navigator Package</code> is an artifact that can be produced at the end of the optimization process. The package is a simple Zip file which contains the optimization details, model metadata and serialized formats and can be saved using:</p> <pre><code>nav.package.save(\npackage=package,\npath=\"/path/to/package.nav\"\n)\n</code></pre> <p>The package can be easily loaded on other machines and used to re-run the optimization process or profile the model. Read more about using package in documentation.</p>"},{"location":"support_matrix/","title":"Support matrix","text":""},{"location":"support_matrix/#support-matrix","title":"Support Matrix","text":"<p>Please find below information about tested models, used environment and libraries.</p>"},{"location":"support_matrix/#verified-models","title":"Verified Models","text":"<p>We have verified that the NVIDIA Model Navigator Optimize API works correctly for the following models.</p> Source Model NVIDIA DeepLearningExamples ResNet50 PyT NVIDIA DeepLearningExamples EfficientNet PyT NVIDIA DeepLearningExamples EfficientNet TF2 NVIDIA DeepLearningExamples BERT TF2 HuggingFace GPT2 Jax HuggingFace GPT2 PyT HuggingFace GPT2 TF2 HuggingFace DistilBERT PyT HuggingFace DistilGPT2 TF2"},{"location":"support_matrix/#third-party-packages","title":"Third-Party Packages","text":"<p>A set of component versions are imposed by the used NGC container. During testing we have used <code>23.03</code> container version that contains:</p> <ul> <li>PyTorch 2.0.0a0+1767026</li> <li>TensorFlow 2.11.0</li> <li>TensorRT 8.5.3.1</li> <li>ONNX Runtime 1.13.1</li> <li>Polygraphy: 0.44.2</li> <li>GraphSurgeon: 0.3.26</li> <li> <p>tf2onnx v1.14.0</p> </li> <li> <p>Refer to the containers support matrix for a detailed summary for each version.</p> </li> </ul>"},{"location":"optimize/config/","title":"Config","text":"<p>Classes, enums and types used to configure Model Navigator.</p>"},{"location":"optimize/config/#model_navigator.api.config","title":"model_navigator.api.config","text":"<p>Definition of enums and classes representing configuration for Model Navigator.</p>"},{"location":"optimize/config/#model_navigator.api.config.CustomConfig","title":"CustomConfig","text":"<p>             Bases: <code>abc.ABC</code></p> <p>Base class used for custom configs. Input for Model Navigator <code>optimize</code> method.</p>"},{"location":"optimize/config/#model_navigator.api.config.CustomConfig.defaults","title":"defaults","text":"<pre><code>defaults()\n</code></pre> <p>Update parameters to defaults.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def defaults(self) -&gt; None:\n\"\"\"Update parameters to defaults.\"\"\"\nreturn None\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.CustomConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(config_dict)\n</code></pre> <p>Instantiate CustomConfig from a dictionary.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: Dict[str, Any]) -&gt; \"CustomConfig\":\n\"\"\"Instantiate CustomConfig from a dictionary.\"\"\"\nreturn cls(**config_dict)\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.CustomConfig.name","title":"name  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>name()\n</code></pre> <p>Name of the CustomConfig.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\n@abc.abstractmethod\ndef name(cls) -&gt; str:\n\"\"\"Name of the CustomConfig.\"\"\"\nraise NotImplementedError()\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.CustomConfigForFormat","title":"CustomConfigForFormat","text":"<p>             Bases: <code>DataObject</code>, <code>CustomConfig</code></p> <p>Abstract base class used for custom configs representing particular format.</p>"},{"location":"optimize/config/#model_navigator.api.config.CustomConfigForFormat.format","title":"format  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>format: Format\n</code></pre> <p>Format represented by CustomConfig.</p>"},{"location":"optimize/config/#model_navigator.api.config.CustomConfigForFormat.defaults","title":"defaults","text":"<pre><code>defaults()\n</code></pre> <p>Update parameters to defaults.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def defaults(self) -&gt; None:\n\"\"\"Update parameters to defaults.\"\"\"\nreturn None\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.CustomConfigForFormat.filter_data","title":"filter_data  <code>staticmethod</code>","text":"<pre><code>filter_data(data, filter_fields)\n</code></pre> <p>Filter fields in dictionary.</p> <p>Parameters:</p> <ul> <li> data             (<code>Dict</code>)         \u2013          <p>Dictionary with data to filter</p> </li> <li> filter_fields             (<code>List[str]</code>)         \u2013          <p>Fields to filter</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>Filtered dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef filter_data(data: Dict, filter_fields: List[str]):\n\"\"\"Filter fields in dictionary.\n    Args:\n        data: Dictionary with data to filter\n        filter_fields: Fields to filter\n    Returns:\n        Filtered dictionary\n    \"\"\"\nfiltered_data = {key: value for key, value in data.items() if key not in filter_fields}\nreturn filtered_data\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.CustomConfigForFormat.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(config_dict)\n</code></pre> <p>Instantiate CustomConfig from a dictionary.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: Dict[str, Any]) -&gt; \"CustomConfig\":\n\"\"\"Instantiate CustomConfig from a dictionary.\"\"\"\nreturn cls(**config_dict)\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.CustomConfigForFormat.name","title":"name  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>name()\n</code></pre> <p>Name of the CustomConfig.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\n@abc.abstractmethod\ndef name(cls) -&gt; str:\n\"\"\"Name of the CustomConfig.\"\"\"\nraise NotImplementedError()\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.CustomConfigForFormat.parse_data","title":"parse_data  <code>staticmethod</code>","text":"<pre><code>parse_data(data)\n</code></pre> <p>Parse values in provided data.</p> <p>Parameters:</p> <ul> <li> data             (<code>Dict</code>)         \u2013          <p>Dictionary with data to parse</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>Parsed dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_data(data: Dict):\n\"\"\"Parse values in provided data.\n    Args:\n        data: Dictionary with data to parse\n    Returns:\n        Parsed dictionary\n    \"\"\"\nparsed_data = {}\nfor key, value in data.items():\nparsed_data[key] = DataObject.parse_value(value)\nreturn parsed_data\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.CustomConfigForFormat.parse_value","title":"parse_value  <code>staticmethod</code>","text":"<pre><code>parse_value(value)\n</code></pre> <p>Parse value to jsonable format.</p> <p>Parameters:</p> <ul> <li> value             (<code>Any</code>)         \u2013          <p>Value to be parsed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[str, Dict, List]</code>         \u2013          <p>Union[str, Dict, List]: Jsonable value.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_value(value: Any) -&gt; Union[str, Dict, List]:\n\"\"\"Parse value to jsonable format.\n    Args:\n        value (Any): Value to be parsed.\n    Returns:\n        Union[str, Dict, List]: Jsonable value.\n    \"\"\"\nif isinstance(value, DataObject):\nvalue = value.to_dict(parse=True)\nelif hasattr(value, \"to_json\"):\nvalue = value.to_json()\nelif isinstance(value, (Mapping, Profile)):\nvalue = DataObject._from_dict(value)\nelif isinstance(value, list) or isinstance(value, tuple):\nvalue = DataObject._from_list(value)\nelif isinstance(value, Enum):\nvalue = value.value\nelif isinstance(value, pathlib.Path):\nvalue = str(value)\nelif isinstance(value, ShapeTuple):\nvalue = vars(value)\nreturn value\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.CustomConfigForFormat.to_dict","title":"to_dict","text":"<pre><code>to_dict(filter_fields=None, parse=False)\n</code></pre> <p>Serialize to a dictionary.</p> <p>Parameters:</p> <ul> <li> filter_fields             (<code>Optional[List[str]]</code>)         \u2013          <p>List of fields to filter out. Defaults to None.</p> </li> <li> parse             (<code>bool</code>)         \u2013          <p>If True recursively parse field values to jsonable representation. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> Dict(            <code>Dict</code> )        \u2013          <p>Data serialized to a dictionary.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>def to_dict(self, filter_fields: Optional[List[str]] = None, parse: bool = False) -&gt; Dict:\n\"\"\"Serialize to a dictionary.\n    Args:\n        filter_fields (Optional[List[str]], optional): List of fields to filter out.\n            Defaults to None.\n        parse (bool, optional): If True recursively parse field values to jsonable representation.\n            Defaults to False.\n    Returns:\n        Dict: Data serialized to a dictionary.\n    \"\"\"\nif filter_fields:\nfiltered_data = DataObject.filter_data(\ndata=self.__dict__,\nfilter_fields=filter_fields,\n)\nelse:\nfiltered_data = self.__dict__\nif parse:\ndata = DataObject.parse_data(filtered_data)\nelse:\ndata = filtered_data\nreturn data\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.DeviceKind","title":"DeviceKind","text":"<p>             Bases: <code>Enum</code></p> <p>Supported types of devices.</p> <p>Parameters:</p> <ul> <li> CPU             (<code>str</code>)         \u2013          <p>Select CPU device.</p> </li> <li> GPU             (<code>str</code>)         \u2013          <p>Select GPU with CUDA support.</p> </li> </ul>"},{"location":"optimize/config/#model_navigator.api.config.Format","title":"Format","text":"<p>             Bases: <code>Enum</code></p> <p>All model formats supported by Model Navigator 'optimize' function.</p> <p>Parameters:</p> <ul> <li> PYTHON             (<code>str</code>)         \u2013          <p>Format indicating any model defined in Python.</p> </li> <li> TORCH             (<code>str</code>)         \u2013          <p>Format indicating PyTorch model.</p> </li> <li> TENSORFLOW             (<code>str</code>)         \u2013          <p>Format indicating TensorFlow model.</p> </li> <li> JAX             (<code>str</code>)         \u2013          <p>Format indicating JAX model.</p> </li> <li> TORCHSCRIPT             (<code>str</code>)         \u2013          <p>Format indicating TorchScript model.</p> </li> <li> TF_SAVEDMODEL             (<code>str</code>)         \u2013          <p>Format indicating TensorFlow SavedModel.</p> </li> <li> TF_TRT             (<code>str</code>)         \u2013          <p>Format indicating TensorFlow TensorRT model.</p> </li> <li> TORCH_TRT             (<code>str</code>)         \u2013          <p>Format indicating PyTorch TensorRT model.</p> </li> <li> ONNX             (<code>str</code>)         \u2013          <p>Format indicating ONNX model.</p> </li> <li> TENSORRT             (<code>str</code>)         \u2013          <p>Format indicating TensorRT model.</p> </li> </ul>"},{"location":"optimize/config/#model_navigator.api.config.JitType","title":"JitType","text":"<p>             Bases: <code>Enum</code></p> <p>TorchScript export parameter.</p> <p>Used for selecting the type of TorchScript export.</p> <p>Parameters:</p> <ul> <li> TRACE             (<code>str</code>)         \u2013          <p>Use tracing during export.</p> </li> <li> SCRIPT             (<code>str</code>)         \u2013          <p>Use scripting during export.</p> </li> </ul>"},{"location":"optimize/config/#model_navigator.api.config.OnnxConfig","title":"OnnxConfig  <code>dataclass</code>","text":"<p>             Bases: <code>CustomConfigForFormat</code></p> <p>ONNX custom config used for ONNX export and conversion.</p> <p>Parameters:</p> <ul> <li> opset             (<code>Optional[int]</code>)         \u2013          <p>ONNX opset used for conversion.</p> </li> <li> dynamic_axes             (<code>Optional[Dict[str, Union[Dict[int, str], List[int]]]]</code>)         \u2013          <p>Dynamic axes for ONNX conversion.</p> </li> <li> onnx_extended_conversion             (<code>bool</code>)         \u2013          <p>Enables additional conversions from TorchScript to ONNX.</p> </li> </ul>"},{"location":"optimize/config/#model_navigator.api.config.OnnxConfig.format","title":"format  <code>property</code>","text":"<pre><code>format: Format\n</code></pre> <p>Returns Format.ONNX.</p> <p>Returns:</p> <ul> <li> <code>Format</code>         \u2013          <p>Format.ONNX</p> </li> </ul>"},{"location":"optimize/config/#model_navigator.api.config.OnnxConfig.defaults","title":"defaults","text":"<pre><code>defaults()\n</code></pre> <p>Update parameters to defaults.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def defaults(self) -&gt; None:\n\"\"\"Update parameters to defaults.\"\"\"\nreturn None\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.OnnxConfig.filter_data","title":"filter_data  <code>staticmethod</code>","text":"<pre><code>filter_data(data, filter_fields)\n</code></pre> <p>Filter fields in dictionary.</p> <p>Parameters:</p> <ul> <li> data             (<code>Dict</code>)         \u2013          <p>Dictionary with data to filter</p> </li> <li> filter_fields             (<code>List[str]</code>)         \u2013          <p>Fields to filter</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>Filtered dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef filter_data(data: Dict, filter_fields: List[str]):\n\"\"\"Filter fields in dictionary.\n    Args:\n        data: Dictionary with data to filter\n        filter_fields: Fields to filter\n    Returns:\n        Filtered dictionary\n    \"\"\"\nfiltered_data = {key: value for key, value in data.items() if key not in filter_fields}\nreturn filtered_data\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.OnnxConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(config_dict)\n</code></pre> <p>Instantiate CustomConfig from a dictionary.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: Dict[str, Any]) -&gt; \"CustomConfig\":\n\"\"\"Instantiate CustomConfig from a dictionary.\"\"\"\nreturn cls(**config_dict)\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.OnnxConfig.name","title":"name  <code>classmethod</code>","text":"<pre><code>name()\n</code></pre> <p>Name of the config.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n\"\"\"Name of the config.\"\"\"\nreturn \"Onnx\"\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.OnnxConfig.parse_data","title":"parse_data  <code>staticmethod</code>","text":"<pre><code>parse_data(data)\n</code></pre> <p>Parse values in provided data.</p> <p>Parameters:</p> <ul> <li> data             (<code>Dict</code>)         \u2013          <p>Dictionary with data to parse</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>Parsed dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_data(data: Dict):\n\"\"\"Parse values in provided data.\n    Args:\n        data: Dictionary with data to parse\n    Returns:\n        Parsed dictionary\n    \"\"\"\nparsed_data = {}\nfor key, value in data.items():\nparsed_data[key] = DataObject.parse_value(value)\nreturn parsed_data\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.OnnxConfig.parse_value","title":"parse_value  <code>staticmethod</code>","text":"<pre><code>parse_value(value)\n</code></pre> <p>Parse value to jsonable format.</p> <p>Parameters:</p> <ul> <li> value             (<code>Any</code>)         \u2013          <p>Value to be parsed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[str, Dict, List]</code>         \u2013          <p>Union[str, Dict, List]: Jsonable value.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_value(value: Any) -&gt; Union[str, Dict, List]:\n\"\"\"Parse value to jsonable format.\n    Args:\n        value (Any): Value to be parsed.\n    Returns:\n        Union[str, Dict, List]: Jsonable value.\n    \"\"\"\nif isinstance(value, DataObject):\nvalue = value.to_dict(parse=True)\nelif hasattr(value, \"to_json\"):\nvalue = value.to_json()\nelif isinstance(value, (Mapping, Profile)):\nvalue = DataObject._from_dict(value)\nelif isinstance(value, list) or isinstance(value, tuple):\nvalue = DataObject._from_list(value)\nelif isinstance(value, Enum):\nvalue = value.value\nelif isinstance(value, pathlib.Path):\nvalue = str(value)\nelif isinstance(value, ShapeTuple):\nvalue = vars(value)\nreturn value\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.OnnxConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict(filter_fields=None, parse=False)\n</code></pre> <p>Serialize to a dictionary.</p> <p>Parameters:</p> <ul> <li> filter_fields             (<code>Optional[List[str]]</code>)         \u2013          <p>List of fields to filter out. Defaults to None.</p> </li> <li> parse             (<code>bool</code>)         \u2013          <p>If True recursively parse field values to jsonable representation. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> Dict(            <code>Dict</code> )        \u2013          <p>Data serialized to a dictionary.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>def to_dict(self, filter_fields: Optional[List[str]] = None, parse: bool = False) -&gt; Dict:\n\"\"\"Serialize to a dictionary.\n    Args:\n        filter_fields (Optional[List[str]], optional): List of fields to filter out.\n            Defaults to None.\n        parse (bool, optional): If True recursively parse field values to jsonable representation.\n            Defaults to False.\n    Returns:\n        Dict: Data serialized to a dictionary.\n    \"\"\"\nif filter_fields:\nfiltered_data = DataObject.filter_data(\ndata=self.__dict__,\nfilter_fields=filter_fields,\n)\nelse:\nfiltered_data = self.__dict__\nif parse:\ndata = DataObject.parse_data(filtered_data)\nelse:\ndata = filtered_data\nreturn data\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.OptimizationProfile","title":"OptimizationProfile  <code>dataclass</code>","text":"<p>             Bases: <code>DataObject</code></p> <p>Optimization profile configuration.</p> <p>For each batch size profiler will run measurements in windows of fixed number of queries. Batch sizes are profiled in the ascending order.</p> <p>Profiler will run multiple trials and will stop when the measurements are stable (within <code>stability_percentage</code> from the mean) within three consecutive windows. If the measurements are not stable after <code>max_trials</code> trials, the profiler will stop with an error. Profiler will also stop profiling when the throughput does not increase at least by <code>throughput_cutoff_threshold</code>.</p> <p>Parameters:</p> <ul> <li> max_batch_size             (<code>Optional[int]</code>)         \u2013          <p>Maximal batch size used during conversion and profiling. None mean automatic search is enabled.</p> </li> <li> batch_sizes         \u2013          <p>List of batch sizes to profile. None mean automatic search is enabled.</p> </li> <li> window_size             (<code>Optional[int]</code>)         \u2013          <p>Number of requests to measure in each window.</p> </li> <li> stability_percentage             (<code>float</code>)         \u2013          <p>Allowed percentage of variation from the mean in three consecutive windows.</p> </li> <li> max_trials             (<code>int</code>)         \u2013          <p>Maximum number of window trials.</p> </li> <li> throughput_cutoff_threshold             (<code>float</code>)         \u2013          <p>Minimum throughput increase to continue profiling.</p> </li> <li> dataloader             (<code>Optional[SizedDataLoader]</code>)         \u2013          <p>Optional dataloader for profiling. Use only 1 sample.</p> </li> </ul>"},{"location":"optimize/config/#model_navigator.api.config.OptimizationProfile.filter_data","title":"filter_data  <code>staticmethod</code>","text":"<pre><code>filter_data(data, filter_fields)\n</code></pre> <p>Filter fields in dictionary.</p> <p>Parameters:</p> <ul> <li> data             (<code>Dict</code>)         \u2013          <p>Dictionary with data to filter</p> </li> <li> filter_fields             (<code>List[str]</code>)         \u2013          <p>Fields to filter</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>Filtered dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef filter_data(data: Dict, filter_fields: List[str]):\n\"\"\"Filter fields in dictionary.\n    Args:\n        data: Dictionary with data to filter\n        filter_fields: Fields to filter\n    Returns:\n        Filtered dictionary\n    \"\"\"\nfiltered_data = {key: value for key, value in data.items() if key not in filter_fields}\nreturn filtered_data\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.OptimizationProfile.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(optimization_profile_dict)\n</code></pre> <p>Instantiate OptimizationProfile class from a dictionary.</p> <p>Parameters:</p> <ul> <li> optimization_profile_dict             (<code>Mapping</code>)         \u2013          <p>Data dictionary.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>OptimizationProfile</code>         \u2013          <p>OptimizationProfile</p> </li> </ul> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\ndef from_dict(cls, optimization_profile_dict: Mapping) -&gt; \"OptimizationProfile\":\n\"\"\"Instantiate OptimizationProfile class from a dictionary.\n    Args:\n        optimization_profile_dict (Mapping): Data dictionary.\n    Returns:\n        OptimizationProfile\n    \"\"\"\nreturn cls(\nmax_batch_size=optimization_profile_dict.get(\"max_batch_size\"),\nbatch_sizes=optimization_profile_dict.get(\"batch_sizes\"),\nwindow_size=optimization_profile_dict.get(\"window_size\"),\nstability_percentage=optimization_profile_dict.get(\"stability_percentage\", 10.0),\nmax_trials=optimization_profile_dict.get(\"max_trials\", 10),\nthroughput_cutoff_threshold=optimization_profile_dict.get(\"throughput_cutoff_threshold\", -2),\n)\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.OptimizationProfile.parse_data","title":"parse_data  <code>staticmethod</code>","text":"<pre><code>parse_data(data)\n</code></pre> <p>Parse values in provided data.</p> <p>Parameters:</p> <ul> <li> data             (<code>Dict</code>)         \u2013          <p>Dictionary with data to parse</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>Parsed dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_data(data: Dict):\n\"\"\"Parse values in provided data.\n    Args:\n        data: Dictionary with data to parse\n    Returns:\n        Parsed dictionary\n    \"\"\"\nparsed_data = {}\nfor key, value in data.items():\nparsed_data[key] = DataObject.parse_value(value)\nreturn parsed_data\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.OptimizationProfile.parse_value","title":"parse_value  <code>staticmethod</code>","text":"<pre><code>parse_value(value)\n</code></pre> <p>Parse value to jsonable format.</p> <p>Parameters:</p> <ul> <li> value             (<code>Any</code>)         \u2013          <p>Value to be parsed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[str, Dict, List]</code>         \u2013          <p>Union[str, Dict, List]: Jsonable value.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_value(value: Any) -&gt; Union[str, Dict, List]:\n\"\"\"Parse value to jsonable format.\n    Args:\n        value (Any): Value to be parsed.\n    Returns:\n        Union[str, Dict, List]: Jsonable value.\n    \"\"\"\nif isinstance(value, DataObject):\nvalue = value.to_dict(parse=True)\nelif hasattr(value, \"to_json\"):\nvalue = value.to_json()\nelif isinstance(value, (Mapping, Profile)):\nvalue = DataObject._from_dict(value)\nelif isinstance(value, list) or isinstance(value, tuple):\nvalue = DataObject._from_list(value)\nelif isinstance(value, Enum):\nvalue = value.value\nelif isinstance(value, pathlib.Path):\nvalue = str(value)\nelif isinstance(value, ShapeTuple):\nvalue = vars(value)\nreturn value\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.OptimizationProfile.to_dict","title":"to_dict","text":"<pre><code>to_dict(filter_fields=None, parse=False)\n</code></pre> <p>Serialize to a dictionary.</p> <p>Append <code>dataloader</code> field to filtered fields during dump.</p> <p>Parameters:</p> <ul> <li> filter_fields             (<code>Optional[List[str]]</code>)         \u2013          <p>List of fields to filter out. Defaults to None.</p> </li> <li> parse             (<code>bool</code>)         \u2013          <p>If True recursively parse field values to jsonable representation. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> Dict(            <code>Dict</code> )        \u2013          <p>Data serialized to a dictionary.</p> </li> </ul> Source code in <code>model_navigator/api/config.py</code> <pre><code>def to_dict(self, filter_fields: Optional[List[str]] = None, parse: bool = False) -&gt; Dict:\n\"\"\"Serialize to a dictionary.\n    Append `dataloader` field to filtered fields during dump.\n    Args:\n        filter_fields (Optional[List[str]], optional): List of fields to filter out.\n            Defaults to None.\n        parse (bool, optional): If True recursively parse field values to jsonable representation.\n            Defaults to False.\n    Returns:\n        Dict: Data serialized to a dictionary.\n    \"\"\"\nif not filter_fields:\nfilter_fields = []\nfilter_fields += [\"dataloader\"]\nreturn super().to_dict(filter_fields=filter_fields, parse=parse)\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.ShapeTuple","title":"ShapeTuple  <code>dataclass</code>","text":"<p>             Bases: <code>DataObject</code></p> <p>Represents a set of shapes for a single binding in a profile.</p> <p>Each element of the tuple represents a shape for a single dimension of the binding.</p> <p>Parameters:</p> <ul> <li> min             (<code>Tuple[int]</code>)         \u2013          <p>The minimum shape that the profile will support.</p> </li> <li> opt             (<code>Tuple[int]</code>)         \u2013          <p>The shape for which TensorRT will optimize the engine.</p> </li> <li> max             (<code>Tuple[int]</code>)         \u2013          <p>The maximum shape that the profile will support.</p> </li> </ul>"},{"location":"optimize/config/#model_navigator.api.config.ShapeTuple.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> <p>Iterate over shapes.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def __iter__(self):\n\"\"\"Iterate over shapes.\"\"\"\nyield from [self.min, self.opt, self.max]\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.ShapeTuple.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> <p>Representation.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def __repr__(self):\n\"\"\"Representation.\"\"\"\nreturn type(self).__name__ + self.__str__()\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.ShapeTuple.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> <p>String representation.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def __str__(self):\n\"\"\"String representation.\"\"\"\nreturn f\"(min={self.min}, opt={self.opt}, max={self.max})\"\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.ShapeTuple.filter_data","title":"filter_data  <code>staticmethod</code>","text":"<pre><code>filter_data(data, filter_fields)\n</code></pre> <p>Filter fields in dictionary.</p> <p>Parameters:</p> <ul> <li> data             (<code>Dict</code>)         \u2013          <p>Dictionary with data to filter</p> </li> <li> filter_fields             (<code>List[str]</code>)         \u2013          <p>Fields to filter</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>Filtered dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef filter_data(data: Dict, filter_fields: List[str]):\n\"\"\"Filter fields in dictionary.\n    Args:\n        data: Dictionary with data to filter\n        filter_fields: Fields to filter\n    Returns:\n        Filtered dictionary\n    \"\"\"\nfiltered_data = {key: value for key, value in data.items() if key not in filter_fields}\nreturn filtered_data\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.ShapeTuple.parse_data","title":"parse_data  <code>staticmethod</code>","text":"<pre><code>parse_data(data)\n</code></pre> <p>Parse values in provided data.</p> <p>Parameters:</p> <ul> <li> data             (<code>Dict</code>)         \u2013          <p>Dictionary with data to parse</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>Parsed dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_data(data: Dict):\n\"\"\"Parse values in provided data.\n    Args:\n        data: Dictionary with data to parse\n    Returns:\n        Parsed dictionary\n    \"\"\"\nparsed_data = {}\nfor key, value in data.items():\nparsed_data[key] = DataObject.parse_value(value)\nreturn parsed_data\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.ShapeTuple.parse_value","title":"parse_value  <code>staticmethod</code>","text":"<pre><code>parse_value(value)\n</code></pre> <p>Parse value to jsonable format.</p> <p>Parameters:</p> <ul> <li> value             (<code>Any</code>)         \u2013          <p>Value to be parsed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[str, Dict, List]</code>         \u2013          <p>Union[str, Dict, List]: Jsonable value.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_value(value: Any) -&gt; Union[str, Dict, List]:\n\"\"\"Parse value to jsonable format.\n    Args:\n        value (Any): Value to be parsed.\n    Returns:\n        Union[str, Dict, List]: Jsonable value.\n    \"\"\"\nif isinstance(value, DataObject):\nvalue = value.to_dict(parse=True)\nelif hasattr(value, \"to_json\"):\nvalue = value.to_json()\nelif isinstance(value, (Mapping, Profile)):\nvalue = DataObject._from_dict(value)\nelif isinstance(value, list) or isinstance(value, tuple):\nvalue = DataObject._from_list(value)\nelif isinstance(value, Enum):\nvalue = value.value\nelif isinstance(value, pathlib.Path):\nvalue = str(value)\nelif isinstance(value, ShapeTuple):\nvalue = vars(value)\nreturn value\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.ShapeTuple.to_dict","title":"to_dict","text":"<pre><code>to_dict(filter_fields=None, parse=False)\n</code></pre> <p>Serialize to a dictionary.</p> <p>Parameters:</p> <ul> <li> filter_fields             (<code>Optional[List[str]]</code>)         \u2013          <p>List of fields to filter out. Defaults to None.</p> </li> <li> parse             (<code>bool</code>)         \u2013          <p>If True recursively parse field values to jsonable representation. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> Dict(            <code>Dict</code> )        \u2013          <p>Data serialized to a dictionary.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>def to_dict(self, filter_fields: Optional[List[str]] = None, parse: bool = False) -&gt; Dict:\n\"\"\"Serialize to a dictionary.\n    Args:\n        filter_fields (Optional[List[str]], optional): List of fields to filter out.\n            Defaults to None.\n        parse (bool, optional): If True recursively parse field values to jsonable representation.\n            Defaults to False.\n    Returns:\n        Dict: Data serialized to a dictionary.\n    \"\"\"\nif filter_fields:\nfiltered_data = DataObject.filter_data(\ndata=self.__dict__,\nfilter_fields=filter_fields,\n)\nelse:\nfiltered_data = self.__dict__\nif parse:\ndata = DataObject.parse_data(filtered_data)\nelse:\ndata = filtered_data\nreturn data\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.SizedIterable","title":"SizedIterable","text":"<p>             Bases: <code>Protocol</code></p> <p>Protocol representing sized iterable. Used by dataloader.</p>"},{"location":"optimize/config/#model_navigator.api.config.SizedIterable.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> <p>Magic method iter.</p> <p>Returns:</p> <ul> <li> <code>Iterator</code>         \u2013          <p>Iterator to next item.</p> </li> </ul> Source code in <code>model_navigator/api/config.py</code> <pre><code>def __iter__(self) -&gt; Iterator:\n\"\"\"Magic method __iter__.\n    Returns:\n        Iterator to next item.\n    \"\"\"\n...\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.SizedIterable.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Magic method len.</p> <p>Returns:</p> <ul> <li> <code>int</code>         \u2013          <p>Length of size iterable.</p> </li> </ul> Source code in <code>model_navigator/api/config.py</code> <pre><code>def __len__(self) -&gt; int:\n\"\"\"Magic method __len__.\n    Returns:\n        Length of size iterable.\n    \"\"\"\n...\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.TensorFlowConfig","title":"TensorFlowConfig  <code>dataclass</code>","text":"<p>             Bases: <code>CustomConfigForFormat</code></p> <p>TensorFlow custom config used for SavedModel export.</p> <p>Parameters:</p> <ul> <li> jit_compile             (<code>Tuple[Optional[bool], ...]</code>)         \u2013          <p>Enable or Disable jit_compile flag for tf.function wrapper for Jax infer function.</p> </li> <li> enable_xla             (<code>Tuple[Optional[bool], ...]</code>)         \u2013          <p>Enable or Disable enable_xla flag for jax2tf converter.</p> </li> </ul>"},{"location":"optimize/config/#model_navigator.api.config.TensorFlowConfig.format","title":"format  <code>property</code>","text":"<pre><code>format: Format\n</code></pre> <p>Returns Format.TF_SAVEDMODEL.</p> <p>Returns:</p> <ul> <li> <code>Format</code>         \u2013          <p>Format.TF_SAVEDMODEL</p> </li> </ul>"},{"location":"optimize/config/#model_navigator.api.config.TensorFlowConfig.defaults","title":"defaults","text":"<pre><code>defaults()\n</code></pre> <p>Update parameters to defaults.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def defaults(self) -&gt; None:\n\"\"\"Update parameters to defaults.\"\"\"\nself.jit_compile = (None,)\nself.enable_xla = (None,)\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.TensorFlowConfig.filter_data","title":"filter_data  <code>staticmethod</code>","text":"<pre><code>filter_data(data, filter_fields)\n</code></pre> <p>Filter fields in dictionary.</p> <p>Parameters:</p> <ul> <li> data             (<code>Dict</code>)         \u2013          <p>Dictionary with data to filter</p> </li> <li> filter_fields             (<code>List[str]</code>)         \u2013          <p>Fields to filter</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>Filtered dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef filter_data(data: Dict, filter_fields: List[str]):\n\"\"\"Filter fields in dictionary.\n    Args:\n        data: Dictionary with data to filter\n        filter_fields: Fields to filter\n    Returns:\n        Filtered dictionary\n    \"\"\"\nfiltered_data = {key: value for key, value in data.items() if key not in filter_fields}\nreturn filtered_data\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.TensorFlowConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(config_dict)\n</code></pre> <p>Instantiate CustomConfig from a dictionary.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: Dict[str, Any]) -&gt; \"CustomConfig\":\n\"\"\"Instantiate CustomConfig from a dictionary.\"\"\"\nreturn cls(**config_dict)\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.TensorFlowConfig.name","title":"name  <code>classmethod</code>","text":"<pre><code>name()\n</code></pre> <p>Name of the config.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n\"\"\"Name of the config.\"\"\"\nreturn \"TensorFlow\"\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.TensorFlowConfig.parse_data","title":"parse_data  <code>staticmethod</code>","text":"<pre><code>parse_data(data)\n</code></pre> <p>Parse values in provided data.</p> <p>Parameters:</p> <ul> <li> data             (<code>Dict</code>)         \u2013          <p>Dictionary with data to parse</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>Parsed dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_data(data: Dict):\n\"\"\"Parse values in provided data.\n    Args:\n        data: Dictionary with data to parse\n    Returns:\n        Parsed dictionary\n    \"\"\"\nparsed_data = {}\nfor key, value in data.items():\nparsed_data[key] = DataObject.parse_value(value)\nreturn parsed_data\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.TensorFlowConfig.parse_value","title":"parse_value  <code>staticmethod</code>","text":"<pre><code>parse_value(value)\n</code></pre> <p>Parse value to jsonable format.</p> <p>Parameters:</p> <ul> <li> value             (<code>Any</code>)         \u2013          <p>Value to be parsed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[str, Dict, List]</code>         \u2013          <p>Union[str, Dict, List]: Jsonable value.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_value(value: Any) -&gt; Union[str, Dict, List]:\n\"\"\"Parse value to jsonable format.\n    Args:\n        value (Any): Value to be parsed.\n    Returns:\n        Union[str, Dict, List]: Jsonable value.\n    \"\"\"\nif isinstance(value, DataObject):\nvalue = value.to_dict(parse=True)\nelif hasattr(value, \"to_json\"):\nvalue = value.to_json()\nelif isinstance(value, (Mapping, Profile)):\nvalue = DataObject._from_dict(value)\nelif isinstance(value, list) or isinstance(value, tuple):\nvalue = DataObject._from_list(value)\nelif isinstance(value, Enum):\nvalue = value.value\nelif isinstance(value, pathlib.Path):\nvalue = str(value)\nelif isinstance(value, ShapeTuple):\nvalue = vars(value)\nreturn value\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.TensorFlowConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict(filter_fields=None, parse=False)\n</code></pre> <p>Serialize to a dictionary.</p> <p>Parameters:</p> <ul> <li> filter_fields             (<code>Optional[List[str]]</code>)         \u2013          <p>List of fields to filter out. Defaults to None.</p> </li> <li> parse             (<code>bool</code>)         \u2013          <p>If True recursively parse field values to jsonable representation. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> Dict(            <code>Dict</code> )        \u2013          <p>Data serialized to a dictionary.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>def to_dict(self, filter_fields: Optional[List[str]] = None, parse: bool = False) -&gt; Dict:\n\"\"\"Serialize to a dictionary.\n    Args:\n        filter_fields (Optional[List[str]], optional): List of fields to filter out.\n            Defaults to None.\n        parse (bool, optional): If True recursively parse field values to jsonable representation.\n            Defaults to False.\n    Returns:\n        Dict: Data serialized to a dictionary.\n    \"\"\"\nif filter_fields:\nfiltered_data = DataObject.filter_data(\ndata=self.__dict__,\nfilter_fields=filter_fields,\n)\nelse:\nfiltered_data = self.__dict__\nif parse:\ndata = DataObject.parse_data(filtered_data)\nelse:\ndata = filtered_data\nreturn data\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.TensorFlowTensorRTConfig","title":"TensorFlowTensorRTConfig  <code>dataclass</code>","text":"<p>             Bases: <code>CustomConfigForFormat</code></p> <p>TensorFlow TensorRT custom config used for TensorRT SavedModel export.</p> <p>Parameters:</p> <ul> <li> precision             (<code>Union[Union[str, TensorRTPrecision], Tuple[Union[str, TensorRTPrecision], ...]]</code>)         \u2013          <p>TensorRT precision.</p> </li> <li> max_workspace_size             (<code>Optional[int]</code>)         \u2013          <p>Max workspace size used by converter.</p> </li> <li> minimum_segment_size             (<code>int</code>)         \u2013          <p>Min size of subgraph.</p> </li> <li> trt_profile             (<code>Optional[TensorRTProfile]</code>)         \u2013          <p>TensorRT profile.</p> </li> </ul>"},{"location":"optimize/config/#model_navigator.api.config.TensorFlowTensorRTConfig.format","title":"format  <code>property</code>","text":"<pre><code>format: Format\n</code></pre> <p>Returns Format.TF_TRT.</p> <p>Returns:</p> <ul> <li> <code>Format</code>         \u2013          <p>Format.TF_TRT</p> </li> </ul>"},{"location":"optimize/config/#model_navigator.api.config.TensorFlowTensorRTConfig.defaults","title":"defaults","text":"<pre><code>defaults()\n</code></pre> <p>Update parameters to defaults.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def defaults(self) -&gt; None:\n\"\"\"Update parameters to defaults.\"\"\"\nself.precision = tuple(TensorRTPrecision(p) for p in DEFAULT_TENSORRT_PRECISION)\nself.max_workspace_size = DEFAULT_MAX_WORKSPACE_SIZE\nself.minimum_segment_size = DEFAULT_MIN_SEGMENT_SIZE\nself.trt_profile = None\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.TensorFlowTensorRTConfig.filter_data","title":"filter_data  <code>staticmethod</code>","text":"<pre><code>filter_data(data, filter_fields)\n</code></pre> <p>Filter fields in dictionary.</p> <p>Parameters:</p> <ul> <li> data             (<code>Dict</code>)         \u2013          <p>Dictionary with data to filter</p> </li> <li> filter_fields             (<code>List[str]</code>)         \u2013          <p>Fields to filter</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>Filtered dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef filter_data(data: Dict, filter_fields: List[str]):\n\"\"\"Filter fields in dictionary.\n    Args:\n        data: Dictionary with data to filter\n        filter_fields: Fields to filter\n    Returns:\n        Filtered dictionary\n    \"\"\"\nfiltered_data = {key: value for key, value in data.items() if key not in filter_fields}\nreturn filtered_data\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.TensorFlowTensorRTConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(config_dict)\n</code></pre> <p>Instantiate TensorFlowTensorRTConfig from  adictionary.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: Dict[str, Any]) -&gt; \"TensorFlowTensorRTConfig\":\n\"\"\"Instantiate TensorFlowTensorRTConfig from  adictionary.\"\"\"\nif config_dict.get(\"trt_profile\") is not None and not isinstance(config_dict[\"trt_profile\"], TensorRTProfile):\nconfig_dict[\"trt_profile\"] = TensorRTProfile.from_dict(config_dict[\"trt_profile\"])\nreturn cls(**config_dict)\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.TensorFlowTensorRTConfig.name","title":"name  <code>classmethod</code>","text":"<pre><code>name()\n</code></pre> <p>Name of the config.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n\"\"\"Name of the config.\"\"\"\nreturn \"TensorFlowTensorRT\"\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.TensorFlowTensorRTConfig.parse_data","title":"parse_data  <code>staticmethod</code>","text":"<pre><code>parse_data(data)\n</code></pre> <p>Parse values in provided data.</p> <p>Parameters:</p> <ul> <li> data             (<code>Dict</code>)         \u2013          <p>Dictionary with data to parse</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>Parsed dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_data(data: Dict):\n\"\"\"Parse values in provided data.\n    Args:\n        data: Dictionary with data to parse\n    Returns:\n        Parsed dictionary\n    \"\"\"\nparsed_data = {}\nfor key, value in data.items():\nparsed_data[key] = DataObject.parse_value(value)\nreturn parsed_data\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.TensorFlowTensorRTConfig.parse_value","title":"parse_value  <code>staticmethod</code>","text":"<pre><code>parse_value(value)\n</code></pre> <p>Parse value to jsonable format.</p> <p>Parameters:</p> <ul> <li> value             (<code>Any</code>)         \u2013          <p>Value to be parsed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[str, Dict, List]</code>         \u2013          <p>Union[str, Dict, List]: Jsonable value.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_value(value: Any) -&gt; Union[str, Dict, List]:\n\"\"\"Parse value to jsonable format.\n    Args:\n        value (Any): Value to be parsed.\n    Returns:\n        Union[str, Dict, List]: Jsonable value.\n    \"\"\"\nif isinstance(value, DataObject):\nvalue = value.to_dict(parse=True)\nelif hasattr(value, \"to_json\"):\nvalue = value.to_json()\nelif isinstance(value, (Mapping, Profile)):\nvalue = DataObject._from_dict(value)\nelif isinstance(value, list) or isinstance(value, tuple):\nvalue = DataObject._from_list(value)\nelif isinstance(value, Enum):\nvalue = value.value\nelif isinstance(value, pathlib.Path):\nvalue = str(value)\nelif isinstance(value, ShapeTuple):\nvalue = vars(value)\nreturn value\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.TensorFlowTensorRTConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict(filter_fields=None, parse=False)\n</code></pre> <p>Serialize to a dictionary.</p> <p>Parameters:</p> <ul> <li> filter_fields             (<code>Optional[List[str]]</code>)         \u2013          <p>List of fields to filter out. Defaults to None.</p> </li> <li> parse             (<code>bool</code>)         \u2013          <p>If True recursively parse field values to jsonable representation. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> Dict(            <code>Dict</code> )        \u2013          <p>Data serialized to a dictionary.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>def to_dict(self, filter_fields: Optional[List[str]] = None, parse: bool = False) -&gt; Dict:\n\"\"\"Serialize to a dictionary.\n    Args:\n        filter_fields (Optional[List[str]], optional): List of fields to filter out.\n            Defaults to None.\n        parse (bool, optional): If True recursively parse field values to jsonable representation.\n            Defaults to False.\n    Returns:\n        Dict: Data serialized to a dictionary.\n    \"\"\"\nif filter_fields:\nfiltered_data = DataObject.filter_data(\ndata=self.__dict__,\nfilter_fields=filter_fields,\n)\nelse:\nfiltered_data = self.__dict__\nif parse:\ndata = DataObject.parse_data(filtered_data)\nelse:\ndata = filtered_data\nreturn data\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.TensorRTCompatibilityLevel","title":"TensorRTCompatibilityLevel","text":"<p>             Bases: <code>Enum</code></p> <p>Compatibility level for TensorRT.</p> <p>Parameters:</p> <ul> <li> AMPERE_PLUS             (<code>str</code>)         \u2013          <p>Support AMPERE plus architecture</p> </li> </ul>"},{"location":"optimize/config/#model_navigator.api.config.TensorRTConfig","title":"TensorRTConfig  <code>dataclass</code>","text":"<p>             Bases: <code>CustomConfigForFormat</code></p> <p>TensorRT custom config used for TensorRT conversion.</p> <p>Parameters:</p> <ul> <li> precision             (<code>Union[Union[str, TensorRTPrecision], Tuple[Union[str, TensorRTPrecision], ...]]</code>)         \u2013          <p>TensorRT precision.</p> </li> <li> max_workspace_size             (<code>Optional[int]</code>)         \u2013          <p>Max workspace size used by converter.</p> </li> <li> trt_profile             (<code>Optional[TensorRTProfile]</code>)         \u2013          <p>TensorRT profile.</p> </li> <li> optimization_level             (<code>Optional[int]</code>)         \u2013          <p>Optimization level for TensorRT conversion. Allowed values are fom 0 to 5. Where default is                 3 based on TensorRT API documentation.</p> </li> </ul>"},{"location":"optimize/config/#model_navigator.api.config.TensorRTConfig.format","title":"format  <code>property</code>","text":"<pre><code>format: Format\n</code></pre> <p>Returns Format.TENSORRT.</p> <p>Returns:</p> <ul> <li> <code>Format</code>         \u2013          <p>Format.TENSORRT</p> </li> </ul>"},{"location":"optimize/config/#model_navigator.api.config.TensorRTConfig.defaults","title":"defaults","text":"<pre><code>defaults()\n</code></pre> <p>Update parameters to defaults.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def defaults(self) -&gt; None:\n\"\"\"Update parameters to defaults.\"\"\"\nself.precision = tuple(TensorRTPrecision(p) for p in DEFAULT_TENSORRT_PRECISION)\nself.precision_mode = DEFAULT_TENSORRT_PRECISION_MODE\nself.trt_profile = None\nself.max_workspace_size = DEFAULT_MAX_WORKSPACE_SIZE\nself.optimization_level = None\nself.compatibility_level = None\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.TensorRTConfig.filter_data","title":"filter_data  <code>staticmethod</code>","text":"<pre><code>filter_data(data, filter_fields)\n</code></pre> <p>Filter fields in dictionary.</p> <p>Parameters:</p> <ul> <li> data             (<code>Dict</code>)         \u2013          <p>Dictionary with data to filter</p> </li> <li> filter_fields             (<code>List[str]</code>)         \u2013          <p>Fields to filter</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>Filtered dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef filter_data(data: Dict, filter_fields: List[str]):\n\"\"\"Filter fields in dictionary.\n    Args:\n        data: Dictionary with data to filter\n        filter_fields: Fields to filter\n    Returns:\n        Filtered dictionary\n    \"\"\"\nfiltered_data = {key: value for key, value in data.items() if key not in filter_fields}\nreturn filtered_data\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.TensorRTConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(config_dict)\n</code></pre> <p>Instantiate TensorRTConfig from  adictionary.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: Dict[str, Any]) -&gt; \"TensorRTConfig\":\n\"\"\"Instantiate TensorRTConfig from  adictionary.\"\"\"\nif config_dict.get(\"trt_profile\") is not None and not isinstance(config_dict[\"trt_profile\"], TensorRTProfile):\nconfig_dict[\"trt_profile\"] = TensorRTProfile.from_dict(config_dict[\"trt_profile\"])\nreturn cls(**config_dict)\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.TensorRTConfig.name","title":"name  <code>classmethod</code>","text":"<pre><code>name()\n</code></pre> <p>Name of the config.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n\"\"\"Name of the config.\"\"\"\nreturn \"TensorRT\"\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.TensorRTConfig.parse_data","title":"parse_data  <code>staticmethod</code>","text":"<pre><code>parse_data(data)\n</code></pre> <p>Parse values in provided data.</p> <p>Parameters:</p> <ul> <li> data             (<code>Dict</code>)         \u2013          <p>Dictionary with data to parse</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>Parsed dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_data(data: Dict):\n\"\"\"Parse values in provided data.\n    Args:\n        data: Dictionary with data to parse\n    Returns:\n        Parsed dictionary\n    \"\"\"\nparsed_data = {}\nfor key, value in data.items():\nparsed_data[key] = DataObject.parse_value(value)\nreturn parsed_data\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.TensorRTConfig.parse_value","title":"parse_value  <code>staticmethod</code>","text":"<pre><code>parse_value(value)\n</code></pre> <p>Parse value to jsonable format.</p> <p>Parameters:</p> <ul> <li> value             (<code>Any</code>)         \u2013          <p>Value to be parsed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[str, Dict, List]</code>         \u2013          <p>Union[str, Dict, List]: Jsonable value.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_value(value: Any) -&gt; Union[str, Dict, List]:\n\"\"\"Parse value to jsonable format.\n    Args:\n        value (Any): Value to be parsed.\n    Returns:\n        Union[str, Dict, List]: Jsonable value.\n    \"\"\"\nif isinstance(value, DataObject):\nvalue = value.to_dict(parse=True)\nelif hasattr(value, \"to_json\"):\nvalue = value.to_json()\nelif isinstance(value, (Mapping, Profile)):\nvalue = DataObject._from_dict(value)\nelif isinstance(value, list) or isinstance(value, tuple):\nvalue = DataObject._from_list(value)\nelif isinstance(value, Enum):\nvalue = value.value\nelif isinstance(value, pathlib.Path):\nvalue = str(value)\nelif isinstance(value, ShapeTuple):\nvalue = vars(value)\nreturn value\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.TensorRTConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict(filter_fields=None, parse=False)\n</code></pre> <p>Serialize to a dictionary.</p> <p>Parameters:</p> <ul> <li> filter_fields             (<code>Optional[List[str]]</code>)         \u2013          <p>List of fields to filter out. Defaults to None.</p> </li> <li> parse             (<code>bool</code>)         \u2013          <p>If True recursively parse field values to jsonable representation. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> Dict(            <code>Dict</code> )        \u2013          <p>Data serialized to a dictionary.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>def to_dict(self, filter_fields: Optional[List[str]] = None, parse: bool = False) -&gt; Dict:\n\"\"\"Serialize to a dictionary.\n    Args:\n        filter_fields (Optional[List[str]], optional): List of fields to filter out.\n            Defaults to None.\n        parse (bool, optional): If True recursively parse field values to jsonable representation.\n            Defaults to False.\n    Returns:\n        Dict: Data serialized to a dictionary.\n    \"\"\"\nif filter_fields:\nfiltered_data = DataObject.filter_data(\ndata=self.__dict__,\nfilter_fields=filter_fields,\n)\nelse:\nfiltered_data = self.__dict__\nif parse:\ndata = DataObject.parse_data(filtered_data)\nelse:\ndata = filtered_data\nreturn data\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.TensorRTPrecision","title":"TensorRTPrecision","text":"<p>             Bases: <code>Enum</code></p> <p>Precisions supported during TensorRT conversions.</p> <p>Parameters:</p> <ul> <li> INT8             (<code>str</code>)         \u2013          <p>8-bit integer precision.</p> </li> <li> FP16             (<code>str</code>)         \u2013          <p>16-bit floating point precision.</p> </li> <li> FP32             (<code>str</code>)         \u2013          <p>32-bit floating point precision.</p> </li> </ul>"},{"location":"optimize/config/#model_navigator.api.config.TensorRTPrecisionMode","title":"TensorRTPrecisionMode","text":"<p>             Bases: <code>Enum</code></p> <p>Precision modes for TensorRT conversions.</p> <p>Parameters:</p> <ul> <li> HIERARCHY             (<code>str</code>)         \u2013          <p>Use TensorRT precision hierarchy starting from highest to lowest.</p> </li> <li> SINGLE             (<code>str</code>)         \u2013          <p>Use single precision.</p> </li> <li> MIXED             (<code>str</code>)         \u2013          <p>Use mixed precision.</p> </li> </ul>"},{"location":"optimize/config/#model_navigator.api.config.TensorRTProfile","title":"TensorRTProfile","text":"<p>             Bases: <code>Dict[str, ShapeTuple]</code></p> <p>Single optimization profile that can be used to build an engine.</p> <p>More specifically, it is an <code>Dict[str, ShapeTuple]</code> which maps binding names to a set of min/opt/max shapes.</p>"},{"location":"optimize/config/#model_navigator.api.config.TensorRTProfile.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key)\n</code></pre> <p>Retrieves the shapes registered for a given input name.</p> <p>Returns:</p> <ul> <li> ShapeTuple        \u2013          <pre><code>A named tuple including ``min``, ``opt``, and ``max`` members for the shapes\ncorresponding to the input.\n</code></pre> </li> </ul> Source code in <code>model_navigator/api/config.py</code> <pre><code>def __getitem__(self, key):\n\"\"\"Retrieves the shapes registered for a given input name.\n    Returns:\n        ShapeTuple:\n                A named tuple including ``min``, ``opt``, and ``max`` members for the shapes\n                corresponding to the input.\n    \"\"\"\nif key not in self:\nLOGGER.error(f\"Binding: {key} does not have shapes set in this profile\")\nreturn super().__getitem__(key)\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.TensorRTProfile.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> <p>Representation.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def __repr__(self):\n\"\"\"Representation.\"\"\"\nret = \"TensorRTProfile()\"\nfor name, (min, opt, max) in self.items():\nret += f\".add('{name}', min={min}, opt={opt}, max={max})\"\nreturn ret\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.TensorRTProfile.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> <p>String representation.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def __str__(self):\n\"\"\"String representation.\"\"\"\nelems = []\nfor name, (min, opt, max) in self.items():\nelems.append(f\"{name} [min={min}, opt={opt}, max={max}]\")\nsep = \",\\n \"\nreturn \"{\" + sep.join(elems) + \"}\"\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.TensorRTProfile.add","title":"add","text":"<pre><code>add(name, min, opt, max)\n</code></pre> <p>A convenience function to add shapes for a single binding.</p> <p>Parameters:</p> <ul> <li> name             (<code>str</code>)         \u2013          <p>The name of the binding.</p> </li> <li> min             (<code>Tuple[int]</code>)         \u2013          <p>The minimum shape that the profile will support.</p> </li> <li> opt             (<code>Tuple[int]</code>)         \u2013          <p>The shape for which TensorRT will optimize the engine.</p> </li> <li> max             (<code>Tuple[int]</code>)         \u2013          <p>The maximum shape that the profile will support.</p> </li> </ul> <p>Returns:</p> <ul> <li> Profile        \u2013          <p>self, which allows this function to be easily chained to add multiple bindings, e.g., TensorRTProfile().add(...).add(...)</p> </li> </ul> Source code in <code>model_navigator/api/config.py</code> <pre><code>def add(self, name, min, opt, max):\n\"\"\"A convenience function to add shapes for a single binding.\n    Args:\n        name (str): The name of the binding.\n        min (Tuple[int]): The minimum shape that the profile will support.\n        opt (Tuple[int]): The shape for which TensorRT will optimize the engine.\n        max (Tuple[int]): The maximum shape that the profile will support.\n    Returns:\n        Profile:\n            self, which allows this function to be easily chained to add multiple bindings,\n            e.g., TensorRTProfile().add(...).add(...)\n    \"\"\"\nself[name] = ShapeTuple(min, opt, max)\nreturn self\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.TensorRTProfile.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(profile_dict)\n</code></pre> <p>Create a TensorRTProfile from a dictionary.</p> <p>Parameters:</p> <ul> <li> profile_dict             (<code>Dict[str, Dict[str, Tuple[int, ...]]]</code>)         \u2013          <p>A dictionary mapping binding names to a dictionary containing <code>min</code>, <code>opt</code>, and <code>max</code> keys.</p> </li> </ul> <p>Returns:</p> <ul> <li> TensorRTProfile        \u2013          <p>A TensorRTProfile object.</p> </li> </ul> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\ndef from_dict(cls, profile_dict: Dict[str, Dict[str, Tuple[int, ...]]]):\n\"\"\"Create a TensorRTProfile from a dictionary.\n    Args:\n        profile_dict (Dict[str, Dict[str, Tuple[int, ...]]]):\n            A dictionary mapping binding names to a dictionary containing ``min``, ``opt``, and\n            ``max`` keys.\n    Returns:\n        TensorRTProfile:\n            A TensorRTProfile object.\n    \"\"\"\nreturn cls({name: ShapeTuple(**shapes) for name, shapes in profile_dict.items()})\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.TensorType","title":"TensorType","text":"<p>             Bases: <code>Enum</code></p> <p>All model formats supported by Model Navigator 'optimize' function.</p>"},{"location":"optimize/config/#model_navigator.api.config.TorchConfig","title":"TorchConfig  <code>dataclass</code>","text":"<p>             Bases: <code>CustomConfigForFormat</code></p> <p>Torch custom config used for TorchScript export.</p> <p>Parameters:</p> <ul> <li> jit_type             (<code>Union[Union[str, JitType], Tuple[Union[str, JitType], ...]]</code>)         \u2013          <p>Type of TorchScript export.</p> </li> <li> strict             (<code>bool</code>)         \u2013          <p>Enable or Disable strict flag for tracer used in TorchScript export, default: True.</p> </li> </ul>"},{"location":"optimize/config/#model_navigator.api.config.TorchConfig.format","title":"format  <code>property</code>","text":"<pre><code>format: Format\n</code></pre> <p>Returns Format.TORCHSCRIPT.</p> <p>Returns:</p> <ul> <li> <code>Format</code>         \u2013          <p>Format.TORCHSCRIPT</p> </li> </ul>"},{"location":"optimize/config/#model_navigator.api.config.TorchConfig.defaults","title":"defaults","text":"<pre><code>defaults()\n</code></pre> <p>Update parameters to defaults.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def defaults(self) -&gt; None:\n\"\"\"Update parameters to defaults.\"\"\"\nself.jit_type = (JitType.SCRIPT, JitType.TRACE)\nself.strict = True\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.TorchConfig.filter_data","title":"filter_data  <code>staticmethod</code>","text":"<pre><code>filter_data(data, filter_fields)\n</code></pre> <p>Filter fields in dictionary.</p> <p>Parameters:</p> <ul> <li> data             (<code>Dict</code>)         \u2013          <p>Dictionary with data to filter</p> </li> <li> filter_fields             (<code>List[str]</code>)         \u2013          <p>Fields to filter</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>Filtered dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef filter_data(data: Dict, filter_fields: List[str]):\n\"\"\"Filter fields in dictionary.\n    Args:\n        data: Dictionary with data to filter\n        filter_fields: Fields to filter\n    Returns:\n        Filtered dictionary\n    \"\"\"\nfiltered_data = {key: value for key, value in data.items() if key not in filter_fields}\nreturn filtered_data\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.TorchConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(config_dict)\n</code></pre> <p>Instantiate CustomConfig from a dictionary.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: Dict[str, Any]) -&gt; \"CustomConfig\":\n\"\"\"Instantiate CustomConfig from a dictionary.\"\"\"\nreturn cls(**config_dict)\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.TorchConfig.name","title":"name  <code>classmethod</code>","text":"<pre><code>name()\n</code></pre> <p>Name of the config.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n\"\"\"Name of the config.\"\"\"\nreturn \"Torch\"\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.TorchConfig.parse_data","title":"parse_data  <code>staticmethod</code>","text":"<pre><code>parse_data(data)\n</code></pre> <p>Parse values in provided data.</p> <p>Parameters:</p> <ul> <li> data             (<code>Dict</code>)         \u2013          <p>Dictionary with data to parse</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>Parsed dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_data(data: Dict):\n\"\"\"Parse values in provided data.\n    Args:\n        data: Dictionary with data to parse\n    Returns:\n        Parsed dictionary\n    \"\"\"\nparsed_data = {}\nfor key, value in data.items():\nparsed_data[key] = DataObject.parse_value(value)\nreturn parsed_data\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.TorchConfig.parse_value","title":"parse_value  <code>staticmethod</code>","text":"<pre><code>parse_value(value)\n</code></pre> <p>Parse value to jsonable format.</p> <p>Parameters:</p> <ul> <li> value             (<code>Any</code>)         \u2013          <p>Value to be parsed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[str, Dict, List]</code>         \u2013          <p>Union[str, Dict, List]: Jsonable value.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_value(value: Any) -&gt; Union[str, Dict, List]:\n\"\"\"Parse value to jsonable format.\n    Args:\n        value (Any): Value to be parsed.\n    Returns:\n        Union[str, Dict, List]: Jsonable value.\n    \"\"\"\nif isinstance(value, DataObject):\nvalue = value.to_dict(parse=True)\nelif hasattr(value, \"to_json\"):\nvalue = value.to_json()\nelif isinstance(value, (Mapping, Profile)):\nvalue = DataObject._from_dict(value)\nelif isinstance(value, list) or isinstance(value, tuple):\nvalue = DataObject._from_list(value)\nelif isinstance(value, Enum):\nvalue = value.value\nelif isinstance(value, pathlib.Path):\nvalue = str(value)\nelif isinstance(value, ShapeTuple):\nvalue = vars(value)\nreturn value\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.TorchConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict(filter_fields=None, parse=False)\n</code></pre> <p>Serialize to a dictionary.</p> <p>Parameters:</p> <ul> <li> filter_fields             (<code>Optional[List[str]]</code>)         \u2013          <p>List of fields to filter out. Defaults to None.</p> </li> <li> parse             (<code>bool</code>)         \u2013          <p>If True recursively parse field values to jsonable representation. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> Dict(            <code>Dict</code> )        \u2013          <p>Data serialized to a dictionary.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>def to_dict(self, filter_fields: Optional[List[str]] = None, parse: bool = False) -&gt; Dict:\n\"\"\"Serialize to a dictionary.\n    Args:\n        filter_fields (Optional[List[str]], optional): List of fields to filter out.\n            Defaults to None.\n        parse (bool, optional): If True recursively parse field values to jsonable representation.\n            Defaults to False.\n    Returns:\n        Dict: Data serialized to a dictionary.\n    \"\"\"\nif filter_fields:\nfiltered_data = DataObject.filter_data(\ndata=self.__dict__,\nfilter_fields=filter_fields,\n)\nelse:\nfiltered_data = self.__dict__\nif parse:\ndata = DataObject.parse_data(filtered_data)\nelse:\ndata = filtered_data\nreturn data\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.TorchTensorRTConfig","title":"TorchTensorRTConfig  <code>dataclass</code>","text":"<p>             Bases: <code>CustomConfigForFormat</code></p> <p>Torch custom config used for TensorRT TorchScript conversion.</p> <p>Parameters:</p> <ul> <li> precision             (<code>Union[Union[str, TensorRTPrecision], Tuple[Union[str, TensorRTPrecision], ...]]</code>)         \u2013          <p>TensorRT precision.</p> </li> <li> max_workspace_size             (<code>Optional[int]</code>)         \u2013          <p>Max workspace size used by converter.</p> </li> <li> trt_profile             (<code>Optional[TensorRTProfile]</code>)         \u2013          <p>TensorRT profile.</p> </li> </ul>"},{"location":"optimize/config/#model_navigator.api.config.TorchTensorRTConfig.format","title":"format  <code>property</code>","text":"<pre><code>format: Format\n</code></pre> <p>Returns Format.TORCH_TRT.</p> <p>Returns:</p> <ul> <li> <code>Format</code>         \u2013          <p>Format.TORCH_TRT</p> </li> </ul>"},{"location":"optimize/config/#model_navigator.api.config.TorchTensorRTConfig.defaults","title":"defaults","text":"<pre><code>defaults()\n</code></pre> <p>Update parameters to defaults.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>def defaults(self) -&gt; None:\n\"\"\"Update parameters to defaults.\"\"\"\nself.precision = tuple(TensorRTPrecision(p) for p in DEFAULT_TENSORRT_PRECISION)\nself.precision_mode = DEFAULT_TENSORRT_PRECISION_MODE\nself.trt_profile = None\nself.max_workspace_size = DEFAULT_MAX_WORKSPACE_SIZE\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.TorchTensorRTConfig.filter_data","title":"filter_data  <code>staticmethod</code>","text":"<pre><code>filter_data(data, filter_fields)\n</code></pre> <p>Filter fields in dictionary.</p> <p>Parameters:</p> <ul> <li> data             (<code>Dict</code>)         \u2013          <p>Dictionary with data to filter</p> </li> <li> filter_fields             (<code>List[str]</code>)         \u2013          <p>Fields to filter</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>Filtered dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef filter_data(data: Dict, filter_fields: List[str]):\n\"\"\"Filter fields in dictionary.\n    Args:\n        data: Dictionary with data to filter\n        filter_fields: Fields to filter\n    Returns:\n        Filtered dictionary\n    \"\"\"\nfiltered_data = {key: value for key, value in data.items() if key not in filter_fields}\nreturn filtered_data\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.TorchTensorRTConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(config_dict)\n</code></pre> <p>Instantiate TorchTensorRTConfig from  adictionary.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: Dict[str, Any]) -&gt; \"TorchTensorRTConfig\":\n\"\"\"Instantiate TorchTensorRTConfig from  adictionary.\"\"\"\nif config_dict.get(\"trt_profile\") is not None and not isinstance(config_dict[\"trt_profile\"], TensorRTProfile):\nconfig_dict[\"trt_profile\"] = TensorRTProfile.from_dict(config_dict[\"trt_profile\"])\nreturn cls(**config_dict)\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.TorchTensorRTConfig.name","title":"name  <code>classmethod</code>","text":"<pre><code>name()\n</code></pre> <p>Name of the config.</p> Source code in <code>model_navigator/api/config.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n\"\"\"Name of the config.\"\"\"\nreturn \"TorchTensorRT\"\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.TorchTensorRTConfig.parse_data","title":"parse_data  <code>staticmethod</code>","text":"<pre><code>parse_data(data)\n</code></pre> <p>Parse values in provided data.</p> <p>Parameters:</p> <ul> <li> data             (<code>Dict</code>)         \u2013          <p>Dictionary with data to parse</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>Parsed dictionary</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_data(data: Dict):\n\"\"\"Parse values in provided data.\n    Args:\n        data: Dictionary with data to parse\n    Returns:\n        Parsed dictionary\n    \"\"\"\nparsed_data = {}\nfor key, value in data.items():\nparsed_data[key] = DataObject.parse_value(value)\nreturn parsed_data\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.TorchTensorRTConfig.parse_value","title":"parse_value  <code>staticmethod</code>","text":"<pre><code>parse_value(value)\n</code></pre> <p>Parse value to jsonable format.</p> <p>Parameters:</p> <ul> <li> value             (<code>Any</code>)         \u2013          <p>Value to be parsed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[str, Dict, List]</code>         \u2013          <p>Union[str, Dict, List]: Jsonable value.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>@staticmethod\ndef parse_value(value: Any) -&gt; Union[str, Dict, List]:\n\"\"\"Parse value to jsonable format.\n    Args:\n        value (Any): Value to be parsed.\n    Returns:\n        Union[str, Dict, List]: Jsonable value.\n    \"\"\"\nif isinstance(value, DataObject):\nvalue = value.to_dict(parse=True)\nelif hasattr(value, \"to_json\"):\nvalue = value.to_json()\nelif isinstance(value, (Mapping, Profile)):\nvalue = DataObject._from_dict(value)\nelif isinstance(value, list) or isinstance(value, tuple):\nvalue = DataObject._from_list(value)\nelif isinstance(value, Enum):\nvalue = value.value\nelif isinstance(value, pathlib.Path):\nvalue = str(value)\nelif isinstance(value, ShapeTuple):\nvalue = vars(value)\nreturn value\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.TorchTensorRTConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict(filter_fields=None, parse=False)\n</code></pre> <p>Serialize to a dictionary.</p> <p>Parameters:</p> <ul> <li> filter_fields             (<code>Optional[List[str]]</code>)         \u2013          <p>List of fields to filter out. Defaults to None.</p> </li> <li> parse             (<code>bool</code>)         \u2013          <p>If True recursively parse field values to jsonable representation. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> Dict(            <code>Dict</code> )        \u2013          <p>Data serialized to a dictionary.</p> </li> </ul> Source code in <code>model_navigator/utils/common.py</code> <pre><code>def to_dict(self, filter_fields: Optional[List[str]] = None, parse: bool = False) -&gt; Dict:\n\"\"\"Serialize to a dictionary.\n    Args:\n        filter_fields (Optional[List[str]], optional): List of fields to filter out.\n            Defaults to None.\n        parse (bool, optional): If True recursively parse field values to jsonable representation.\n            Defaults to False.\n    Returns:\n        Dict: Data serialized to a dictionary.\n    \"\"\"\nif filter_fields:\nfiltered_data = DataObject.filter_data(\ndata=self.__dict__,\nfilter_fields=filter_fields,\n)\nelse:\nfiltered_data = self.__dict__\nif parse:\ndata = DataObject.parse_data(filtered_data)\nelse:\ndata = filtered_data\nreturn data\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.config.map_custom_configs","title":"map_custom_configs","text":"<pre><code>map_custom_configs(custom_configs)\n</code></pre> <p>Map custom configs from list to dictionary.</p> <p>Parameters:</p> <ul> <li> custom_configs             (<code>Optional[Sequence[CustomConfig]]</code>)         \u2013          <p>List of custom configs passed to API method</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict</code>         \u2013          <p>Mapped configs to dictionary</p> </li> </ul> Source code in <code>model_navigator/api/config.py</code> <pre><code>def map_custom_configs(custom_configs: Optional[Sequence[CustomConfig]]) -&gt; Dict:\n\"\"\"Map custom configs from list to dictionary.\n    Args:\n        custom_configs: List of custom configs passed to API method\n    Returns:\n        Mapped configs to dictionary\n    \"\"\"\nif not custom_configs:\nreturn {}\nreturn {config.name(): config for config in custom_configs}\n</code></pre>"},{"location":"optimize/config/#model_navigator.api.MaxThroughputAndMinLatencyStrategy","title":"model_navigator.api.MaxThroughputAndMinLatencyStrategy","text":"<p>             Bases: <code>RuntimeSearchStrategy</code></p> <p>Get runtime with the highest throughput and the lowest latency.</p>"},{"location":"optimize/config/#model_navigator.api.MaxThroughputStrategy","title":"model_navigator.api.MaxThroughputStrategy","text":"<p>             Bases: <code>RuntimeSearchStrategy</code></p> <p>Get runtime with the highest throughput.</p>"},{"location":"optimize/config/#model_navigator.api.MinLatencyStrategy","title":"model_navigator.api.MinLatencyStrategy","text":"<p>             Bases: <code>RuntimeSearchStrategy</code></p> <p>Get runtime with the lowest latency.</p>"},{"location":"optimize/jax/","title":"JAX","text":""},{"location":"optimize/jax/#model_navigator.api.jax","title":"model_navigator.api.jax","text":"<p>JAX optimize API.</p>"},{"location":"optimize/jax/#model_navigator.api.jax.optimize","title":"optimize","text":"<pre><code>optimize(\nmodel,\nmodel_params,\ndataloader,\nsample_count=DEFAULT_SAMPLE_COUNT,\nbatching=True,\ntarget_formats=None,\ntarget_device=DeviceKind.CUDA,\nrunners=None,\noptimization_profile=None,\nworkspace=None,\nverbose=False,\ndebug=False,\nverify_func=None,\ncustom_configs=None,\n)\n</code></pre> <p>Entry point for JAX optimize.</p> <p>Perform export, conversion, correctness testing, profiling and model verification.</p> <p>Parameters:</p> <ul> <li> model             (<code>Callable</code>)         \u2013          <p>JAX forward function</p> </li> <li> model_params             (<code>Any</code>)         \u2013          <p>JAX model parameters (weights)</p> </li> <li> dataloader             (<code>SizedDataLoader</code>)         \u2013          <p>Sized iterable with data that will be feed to the model</p> </li> <li> sample_count             (<code>int</code>)         \u2013          <p>Limits how many samples will be used from dataloader</p> </li> <li> batching             (<code>Optional[bool]</code>)         \u2013          <p>Enable or disable batching on first (index 0) dimension of the model</p> </li> <li> target_formats             (<code>Optional[Tuple[Union[str, Format], ...]]</code>)         \u2013          <p>Target model formats for optimize process</p> </li> <li> target_device             (<code>Optional[DeviceKind]</code>)         \u2013          <p>Target device for optimize process, default is CUDA</p> </li> <li> runners             (<code>Optional[Tuple[Union[str, Type[NavigatorRunner]], ...]]</code>)         \u2013          <p>Use only runners provided as parameter</p> </li> <li> optimization_profile             (<code>Optional[OptimizationProfile]</code>)         \u2013          <p>Optimization profile for conversion and profiling</p> </li> <li> workspace             (<code>Optional[pathlib.Path]</code>)         \u2013          <p>Workspace where packages will be extracted</p> </li> <li> verbose             (<code>bool</code>)         \u2013          <p>Enable verbose logging</p> </li> <li> debug             (<code>bool</code>)         \u2013          <p>Enable debug logging from commands</p> </li> <li> verify_func             (<code>Optional[VerifyFunction]</code>)         \u2013          <p>Function for additional model verification</p> </li> <li> custom_configs             (<code>Optional[Sequence[CustomConfig]]</code>)         \u2013          <p>Sequence of CustomConfigs used to control produced artifacts</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Package</code>         \u2013          <p>Package descriptor representing created package.</p> </li> </ul> Source code in <code>model_navigator/api/jax.py</code> <pre><code>def optimize(\nmodel: Callable,\nmodel_params: Any,\ndataloader: SizedDataLoader,\nsample_count: int = DEFAULT_SAMPLE_COUNT,\nbatching: Optional[bool] = True,\ntarget_formats: Optional[Tuple[Union[str, Format], ...]] = None,\ntarget_device: Optional[DeviceKind] = DeviceKind.CUDA,\nrunners: Optional[Tuple[Union[str, Type[NavigatorRunner]], ...]] = None,\noptimization_profile: Optional[OptimizationProfile] = None,\nworkspace: Optional[pathlib.Path] = None,\nverbose: bool = False,\ndebug: bool = False,\nverify_func: Optional[VerifyFunction] = None,\ncustom_configs: Optional[Sequence[CustomConfig]] = None,\n) -&gt; Package:\n\"\"\"Entry point for JAX optimize.\n    Perform export, conversion, correctness testing, profiling and model verification.\n    Args:\n        model: JAX forward function\n        model_params: JAX model parameters (weights)\n        dataloader: Sized iterable with data that will be feed to the model\n        sample_count: Limits how many samples will be used from dataloader\n        batching: Enable or disable batching on first (index 0) dimension of the model\n        target_formats: Target model formats for optimize process\n        target_device: Target device for optimize process, default is CUDA\n        runners: Use only runners provided as parameter\n        optimization_profile: Optimization profile for conversion and profiling\n        workspace: Workspace where packages will be extracted\n        verbose: Enable verbose logging\n        debug: Enable debug logging from commands\n        verify_func: Function for additional model verification\n        custom_configs: Sequence of CustomConfigs used to control produced artifacts\n    Returns:\n        Package descriptor representing created package.\n    \"\"\"\nif target_device == DeviceKind.CPU and any(\ndevice.device_type == \"GPU\" for device in tensorflow.config.get_visible_devices()\n):\nraise ModelNavigatorConfigurationError(\n\"\\n\"\n\"    'target_device == nav.DeviceKind.CPU' is not supported for TensorFlow2 \"\n\"(exported from JAX) when GPU is available.\\n\"\n\"    To optimize model for CPU, disable GPU with: \"\n\"'tf.config.set_visible_devices([], 'GPU')' directly after importing TensorFlow.\\n\"\n)\nif isinstance(model, str):\nmodel = pathlib.Path(model)\nif target_formats is None:\ntarget_formats = DEFAULT_JAX_TARGET_FORMATS\nsample = next(iter(dataloader))\nforward_kw_names = tuple(sample.keys()) if isinstance(sample, Mapping) else None\nif runners is None:\nrunners = default_runners(device_kind=target_device)\nif optimization_profile is None:\noptimization_profile = OptimizationProfile()\ntarget_formats_enums = enums.parse(target_formats, Format)\nrunner_names = enums.parse(runners, lambda runner: runner if isinstance(runner, str) else runner.name())\nif Format.JAX not in target_formats_enums:\ntarget_formats_enums = (Format.JAX,) + target_formats_enums\nconfig = CommonConfig(\nFramework.JAX,\nmodel=JaxModel(model=model, params=model_params),\ndataloader=dataloader,\nforward_kw_names=forward_kw_names,\ntarget_formats=target_formats_enums,\ntarget_device=target_device,\nsample_count=sample_count,\nbatch_dim=0 if batching else None,\nrunner_names=runner_names,\noptimization_profile=optimization_profile,\nverbose=verbose,\ndebug=debug,\nverify_func=verify_func,\ncustom_configs=map_custom_configs(custom_configs=custom_configs),\n)\nmodels_config = ModelConfigBuilder.generate_model_config(\nframework=Framework.JAX,\ntarget_formats=target_formats_enums,\ncustom_configs=custom_configs,\n)\nbuilders = [\npreprocessing_builder,\njax_export_builder,\nfind_device_max_batch_size_builder,\ntensorflow_conversion_builder,\ncorrectness_builder,\nperformance_builder,\nverify_builder,\n]\npackage = optimize_pipeline(\nmodel=model,\nworkspace=workspace,\nbuilders=builders,\nconfig=config,\nmodels_config=models_config,\n)\nreturn package\n</code></pre>"},{"location":"optimize/onnx/","title":"ONNX","text":""},{"location":"optimize/onnx/#model_navigator.api.onnx","title":"model_navigator.api.onnx","text":"<p>ONNX optimize API.</p>"},{"location":"optimize/onnx/#model_navigator.api.onnx.optimize","title":"optimize","text":"<pre><code>optimize(\nmodel,\ndataloader,\nsample_count=DEFAULT_SAMPLE_COUNT,\nbatching=True,\ntarget_formats=None,\ntarget_device=DeviceKind.CUDA,\nrunners=None,\noptimization_profile=None,\nworkspace=None,\nverbose=False,\ndebug=False,\nverify_func=None,\ncustom_configs=None,\n)\n</code></pre> <p>Entrypoint for ONNX optimize.</p> <p>Perform conversion, correctness testing, profiling and model verification.</p> <p>Parameters:</p> <ul> <li> model             (<code>Union[pathlib.Path, str]</code>)         \u2013          <p>ONNX model path or string</p> </li> <li> dataloader             (<code>SizedDataLoader</code>)         \u2013          <p>Sized iterable with data that will be feed to the model</p> </li> <li> sample_count             (<code>int</code>)         \u2013          <p>Limits how many samples will be used from dataloader</p> </li> <li> batching             (<code>Optional[bool]</code>)         \u2013          <p>Enable or disable batching on first (index 0) dimension of the model</p> </li> <li> target_formats             (<code>Optional[Tuple[Union[str, Format], ...]]</code>)         \u2013          <p>Target model formats for optimize process</p> </li> <li> target_device             (<code>Optional[DeviceKind]</code>)         \u2013          <p>Target device for optimize process, default is CUDA</p> </li> <li> runners             (<code>Optional[Tuple[Union[str, Type[NavigatorRunner]], ...]]</code>)         \u2013          <p>Use only runners provided as parameter</p> </li> <li> optimization_profile             (<code>Optional[OptimizationProfile]</code>)         \u2013          <p>Optimization profile for conversion and profiling</p> </li> <li> workspace             (<code>Optional[pathlib.Path]</code>)         \u2013          <p>Workspace where packages will be extracted</p> </li> <li> verbose             (<code>bool</code>)         \u2013          <p>Enable verbose logging</p> </li> <li> debug             (<code>bool</code>)         \u2013          <p>Enable debug logging from commands</p> </li> <li> verify_func             (<code>Optional[VerifyFunction]</code>)         \u2013          <p>Function for additional model verification</p> </li> <li> custom_configs             (<code>Optional[Sequence[CustomConfig]]</code>)         \u2013          <p>Sequence of CustomConfigs used to control produced artifacts</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Package</code>         \u2013          <p>Package descriptor representing created package.</p> </li> </ul> Source code in <code>model_navigator/api/onnx.py</code> <pre><code>def optimize(\nmodel: Union[pathlib.Path, str],\ndataloader: SizedDataLoader,\nsample_count: int = DEFAULT_SAMPLE_COUNT,\nbatching: Optional[bool] = True,\ntarget_formats: Optional[Tuple[Union[str, Format], ...]] = None,\ntarget_device: Optional[DeviceKind] = DeviceKind.CUDA,\nrunners: Optional[Tuple[Union[str, Type[NavigatorRunner]], ...]] = None,\noptimization_profile: Optional[OptimizationProfile] = None,\nworkspace: Optional[pathlib.Path] = None,\nverbose: bool = False,\ndebug: bool = False,\nverify_func: Optional[VerifyFunction] = None,\ncustom_configs: Optional[Sequence[CustomConfig]] = None,\n) -&gt; Package:\n\"\"\"Entrypoint for ONNX optimize.\n    Perform conversion, correctness testing, profiling and model verification.\n    Args:\n        model: ONNX model path or string\n        dataloader: Sized iterable with data that will be feed to the model\n        sample_count: Limits how many samples will be used from dataloader\n        batching: Enable or disable batching on first (index 0) dimension of the model\n        target_formats: Target model formats for optimize process\n        target_device: Target device for optimize process, default is CUDA\n        runners: Use only runners provided as parameter\n        optimization_profile: Optimization profile for conversion and profiling\n        workspace: Workspace where packages will be extracted\n        verbose: Enable verbose logging\n        debug: Enable debug logging from commands\n        verify_func: Function for additional model verification\n        custom_configs: Sequence of CustomConfigs used to control produced artifacts\n    Returns:\n        Package descriptor representing created package.\n    \"\"\"\nif isinstance(model, str):\nmodel = pathlib.Path(model)\nif target_formats is None:\ntarget_formats = DEFAULT_ONNX_TARGET_FORMATS\nif runners is None:\nrunners = default_runners(device_kind=target_device)\nif optimization_profile is None:\noptimization_profile = OptimizationProfile()\ntarget_formats_enums = enums.parse(target_formats, Format)\nrunner_names = enums.parse(runners, lambda runner: runner if isinstance(runner, str) else runner.name())\nif Format.ONNX not in target_formats_enums:\ntarget_formats_enums = (Format.ONNX,) + target_formats_enums\nconfig = CommonConfig(\nFramework.ONNX,\nmodel=model,\ndataloader=dataloader,\ntarget_formats=target_formats_enums,\ntarget_device=target_device,\nsample_count=sample_count,\nbatch_dim=0 if batching else None,\nrunner_names=runner_names,\noptimization_profile=optimization_profile,\nverbose=verbose,\ndebug=debug,\nverify_func=verify_func,\ncustom_configs=map_custom_configs(custom_configs=custom_configs),\n)\nmodels_config = ModelConfigBuilder.generate_model_config(\nframework=Framework.ONNX,\ntarget_formats=target_formats_enums,\ncustom_configs=custom_configs,\n)\nbuilders = [\npreprocessing_builder,\nonnx_export_builder,\nfind_device_max_batch_size_builder,\nonnx_conversion_builder,\ncorrectness_builder,\nperformance_builder,\nverify_builder,\n]\npackage = optimize_pipeline(\nmodel=model,\nworkspace=workspace,\nbuilders=builders,\nconfig=config,\nmodels_config=models_config,\n)\nreturn package\n</code></pre>"},{"location":"optimize/optimize/","title":"Using optimize","text":""},{"location":"optimize/optimize/#optimize-model","title":"Optimize Model","text":"<p>As mentioned previously model optimization plays a crucial role in unlocking the maximum performance capabilities of the underlying hardware. This sections describe in details how Model Navigator perform the optimization to improve the inference performance and reduce the cost.</p>"},{"location":"optimize/optimize/#overview","title":"Overview","text":"<p>The Model Navigator optimize process encompasses several crucial steps aimed at improving the performance of deep learning models and converting them into the most optimal formats. Model Navigator supports various frameworks, including TensorFlow 2, PyTorch, ONNX, and JAX.</p> <p>To initiate the multistep conversion and optimization process in <code>Model Navigator</code>, users only need to provide the <code>model</code> and <code>dataloader</code>. However, for further customization, additional parameters and <code>custom_configs</code> can be used to tailor the optimization process to specific requirements.</p> <p>The optimization process consists of the following steps:</p> <ol> <li> <p>Model export: The source deep learning model, created using one of the supported frameworks, is exported to one of    the intermediaries formats: TorchScript, SavedModel, ONNX.</p> </li> <li> <p>Model conversion: The exported model is then converted into a target representation with goal of achieving the best    possible performance, it includes: TorchTensorRT, TensorFlowTensorRT, ONNX, TensorRT.</p> </li> <li> <p>Correctness test: To ensure the correctness of the produced models, Model Navigator performs a series of    correctness    tests. These tests calculate absolute and relative tolerance values for source and converted models.</p> </li> <li> <p>Model profiling: Model Navigator conducts performance profiling of the converted models. This process    uses <code>Navigator Runners</code> to perform inference and measure its time. The profiler aims to find the maximum throughput    for each model and calculates its latency. This information can then be used to retrieve the best runners and provide    you with performance details of the optimal configuration. In that stage a single data sample is used to perform    profiling.</p> </li> <li> <p>Verification: Once the profiling is complete, Model Navigator performs verification tests to validate the metrics    provided by the user in <code>verify_func</code> against all converted models.</p> </li> </ol>"},{"location":"optimize/optimize/#example-usage","title":"Example Usage","text":"<p>By going through the Optimize process with Model Navigator, deep learning models can be optimized and converted into the most suitable formats for deployment, with NVIDIA TensorRT often providing the optimal solution to achieve the best performance.</p> <p>NVIDIA TensorRT can be used for applications deployed to the data center, as well as embedded and automotive environments. It powers key NVIDIA solutions such as NVIDIA TAO, NVIDIA DRIVE\u2122, NVIDIA Clara\u2122, and NVIDIA Jetpack\u2122. TensorRT is also integrated with application-specific SDKs, such as NVIDIA DeepStream, NVIDIA Riva, NVIDIA Merlin\u2122, NVIDIA Maxine\u2122, NVIDIA Morpheus, and NVIDIA Broadcast Engine to provide developers with a unified path to deploy intelligent video analytics, speech AI, recommender systems, video conference, AI based cybersecurity, and streaming apps in production.</p> <p>You can use those default TensorRT compute plans for your deployment to get very good performance for NVIDIA hardware.</p> <p>You can also apply quantization for some selected models to get better performance like in HiFiGAN example. This model uses quantization aware training so accuracy is very good but many other models can use post-training quantization by just enabling INT8 flag in optimize function. It can reduce accuracy, so you must validate the quantized model in such cases.</p> <p>Model Navigator can build for your quantized model, when flag <code>INT8</code> is used:</p> <pre><code>package = nav.torch.optimize(\n    model=model,\n    dataloader=dataloader,\n    custom_configs=[\n            nav.TensorRTConfig(precision=nav.api.config.TensorRTPrecision.INT8),\n    ],\n)\n</code></pre> <p>At the end, the summary of the execution is presented and artifacts stored in Navigator workspace, which by default is in the <code>navigator_workspace</code> folder.</p>"},{"location":"optimize/optimize/#what-next","title":"What next?","text":"<p>The result of optimize can be shared in form of Navigator Package or used for inference deployment on PyTriton or Triton Inference Server.</p>"},{"location":"optimize/python/","title":"Python","text":""},{"location":"optimize/python/#model_navigator.api.python","title":"model_navigator.api.python","text":"<p>Python optimize API.</p>"},{"location":"optimize/python/#model_navigator.api.python.optimize","title":"optimize","text":"<pre><code>optimize(\nmodel,\ndataloader,\nsample_count=DEFAULT_SAMPLE_COUNT,\nbatching=True,\ntarget_device=DeviceKind.CPU,\nrunners=None,\noptimization_profile=None,\nworkspace=None,\nverbose=False,\ndebug=False,\nverify_func=None,\ncustom_configs=None,\n)\n</code></pre> <p>Entrypoint for Python model optimize.</p> <p>Perform correctness testing, profiling and model verification.</p> <p>Parameters:</p> <ul> <li> model             (<code>Callable</code>)         \u2013          <p>Model inference function</p> </li> <li> dataloader             (<code>SizedDataLoader</code>)         \u2013          <p>Sized iterable with data that will be feed to the model</p> </li> <li> sample_count             (<code>int</code>)         \u2013          <p>Limits how many samples will be used from dataloader</p> </li> <li> batching             (<code>Optional[bool]</code>)         \u2013          <p>Enable or disable batching on first (index 0) dimension of the model</p> </li> <li> target_device             (<code>Optional[DeviceKind]</code>)         \u2013          <p>Target device for optimize process, default is CPU</p> </li> <li> runners             (<code>Optional[Tuple[Union[str, Type[NavigatorRunner]], ...]]</code>)         \u2013          <p>Use only runners provided as parameter</p> </li> <li> optimization_profile             (<code>Optional[OptimizationProfile]</code>)         \u2013          <p>Optimization profile for conversion and profiling</p> </li> <li> workspace             (<code>Optional[pathlib.Path]</code>)         \u2013          <p>Workspace where packages will be extracted</p> </li> <li> verbose             (<code>bool</code>)         \u2013          <p>Enable verbose logging</p> </li> <li> debug             (<code>bool</code>)         \u2013          <p>Enable debug logging from commands</p> </li> <li> verify_func             (<code>Optional[VerifyFunction]</code>)         \u2013          <p>Function for additional model verification</p> </li> <li> custom_configs             (<code>Optional[Sequence[CustomConfig]]</code>)         \u2013          <p>Sequence of CustomConfigs used to control produced artifacts</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Package</code>         \u2013          <p>Package descriptor representing created package.</p> </li> </ul> Source code in <code>model_navigator/api/python.py</code> <pre><code>def optimize(\nmodel: Callable,\ndataloader: SizedDataLoader,\nsample_count: int = DEFAULT_SAMPLE_COUNT,\nbatching: Optional[bool] = True,\ntarget_device: Optional[DeviceKind] = DeviceKind.CPU,\nrunners: Optional[Tuple[Union[str, Type[NavigatorRunner]], ...]] = None,\noptimization_profile: Optional[OptimizationProfile] = None,\nworkspace: Optional[pathlib.Path] = None,\nverbose: bool = False,\ndebug: bool = False,\nverify_func: Optional[VerifyFunction] = None,\ncustom_configs: Optional[Sequence[CustomConfig]] = None,\n) -&gt; Package:\n\"\"\"Entrypoint for Python model optimize.\n    Perform correctness testing, profiling and model verification.\n    Args:\n        model: Model inference function\n        dataloader: Sized iterable with data that will be feed to the model\n        sample_count: Limits how many samples will be used from dataloader\n        batching: Enable or disable batching on first (index 0) dimension of the model\n        target_device: Target device for optimize process, default is CPU\n        runners: Use only runners provided as parameter\n        optimization_profile: Optimization profile for conversion and profiling\n        workspace: Workspace where packages will be extracted\n        verbose: Enable verbose logging\n        debug: Enable debug logging from commands\n        verify_func: Function for additional model verification\n        custom_configs: Sequence of CustomConfigs used to control produced artifacts\n    Returns:\n        Package descriptor representing created package.\n    \"\"\"\nif isinstance(model, str):\nmodel = pathlib.Path(model)\nsample = next(iter(dataloader))\nforward_kw_names = tuple(sample.keys()) if isinstance(sample, Mapping) else None\nif runners is None:\nrunners = default_runners(device_kind=target_device)\nif optimization_profile is None:\noptimization_profile = OptimizationProfile()\nrunner_names = enums.parse(runners, lambda runner: runner if isinstance(runner, str) else runner.name())\ntarget_formats = DEFAULT_NONE_FRAMEWORK_TARGET_FORMATS\nconfig = CommonConfig(\nFramework.NONE,\nmodel=model,\ndataloader=dataloader,\nforward_kw_names=forward_kw_names,\ntarget_formats=target_formats,\ntarget_device=target_device,\nsample_count=sample_count,\nbatch_dim=0 if batching else None,\nrunner_names=runner_names,\noptimization_profile=optimization_profile,\nverbose=verbose,\ndebug=debug,\nverify_func=verify_func,\ncustom_configs=map_custom_configs(custom_configs=custom_configs),\n)\nmodels_config = ModelConfigBuilder.generate_model_config(\nframework=Framework.NONE,\ntarget_formats=target_formats,\ncustom_configs=[],\n)\nbuilders = [\npreprocessing_builder,\ncorrectness_builder,\nperformance_builder,\nverify_builder,\n]\npackage = optimize_pipeline(\nmodel=model,\nworkspace=workspace,\nbuilders=builders,\nconfig=config,\nmodels_config=models_config,\n)\nreturn package\n</code></pre>"},{"location":"optimize/tensorflow/","title":"TensorFlow 2","text":""},{"location":"optimize/tensorflow/#model_navigator.api.tensorflow","title":"model_navigator.api.tensorflow","text":"<p>TensorFlow optimize API.</p>"},{"location":"optimize/tensorflow/#model_navigator.api.tensorflow.optimize","title":"optimize","text":"<pre><code>optimize(\nmodel,\ndataloader,\nsample_count=DEFAULT_SAMPLE_COUNT,\nbatching=True,\ninput_names=None,\noutput_names=None,\ntarget_formats=None,\ntarget_device=DeviceKind.CUDA,\nrunners=None,\noptimization_profile=None,\nworkspace=None,\nverbose=False,\ndebug=False,\nverify_func=None,\ncustom_configs=None,\n)\n</code></pre> <p>Entrypoint for TensorFlow2 optimize.</p> <p>Perform export, conversion, correctness testing, profiling and model verification.</p> <p>Parameters:</p> <ul> <li> model             (<code>tensorflow.keras.Model</code>)         \u2013          <p>TensorFlow2 model object</p> </li> <li> dataloader             (<code>SizedDataLoader</code>)         \u2013          <p>Sized iterable with data that will be feed to the model</p> </li> <li> sample_count             (<code>int</code>)         \u2013          <p>Limits how many samples will be used from dataloader</p> </li> <li> batching             (<code>Optional[bool]</code>)         \u2013          <p>Enable or disable batching on first (index 0) dimension of the model</p> </li> <li> input_names             (<code>Optional[Tuple[str, ...]]</code>)         \u2013          <p>Model input names</p> </li> <li> output_names             (<code>Optional[Tuple[str, ...]]</code>)         \u2013          <p>Model output names</p> </li> <li> target_formats             (<code>Optional[Tuple[Union[str, Format], ...]]</code>)         \u2013          <p>Target model formats for optimize process</p> </li> <li> target_device             (<code>Optional[DeviceKind]</code>)         \u2013          <p>Target device for optimize process, default is CUDA</p> </li> <li> runners             (<code>Optional[Tuple[Union[str, Type[NavigatorRunner]], ...]]</code>)         \u2013          <p>Use only runners provided as parameter</p> </li> <li> optimization_profile             (<code>Optional[OptimizationProfile]</code>)         \u2013          <p>Optimization profile for conversion and profiling</p> </li> <li> workspace             (<code>Optional[pathlib.Path]</code>)         \u2013          <p>Workspace where packages will be extracted</p> </li> <li> verbose             (<code>bool</code>)         \u2013          <p>Enable verbose logging</p> </li> <li> debug             (<code>bool</code>)         \u2013          <p>Enable debug logging from commands</p> </li> <li> verify_func             (<code>Optional[VerifyFunction]</code>)         \u2013          <p>Function for additional model verification</p> </li> <li> custom_configs             (<code>Optional[Sequence[CustomConfig]]</code>)         \u2013          <p>Sequence of CustomConfigs used to control produced artifacts</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Package</code>         \u2013          <p>Package descriptor representing created package.</p> </li> </ul> Source code in <code>model_navigator/api/tensorflow.py</code> <pre><code>def optimize(\nmodel: tensorflow.keras.Model,\ndataloader: SizedDataLoader,\nsample_count: int = DEFAULT_SAMPLE_COUNT,\nbatching: Optional[bool] = True,\ninput_names: Optional[Tuple[str, ...]] = None,\noutput_names: Optional[Tuple[str, ...]] = None,\ntarget_formats: Optional[Tuple[Union[str, Format], ...]] = None,\ntarget_device: Optional[DeviceKind] = DeviceKind.CUDA,\nrunners: Optional[Tuple[Union[str, Type[NavigatorRunner]], ...]] = None,\noptimization_profile: Optional[OptimizationProfile] = None,\nworkspace: Optional[pathlib.Path] = None,\nverbose: bool = False,\ndebug: bool = False,\nverify_func: Optional[VerifyFunction] = None,\ncustom_configs: Optional[Sequence[CustomConfig]] = None,\n) -&gt; Package:\n\"\"\"Entrypoint for TensorFlow2 optimize.\n    Perform export, conversion, correctness testing, profiling and model verification.\n    Args:\n        model: TensorFlow2 model object\n        dataloader: Sized iterable with data that will be feed to the model\n        sample_count: Limits how many samples will be used from dataloader\n        batching: Enable or disable batching on first (index 0) dimension of the model\n        input_names: Model input names\n        output_names: Model output names\n        target_formats: Target model formats for optimize process\n        target_device: Target device for optimize process, default is CUDA\n        runners: Use only runners provided as parameter\n        optimization_profile: Optimization profile for conversion and profiling\n        workspace: Workspace where packages will be extracted\n        verbose: Enable verbose logging\n        debug: Enable debug logging from commands\n        verify_func: Function for additional model verification\n        custom_configs: Sequence of CustomConfigs used to control produced artifacts\n    Returns:\n        Package descriptor representing created package.\n    \"\"\"\nif target_device == DeviceKind.CPU and any(\ndevice.device_type == \"GPU\" for device in tensorflow.config.get_visible_devices()\n):\nraise ModelNavigatorConfigurationError(\n\"\\n\"\n\"    'target_device == nav.DeviceKind.CPU' is not supported for TensorFlow2 when GPU is available.\\n\"\n\"    To optimize model for CPU, disable GPU with: \"\n\"'tf.config.set_visible_devices([], 'GPU')' directly after importing TensorFlow.\\n\"\n)\nif target_formats is None:\ntarget_formats = DEFAULT_TENSORFLOW_TARGET_FORMATS\nif runners is None:\nrunners = default_runners(device_kind=target_device)\nforward_kw_names = None\nsample = next(iter(dataloader))\nif isinstance(sample, Mapping):\nforward_kw_names = tuple(sample.keys())\ntarget_formats_enums = enums.parse(target_formats, Format)\nrunner_names = enums.parse(runners, lambda runner: runner if isinstance(runner, str) else runner.name())\nif optimization_profile is None:\noptimization_profile = OptimizationProfile()\nif Format.TENSORFLOW not in target_formats_enums:\ntarget_formats_enums = (Format.TENSORFLOW,) + target_formats_enums\nconfig = CommonConfig(\nFramework.TENSORFLOW,\nmodel=model,\ndataloader=dataloader,\ntarget_formats=target_formats_enums,\ntarget_device=target_device,\nsample_count=sample_count,\n_input_names=input_names,\n_output_names=output_names,\nbatch_dim=0 if batching else None,\nrunner_names=runner_names,\noptimization_profile=optimization_profile,\nforward_kw_names=forward_kw_names,\nverbose=verbose,\ndebug=debug,\nverify_func=verify_func,\ncustom_configs=map_custom_configs(custom_configs=custom_configs),\n)\nmodels_config = ModelConfigBuilder.generate_model_config(\nframework=Framework.TENSORFLOW,\ntarget_formats=target_formats_enums,\ncustom_configs=custom_configs,\n)\nbuilders = [\npreprocessing_builder,\ntensorflow_export_builder,\nfind_device_max_batch_size_builder,\ntensorflow_conversion_builder,\ncorrectness_builder,\nperformance_builder,\nverify_builder,\n]\npackage = optimize_pipeline(\nmodel=model,\nworkspace=workspace,\nbuilders=builders,\nconfig=config,\nmodels_config=models_config,\n)\nreturn package\n</code></pre>"},{"location":"optimize/torch/","title":"PyTorch","text":""},{"location":"optimize/torch/#model_navigator.api.torch","title":"model_navigator.api.torch","text":"<p>Torch optimize API.</p>"},{"location":"optimize/torch/#model_navigator.api.torch.optimize","title":"optimize","text":"<pre><code>optimize(\nmodel,\ndataloader,\nsample_count=DEFAULT_SAMPLE_COUNT,\nbatching=True,\ninput_names=None,\noutput_names=None,\ntarget_formats=None,\ntarget_device=DeviceKind.CUDA,\nrunners=None,\noptimization_profile=None,\nworkspace=None,\nverbose=False,\ndebug=False,\nverify_func=None,\ncustom_configs=None,\n)\n</code></pre> <p>Entrypoint for Torch optimize.</p> <p>Perform export, conversion, correctness testing, profiling and model verification.</p> <p>Parameters:</p> <ul> <li> model             (<code>torch.nn.Module</code>)         \u2013          <p>PyTorch model object</p> </li> <li> dataloader             (<code>SizedDataLoader</code>)         \u2013          <p>Sized iterable with data that will be feed to the model</p> </li> <li> sample_count             (<code>Optional[int]</code>)         \u2013          <p>Limits how many samples will be used from dataloader</p> </li> <li> batching             (<code>Optional[bool]</code>)         \u2013          <p>Enable or disable batching on first (index 0) dimension of the model</p> </li> <li> input_names             (<code>Optional[Tuple[str, ...]]</code>)         \u2013          <p>Model input names</p> </li> <li> output_names             (<code>Optional[Tuple[str, ...]]</code>)         \u2013          <p>Model output names</p> </li> <li> target_formats             (<code>Optional[Tuple[Union[str, Format], ...]]</code>)         \u2013          <p>Target model formats for optimize process</p> </li> <li> target_device             (<code>Optional[DeviceKind]</code>)         \u2013          <p>Target device for optimize process, default is CUDA</p> </li> <li> runners             (<code>Optional[Tuple[Union[str, Type[NavigatorRunner]], ...]]</code>)         \u2013          <p>Use only runners provided as parameter</p> </li> <li> optimization_profile             (<code>Optional[OptimizationProfile]</code>)         \u2013          <p>Optimization profile for conversion and profiling</p> </li> <li> workspace             (<code>Optional[pathlib.Path]</code>)         \u2013          <p>Workspace where packages will be extracted</p> </li> <li> verbose             (<code>Optional[bool]</code>)         \u2013          <p>Enable verbose logging</p> </li> <li> debug             (<code>Optional[bool]</code>)         \u2013          <p>Enable debug logging from commands</p> </li> <li> verify_func             (<code>Optional[VerifyFunction]</code>)         \u2013          <p>Function for additional model verification</p> </li> <li> custom_configs             (<code>Optional[Sequence[CustomConfig]]</code>)         \u2013          <p>Sequence of CustomConfigs used to control produced artifacts</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Package</code>         \u2013          <p>Package descriptor representing created package.</p> </li> </ul> Source code in <code>model_navigator/api/torch.py</code> <pre><code>def optimize(\nmodel: torch.nn.Module,\ndataloader: SizedDataLoader,\nsample_count: Optional[int] = DEFAULT_SAMPLE_COUNT,\nbatching: Optional[bool] = True,\ninput_names: Optional[Tuple[str, ...]] = None,\noutput_names: Optional[Tuple[str, ...]] = None,\ntarget_formats: Optional[Tuple[Union[str, Format], ...]] = None,\ntarget_device: Optional[DeviceKind] = DeviceKind.CUDA,\nrunners: Optional[Tuple[Union[str, Type[NavigatorRunner]], ...]] = None,\noptimization_profile: Optional[OptimizationProfile] = None,\nworkspace: Optional[pathlib.Path] = None,\nverbose: Optional[bool] = False,\ndebug: Optional[bool] = False,\nverify_func: Optional[VerifyFunction] = None,\ncustom_configs: Optional[Sequence[CustomConfig]] = None,\n) -&gt; Package:\n\"\"\"Entrypoint for Torch optimize.\n    Perform export, conversion, correctness testing, profiling and model verification.\n    Args:\n        model: PyTorch model object\n        dataloader: Sized iterable with data that will be feed to the model\n        sample_count: Limits how many samples will be used from dataloader\n        batching: Enable or disable batching on first (index 0) dimension of the model\n        input_names: Model input names\n        output_names: Model output names\n        target_formats: Target model formats for optimize process\n        target_device: Target device for optimize process, default is CUDA\n        runners: Use only runners provided as parameter\n        optimization_profile: Optimization profile for conversion and profiling\n        workspace: Workspace where packages will be extracted\n        verbose: Enable verbose logging\n        debug: Enable debug logging from commands\n        verify_func: Function for additional model verification\n        custom_configs: Sequence of CustomConfigs used to control produced artifacts\n    Returns:\n        Package descriptor representing created package.\n    \"\"\"\nif target_formats is None:\ntarget_formats = DEFAULT_TORCH_TARGET_FORMATS\nif batching:\ntarget_formats, custom_configs = update_allowed_batching_parameters(\ntarget_formats=target_formats,\ncustom_configs=custom_configs,\n)\nLOGGER.info(f\"Using default target formats: {[tf.name for tf in target_formats]}\")\nsample = next(iter(dataloader))\nif isinstance(sample, Mapping):\nforward_kw_names = tuple(sample.keys())\nelse:\nforward_kw_names = None\nif runners is None:\nrunners = default_runners(device_kind=target_device)\ntarget_formats = enums.parse(target_formats, Format)\nrunner_names = enums.parse(runners, lambda runner: runner if isinstance(runner, str) else runner.name())\nif optimization_profile is None:\noptimization_profile = OptimizationProfile()\nif Format.TORCH not in target_formats:\ntarget_formats = (Format.TORCH,) + target_formats\nconfig = CommonConfig(\nframework=Framework.TORCH,\nmodel=model,\ndataloader=dataloader,\ntarget_formats=target_formats,\nsample_count=sample_count,\n_input_names=input_names,\n_output_names=output_names,\ntarget_device=target_device,\nforward_kw_names=forward_kw_names,\nbatch_dim=0 if batching else None,\nrunner_names=runner_names,\noptimization_profile=optimization_profile,\nverbose=verbose,\ndebug=debug,\nverify_func=verify_func,\ncustom_configs=map_custom_configs(custom_configs=custom_configs),\n)\nmodels_config = ModelConfigBuilder.generate_model_config(\nframework=Framework.TORCH,\ntarget_formats=target_formats,\ncustom_configs=custom_configs,\n)\nbuilders = [\npreprocessing_builder,\ntorch_export_builder,\nfind_device_max_batch_size_builder,\ntorch_conversion_builder,\ncorrectness_builder,\nperformance_builder,\nverify_builder,\n]\npackage = optimize_pipeline(\nmodel=model,\nworkspace=workspace,\nbuilders=builders,\nconfig=config,\nmodels_config=models_config,\n)\nreturn package\n</code></pre>"},{"location":"package/package/","title":"Using package","text":""},{"location":"package/package/#navigator-package","title":"Navigator Package","text":"<p>The model graph and/or checkpoint is not enough to perform a successful deployment of the model. When you are deploying model for inference you need to be aware of model inputs and outputs definition, maximal batch size that can be used for inference and other.</p> <p>On that purpose we have created a <code>Navigator Package</code> - an artifact containing the serialized model, model metadata and optimization details.</p> <p>The <code>Navigator Package</code> is recommended way of sharing the optimized model for deployment on PyTriton or Triton Inference Server sections or re-running the <code>optimize</code> method on different hardware.</p>"},{"location":"package/package/#save","title":"Save","text":"<p>The package created during optimize can be saved in form of Zip file using the API method:</p> <pre><code>import model_navigator as nav\nnav.package.save(\npackage=package,\npath=\"/path/to/package.nav\"\n)\n</code></pre> <p>The <code>save</code> method collect the generated models from workspace selecting:</p> <ul> <li>base formats - first available serialization formats exporting model from source</li> <li>max throughput format - the model that achieved the highest throughput during profiling</li> <li>min latency format - the model that achieved the minimal latency during profiling</li> </ul> <p>Additionally, the package contains:</p> <ul> <li>status file with optimization details</li> <li>logs from optimize execution</li> <li>reproduction script per each model format</li> <li>input and output data samples in form on numpy files</li> </ul> <p>Read more in save method API specification.</p>"},{"location":"package/package/#load","title":"Load","text":"<p>The packages saved to file can be loaded for further processing:</p> <pre><code>import model_navigator as nav\npackage = nav.package.load(\npath=\"/path/to/package.nav\"\n)\n</code></pre> <p>Once the package is loaded you can obtain desired information or use it to <code>optimize</code> or <code>profile</code> the package. Read more in load method API specification.</p>"},{"location":"package/package/#optimize","title":"Optimize","text":"<p>The loaded package object can be used to re-run the optimize process. In comparison to the framework dedicated API, the package optimize process start from the serialized models inside the package and reproduce the available optimization paths. This step can be used to reproduce the process without access to sources on different hardware.</p> <p>The optimization from package can be run using:</p> <pre><code>import model_navigator as nav\noptimized_package = nav.package.optimize(\npackage=package\n)\n</code></pre> <p>At the end of the process the new optimized models are generated. Please mind, the workspace is overridden in this step. Read more in optimize method API specification.</p>"},{"location":"package/package/#profile","title":"Profile","text":"<p>The optimize process use a single sample from dataloader for profiling. The process is focusing on selecting the best model format and this requires an unequivocal sample for performance comparison.</p> <p>In some cases you may want to profile the models on different dataset. On that purpose the Model Navigator expose the API for profiling all samples in dataset for each model:</p> <pre><code>import torch\nimport model_navigator as nav\nprofiling_results = nav.package.profile(\npackage=package,\ndataloader=[torch.randn(1, 3, 256, 256), torch.randn(1, 3, 512, 512)],\n)\n</code></pre> <p>The results contain profiling information per each model and sample. You can use it to perform desired analysis based on the results. Read more in profile method API specification.</p>"},{"location":"package/package_api/","title":"Package","text":""},{"location":"package/package_api/#model_navigator.package.package","title":"model_navigator.package.package","text":"<p>Package module - structure to snapshot optimization result.</p>"},{"location":"package/package_api/#model_navigator.package.package.Package","title":"Package","text":"<pre><code>Package(status, workspace, model=None)\n</code></pre> <p>Class for storing pipeline execution status.</p> <p>Initialize object.</p> <p>Parameters:</p> <ul> <li> status             (<code>Status</code>)         \u2013          <p>A navigator execution status</p> </li> <li> workspace             (<code>Workspace</code>)         \u2013          <p>Workspace for package files</p> </li> <li> model             (<code>Optional[object]</code>)         \u2013          <p>An optional model</p> </li> </ul> Source code in <code>model_navigator/package/package.py</code> <pre><code>def __init__(self, status: Status, workspace: Workspace, model: Optional[object] = None):\n\"\"\"Initialize object.\n    Args:\n        status: A navigator execution status\n        workspace: Workspace for package files\n        model: An optional model\n    \"\"\"\nself.status = status\nself.workspace = workspace\nself._model = model\nself._forward_kw_names = None\n</code></pre>"},{"location":"package/package_api/#model_navigator.package.package.Package.config","title":"config  <code>property</code>","text":"<pre><code>config: CommonConfig\n</code></pre> <p>Generate configuration from package.</p> <p>Returns:</p> <ul> <li> <code>CommonConfig</code>         \u2013          <p>The configuration object</p> </li> </ul>"},{"location":"package/package_api/#model_navigator.package.package.Package.framework","title":"framework  <code>property</code>","text":"<pre><code>framework: Framework\n</code></pre> <p>Framework for which package was created.</p> <p>Returns:</p> <ul> <li> <code>Framework</code>         \u2013          <p>Framework object for package</p> </li> </ul>"},{"location":"package/package_api/#model_navigator.package.package.Package.model","title":"model  <code>property</code>","text":"<pre><code>model: object\n</code></pre> <p>Return source model.</p> <p>Returns:</p> <ul> <li> <code>object</code>         \u2013          <p>Source model.</p> </li> </ul>"},{"location":"package/package_api/#model_navigator.package.package.Package._create_status_file","title":"_create_status_file","text":"<pre><code>_create_status_file()\n</code></pre> <p>Create a status.yaml file for package.</p> Source code in <code>model_navigator/package/package.py</code> <pre><code>def _create_status_file(self) -&gt; None:\n\"\"\"Create a status.yaml file for package.\"\"\"\npath = self.workspace.path / self.status_filename\nconfig = DataObject.filter_data(\ndata=self.status.config,\nfilter_fields=[\n\"model\",\n\"dataloader\",\n\"verify_func\",\n\"workspace\",\n],\n)\nconfig = DataObject.parse_data(config)\nstatus = copy.copy(self.status)\nstatus.config = config\ndata = status.to_dict(\nparse=True,\n)\nwith path.open(\"w\") as f:\nyaml.safe_dump(data, f, sort_keys=False)\n</code></pre>"},{"location":"package/package_api/#model_navigator.package.package.Package._delete_status_file","title":"_delete_status_file","text":"<pre><code>_delete_status_file()\n</code></pre> <p>Delete the status.yaml file from package.</p> Source code in <code>model_navigator/package/package.py</code> <pre><code>def _delete_status_file(self):\n\"\"\"Delete the status.yaml file from package.\"\"\"\npath = self.workspace.path / self.status_filename\nif path.exists():\npath.unlink()\n</code></pre>"},{"location":"package/package_api/#model_navigator.package.package.Package._get_custom_configs","title":"_get_custom_configs","text":"<pre><code>_get_custom_configs(custom_configs)\n</code></pre> <p>Build custom configs from config data.</p> <p>Parameters:</p> <ul> <li> custom_configs             (<code>Dict[str, Union[Dict, CustomConfigForFormat]]</code>)         \u2013          <p>Dictionary with custom configs data</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict</code>         \u2013          <p>List with mapped objects</p> </li> </ul> Source code in <code>model_navigator/package/package.py</code> <pre><code>def _get_custom_configs(self, custom_configs: Dict[str, Union[Dict, CustomConfigForFormat]]) -&gt; Dict:\n\"\"\"Build custom configs from config data.\n    Args:\n        custom_configs: Dictionary with custom configs data\n    Returns:\n        List with mapped objects\n    \"\"\"\ncustom_configs_mapped = {}\nfor class_name, obj in custom_configs.items():\nif isinstance(obj, dict):\ncustom_config_class = CUSTOM_CONFIGS_MAPPING[class_name]\nobj = custom_config_class.from_dict(obj)  # pytype: disable=not-instantiable\ncustom_configs_mapped[class_name] = obj\nreturn custom_configs_mapped\n</code></pre>"},{"location":"package/package_api/#model_navigator.package.package.Package._get_runner","title":"_get_runner","text":"<pre><code>_get_runner(model_key, runner_name, return_type=TensorType.NUMPY)\n</code></pre> <p>Load runner.</p> <p>Parameters:</p> <ul> <li> model_key             (<code>str</code>)         \u2013          <p>Unique key of the model.</p> </li> <li> runner_name             (<code>str</code>)         \u2013          <p>Name of the runner.</p> </li> <li> return_type             (<code>TensorType</code>)         \u2013          <p>Type of the runner output.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NavigatorRunner</code>         \u2013          <p>NavigatorRunner object</p> </li> </ul> Source code in <code>model_navigator/package/package.py</code> <pre><code>def _get_runner(\nself,\nmodel_key: str,\nrunner_name: str,\nreturn_type: TensorType = TensorType.NUMPY,\n) -&gt; NavigatorRunner:\n\"\"\"Load runner.\n    Args:\n        model_key (str): Unique key of the model.\n        runner_name (str): Name of the runner.\n        return_type (TensorType): Type of the runner output.\n    Raises:\n        ModelNavigatorNotFoundError when no runner found for provided constraints.\n    Returns:\n        NavigatorRunner object\n    \"\"\"\ntry:\nmodel_config = self.status.models_status[model_key].model_config\nexcept KeyError:\nraise ModelNavigatorNotFoundError(f\"Model {model_key} not found.\")\nif is_source_format(model_config.format):\nmodel = self._model\nelse:\nmodel = self.workspace.path / model_config.path\nreturn get_runner(runner_name)(\nmodel=model,\ninput_metadata=self.status.input_metadata,\noutput_metadata=self.status.output_metadata,\nreturn_type=return_type,\n)  # pytype: disable=not-instantiable\n</code></pre>"},{"location":"package/package_api/#model_navigator.package.package.Package.get_best_model_status","title":"get_best_model_status","text":"<pre><code>get_best_model_status(strategy=None, include_source=True)\n</code></pre> <p>Returns ModelStatus of best model for given strategy.</p> <p>Parameters:</p> <ul> <li> strategy             (<code>Optional[RuntimeSearchStrategy]</code>)         \u2013          <p>Strategy for finding the best runtime. Defaults to <code>MaxThroughputAndMinLatencyStrategy</code>.</p> </li> <li> include_source             (<code>bool</code>)         \u2013          <p>Flag if Python based model has to be included in analysis</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ModelStatus</code>         \u2013          <p>ModelStatus of best model for given strategy or None.</p> </li> </ul> Source code in <code>model_navigator/package/package.py</code> <pre><code>def get_best_model_status(\nself,\nstrategy: Optional[RuntimeSearchStrategy] = None,\ninclude_source: bool = True,\n) -&gt; ModelStatus:\n\"\"\"Returns ModelStatus of best model for given strategy.\n    Args:\n        strategy: Strategy for finding the best runtime. Defaults to `MaxThroughputAndMinLatencyStrategy`.\n        include_source: Flag if Python based model has to be included in analysis\n    Returns:\n        ModelStatus of best model for given strategy or None.\n    \"\"\"\nruntime_result = self._get_best_runtime(strategy=strategy, include_source=include_source)\nreturn runtime_result.model_status\n</code></pre>"},{"location":"package/package_api/#model_navigator.package.package.Package.get_model_path","title":"get_model_path","text":"<pre><code>get_model_path(model_key)\n</code></pre> <p>Return path of the model.</p> <p>Parameters:</p> <ul> <li> model_key             (<code>str</code>)         \u2013          <p>Unique key of the model.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ModelNavigatorNotFoundError</code>           \u2013          <p>When model not found.</p> </li> </ul> <p>Returns:</p> <ul> <li> Path(            <code>pathlib.Path</code> )        \u2013          <p>model path</p> </li> </ul> Source code in <code>model_navigator/package/package.py</code> <pre><code>def get_model_path(self, model_key: str) -&gt; pathlib.Path:\n\"\"\"Return path of the model.\n    Args:\n        model_key (str): Unique key of the model.\n    Raises:\n        ModelNavigatorNotFoundError: When model not found.\n    Returns:\n        Path: model path\n    \"\"\"\ntry:\nmodel_config = self.status.models_status[model_key].model_config\nexcept KeyError:\nraise ModelNavigatorNotFoundError(f\"Model {model_key} not found.\")\nreturn self.workspace.path / model_config.path\n</code></pre>"},{"location":"package/package_api/#model_navigator.package.package.Package.get_runner","title":"get_runner","text":"<pre><code>get_runner(strategy=None, include_source=True, return_type=TensorType.NUMPY)\n</code></pre> <p>Get the runner according to the strategy.</p> <p>Parameters:</p> <ul> <li> strategy             (<code>Optional[RuntimeSearchStrategy]</code>)         \u2013          <p>Strategy for finding the best runtime. Defaults to <code>MaxThroughputAndMinLatencyStrategy</code>.</p> </li> <li> include_source             (<code>bool</code>)         \u2013          <p>Flag if Python based model has to be included in analysis</p> </li> <li> return_type             (<code>TensorType</code>)         \u2013          <p>The type of the output tensor. Defaults to <code>TensorType.NUMPY</code>. If the return_type supports CUDA tensors (e.g. TensorType.TORCH) and the input tensors are on CUDA, there will be no additional data transfer between CPU and GPU.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NavigatorRunner</code>         \u2013          <p>The optimal runner for the optimized model.</p> </li> </ul> Source code in <code>model_navigator/package/package.py</code> <pre><code>def get_runner(\nself,\nstrategy: Optional[RuntimeSearchStrategy] = None,\ninclude_source: bool = True,\nreturn_type: TensorType = TensorType.NUMPY,\n) -&gt; NavigatorRunner:\n\"\"\"Get the runner according to the strategy.\n    Args:\n        strategy: Strategy for finding the best runtime. Defaults to `MaxThroughputAndMinLatencyStrategy`.\n        include_source: Flag if Python based model has to be included in analysis\n        return_type: The type of the output tensor. Defaults to `TensorType.NUMPY`.\n            If the return_type supports CUDA tensors (e.g. TensorType.TORCH) and the input tensors are on CUDA,\n            there will be no additional data transfer between CPU and GPU.\n    Returns:\n        The optimal runner for the optimized model.\n    \"\"\"\nruntime_result = self._get_best_runtime(strategy=strategy, include_source=include_source)\nmodel_config = runtime_result.model_status.model_config\nrunner_status = runtime_result.runner_status\nif not is_source_format(model_config.format) and not (self.workspace.path / model_config.path).exists():\nraise ModelNavigatorNotFoundError(\nf\"The best runner expects {model_config.format.value!r} \"\n\"model but it is not available in the loaded package.\"\n)\nif is_source_format(model_config.format) and self._model is None:\nraise ModelNavigatorMissingSourceModelError(\n\"The best runner uses the source model but it is not available in the loaded package. \"\n\"Please load the source model with `package.load_source_model(model)` \"\n\"or exclude source model from optimal runner search \"\n\"with `package.get_runner(include_source=False)`.\"\n)\nreturn self._get_runner(model_config.key, runner_status.runner_name, return_type=return_type)\n</code></pre>"},{"location":"package/package_api/#model_navigator.package.package.Package.is_empty","title":"is_empty","text":"<pre><code>is_empty()\n</code></pre> <p>Validate if package is empty - no models were produced.</p> <p>Returns:</p> <ul> <li> <code>bool</code>         \u2013          <p>True if empty package, False otherwise.</p> </li> </ul> Source code in <code>model_navigator/package/package.py</code> <pre><code>def is_empty(self) -&gt; bool:\n\"\"\"Validate if package is empty - no models were produced.\n    Returns:\n        True if empty package, False otherwise.\n    \"\"\"\nfor model_status in self.status.models_status.values():\nif not is_source_format(model_status.model_config.format):\nfor runner_status in model_status.runners_status.values():\nif (\nrunner_status.status.get(Correctness.__name__) == CommandStatus.OK\nand runner_status.status.get(Performance.__name__) != CommandStatus.FAIL\nand (self.workspace.path / model_status.model_config.path.parent).exists()\n):\nreturn False\nreturn True\n</code></pre>"},{"location":"package/package_api/#model_navigator.package.package.Package.load_source_model","title":"load_source_model","text":"<pre><code>load_source_model(model)\n</code></pre> <p>Load model defined in Python code.</p> <p>Parameters:</p> <ul> <li> model             (<code>object</code>)         \u2013          <p>A model object</p> </li> </ul> Source code in <code>model_navigator/package/package.py</code> <pre><code>def load_source_model(self, model: object) -&gt; None:\n\"\"\"Load model defined in Python code.\n    Args:\n        model: A model object\n    \"\"\"\nif self._model is not None:\nLOGGER.warning(\"Overriding existing source model.\")\nself._model = model\n</code></pre>"},{"location":"package/package_api/#model_navigator.package.package.Package.save_status_file","title":"save_status_file","text":"<pre><code>save_status_file()\n</code></pre> <p>Save the status.yaml.</p> Source code in <code>model_navigator/package/package.py</code> <pre><code>def save_status_file(self) -&gt; None:\n\"\"\"Save the status.yaml.\"\"\"\nself._delete_status_file()\nself._create_status_file()\n</code></pre>"},{"location":"package/package_load_api/","title":"Load","text":""},{"location":"package/package_load_api/#model_navigator.api.package.load","title":"model_navigator.api.package.load","text":"<pre><code>model_navigator.api.package.load(path, workspace=None)\n</code></pre> <p>Load package from provided path.</p> <p>Parameters:</p> <ul> <li> path             (<code>Union[str, pathlib.Path]</code>)         \u2013          <p>The location of package to load</p> </li> <li> workspace             (<code>Optional[Union[str, pathlib.Path]]</code>)         \u2013          <p>Workspace where packages will be extracted</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Package</code>         \u2013          <p>Package.</p> </li> </ul> Source code in <code>model_navigator/api/package.py</code> <pre><code>def load(\npath: Union[str, pathlib.Path],\nworkspace: Optional[Union[str, pathlib.Path]] = None,\n) -&gt; Package:\n\"\"\"Load package from provided path.\n    Args:\n        path: The location of package to load\n        workspace: Workspace where packages will be extracted\n    Returns:\n        Package.\n    \"\"\"\nLOGGER.info(f\"Loading package from {path} to {workspace}.\")\nworkspace = Workspace(workspace)\nworkspace.initialize()\nloader = PackageLoader()\npackage = loader.from_file(path=path, workspace=workspace)\nLOGGER.info(f\"Package loaded and unpacked {workspace}.\")\nreturn package\n</code></pre>"},{"location":"package/package_optimize_api/","title":"Optimize","text":""},{"location":"package/package_optimize_api/#model_navigator.api.package.optimize","title":"model_navigator.api.package.optimize","text":"<pre><code>model_navigator.api.package.optimize(\npackage,\ntarget_formats=None,\ntarget_device=DeviceKind.CUDA,\nrunners=None,\noptimization_profile=None,\nverbose=False,\ndebug=False,\nverify_func=None,\ncustom_configs=None,\ndefaults=True,\nfail_on_empty=True,\n)\n</code></pre> <p>Generate target formats and run correctness and profiling tests for available runners.</p> <p>Parameters:</p> <ul> <li> package             (<code>Package</code>)         \u2013          <p>Package to optimize.</p> </li> <li> target_formats             (<code>Optional[Tuple[Union[str, Format], ...]]</code>)         \u2013          <p>Formats to generate and profile. Defaults to target formats from the package.</p> </li> <li> target_device             (<code>Optional[DeviceKind]</code>)         \u2013          <p>Target device for optimize process, default is CUDA</p> </li> <li> runners             (<code>Optional[Tuple[Union[str, Type[NavigatorRunner]], ...]]</code>)         \u2013          <p>Runners to run correctness tests and profiling on. Defaults to runners from the package.</p> </li> <li> optimization_profile             (<code>Optional[OptimizationProfile]</code>)         \u2013          <p>Optimization profile used for conversion and profiling.</p> </li> <li> verbose             (<code>bool</code>)         \u2013          <p>If True enable verbose logging. Defaults to False.</p> </li> <li> debug             (<code>bool</code>)         \u2013          <p>If True print debugging logs. Defaults to False.</p> </li> <li> verify_func             (<code>Optional[VerifyFunction]</code>)         \u2013          <p>Function used for verifying generated models. Defaults to None.</p> </li> <li> custom_configs             (<code>Optional[List[CustomConfig]]</code>)         \u2013          <p>Custom formats configuration. Defaults to None.</p> </li> <li> defaults             (<code>bool</code>)         \u2013          <p>reset configuration of custom configs to defaults</p> </li> <li> fail_on_empty             (<code>bool</code>)         \u2013          <p>Fail optimization when empty (no model or base exported model) package provided</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Package</code>         \u2013          <p>Optimized package</p> </li> </ul> Source code in <code>model_navigator/api/package.py</code> <pre><code>def optimize(\npackage: Package,\ntarget_formats: Optional[Tuple[Union[str, Format], ...]] = None,\ntarget_device: Optional[DeviceKind] = DeviceKind.CUDA,\nrunners: Optional[Tuple[Union[str, Type[NavigatorRunner]], ...]] = None,\noptimization_profile: Optional[OptimizationProfile] = None,\nverbose: bool = False,\ndebug: bool = False,\nverify_func: Optional[VerifyFunction] = None,\ncustom_configs: Optional[List[CustomConfig]] = None,\ndefaults: bool = True,\nfail_on_empty: bool = True,\n) -&gt; Package:\n\"\"\"Generate target formats and run correctness and profiling tests for available runners.\n    Args:\n        package: Package to optimize.\n        target_formats: Formats to generate and profile. Defaults to target formats from the package.\n        target_device: Target device for optimize process, default is CUDA\n        runners: Runners to run correctness tests and profiling on. Defaults to runners from the package.\n        optimization_profile: Optimization profile used for conversion and profiling.\n        verbose: If True enable verbose logging. Defaults to False.\n        debug: If True print debugging logs. Defaults to False.\n        verify_func: Function used for verifying generated models. Defaults to None.\n        custom_configs: Custom formats configuration. Defaults to None.\n        defaults: reset configuration of custom configs to defaults\n        fail_on_empty: Fail optimization when empty (no model or base exported model) package provided\n    Returns:\n        Optimized package\n    \"\"\"\nif fail_on_empty and package.is_empty() and package.model is None:\nraise ModelNavigatorEmptyPackageError(\n\"Package is empty and source model is not loaded. Unable to run optimize.\"\n)\nconfig = package.config\nis_source_available = package.model is not None\nif target_formats is None:\ntarget_formats = get_target_formats(framework=package.framework, is_source_available=is_source_available)\nif package.framework == Framework.TORCH and config.batch_dim is not None:\ntarget_formats, custom_configs = update_allowed_batching_parameters(\ntarget_formats=target_formats,\ncustom_configs=custom_configs,\n)\nif runners is None:\nrunners = default_runners(device_kind=target_device)\nif optimization_profile is None:\noptimization_profile = OptimizationProfile()\n_update_config(\nconfig=config,\nis_source_available=is_source_available,\ntarget_formats=target_formats,\nrunners=runners,\noptimization_profile=optimization_profile,\nverbose=verbose,\ndebug=debug,\nverify_func=verify_func,\ncustom_configs=custom_configs,\ndefaults=defaults,\ntarget_device=target_device,\n)\nbuilders = _get_builders(\nframework=package.framework,\n)\nmodels_config = _get_model_configs(\nconfig=config,\ncustom_configs=list(config.custom_configs.values()),\n)\noptimized_package = optimize_pipeline(\npackage=package,\nworkspace=package.workspace.path,\nbuilders=builders,\nconfig=config,\nmodels_config=models_config,\n)\nreturn optimized_package\n</code></pre>"},{"location":"package/package_profile_api/","title":"Profile","text":""},{"location":"package/package_profile_api/#model_navigator.api.package.profile","title":"model_navigator.api.package.profile","text":"<pre><code>model_navigator.api.package.profile(\npackage,\ndataloader=None,\ntarget_formats=None,\ntarget_device=DeviceKind.CUDA,\nrunners=None,\nmax_batch_size=None,\nbatch_sizes=None,\nwindow_size=50,\nstability_percentage=10.0,\nmax_trials=10,\nthroughput_cutoff_threshold=DEFAULT_PROFILING_THROUGHPUT_CUTOFF_THRESHOLD,\nverbose=False,\n)\n</code></pre> <p>Profile provided package.</p> <p>When <code>dataloader</code> is provided, use all samples obtained from dataloader per-each batch size to perform profiling. The profiling result return the min, max and average results per batch size from all samples.</p> <p>When no <code>dataloader</code> provided, the profiling sample from package is used.</p> <p>Parameters:</p> <ul> <li> package             (<code>Package</code>)         \u2013          <p>Package to profile.</p> </li> <li> dataloader             (<code>Optional[SizedDataLoader]</code>)         \u2013          <p>Sized iterable with data that will be feed to the model</p> </li> <li> target_formats             (<code>Optional[Tuple[Union[str, Format], ...]]</code>)         \u2013          <p>Formats to profile. Defaults to target formats from the package.</p> </li> <li> target_device             (<code>Optional[DeviceKind]</code>)         \u2013          <p>Target device to run profiling on.</p> </li> <li> runners             (<code>Optional[Tuple[Union[str, Type[NavigatorRunner]], ...]]</code>)         \u2013          <p>Runners to run profiling on. Defaults to runners from the package.</p> </li> <li> max_batch_size             (<code>Optional[int]</code>)         \u2013          <p>Maximal batch size used for profiling. Default: None</p> </li> <li> batch_sizes             (<code>Optional[List[int]]</code>)         \u2013          <p>List of batch sizes to profile. Default: None</p> </li> <li> window_size             (<code>int</code>)         \u2013          <p>Number of inference queries performed in measurement window</p> </li> <li> stability_percentage             (<code>float</code>)         \u2013          <p>Allowed percentage of variation from the mean in three consecutive windows.</p> </li> <li> max_trials             (<code>int</code>)         \u2013          <p>Maximum number of window trials.</p> </li> <li> throughput_cutoff_threshold             (<code>float</code>)         \u2013          <p>Minimum throughput increase to continue profiling.</p> </li> <li> verbose             (<code>bool</code>)         \u2013          <p>If True enable verbose logging. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ProfilingResults</code>         \u2013          <p>Profiling results</p> </li> </ul> Source code in <code>model_navigator/api/package.py</code> <pre><code>def profile(\npackage: Package,\ndataloader: Optional[SizedDataLoader] = None,\ntarget_formats: Optional[Tuple[Union[str, Format], ...]] = None,\ntarget_device: Optional[DeviceKind] = DeviceKind.CUDA,\nrunners: Optional[Tuple[Union[str, Type[NavigatorRunner]], ...]] = None,\nmax_batch_size: Optional[int] = None,\nbatch_sizes: Optional[List[int]] = None,\nwindow_size: int = 50,\nstability_percentage: float = 10.0,\nmax_trials: int = 10,\nthroughput_cutoff_threshold: float = DEFAULT_PROFILING_THROUGHPUT_CUTOFF_THRESHOLD,\nverbose: bool = False,\n) -&gt; ProfilingResults:\n\"\"\"Profile provided package.\n    When `dataloader` is provided, use all samples obtained from dataloader per-each batch size to perform profiling.\n    The profiling result return the min, max and average results per batch size from all samples.\n    When no `dataloader` provided, the profiling sample from package is used.\n    Args:\n        package: Package to profile.\n        dataloader: Sized iterable with data that will be feed to the model\n        target_formats: Formats to profile. Defaults to target formats from the package.\n        target_device: Target device to run profiling on.\n        runners: Runners to run profiling on. Defaults to runners from the package.\n        max_batch_size: Maximal batch size used for profiling. Default: None\n        batch_sizes: List of batch sizes to profile. Default: None\n        window_size: Number of inference queries performed in measurement window\n        stability_percentage: Allowed percentage of variation from the mean in three consecutive windows.\n        max_trials: Maximum number of window trials.\n        throughput_cutoff_threshold: Minimum throughput increase to continue profiling.\n        verbose: If True enable verbose logging. Defaults to False.\n    Returns:\n        Profiling results\n    \"\"\"\nif package.is_empty() and package.model is None:\nraise ModelNavigatorEmptyPackageError(\n\"Package is empty and source model is not loaded. Unable to run optimize.\"\n)\nconfig = package.config\nis_source_available = package.model is not None\nif target_formats is None:\ntarget_formats = get_target_formats(framework=package.framework, is_source_available=is_source_available)\nif package.framework == Framework.TORCH and config.batch_dim is not None:\ntarget_formats, custom_configs = update_allowed_batching_parameters(\ntarget_formats=target_formats,\ncustom_configs=(),\n)\nif runners is None:\nrunners = default_runners(device_kind=target_device)\nif dataloader is None:\ndataloader = []\noptimization_profile = OptimizationProfile(\nmax_batch_size=max_batch_size,\nbatch_sizes=batch_sizes,\nwindow_size=window_size,\nstability_percentage=stability_percentage,\nmax_trials=max_trials,\nthroughput_cutoff_threshold=throughput_cutoff_threshold,\n)\n_update_config(\nconfig=config,\ndataloader=dataloader,\nis_source_available=is_source_available,\ntarget_formats=target_formats,\nrunners=runners,\noptimization_profile=optimization_profile,\nverbose=verbose,\ntarget_device=target_device,\n)\nbuilders = [\npreprocessing_builder,\nprofiling_builder,\n]\nmodel_configs = _get_model_configs(\nconfig=config,\ncustom_configs=[],\n)\nprofiling_results = profile_pipeline(\npackage=package,\nconfig=config,\nbuilders=builders,\nmodels_config=model_configs,\n)\nreturn profiling_results\n</code></pre>"},{"location":"package/package_profile_api/#model_navigator.api.package.ProfilingResults","title":"model_navigator.api.package.ProfilingResults  <code>dataclass</code>","text":"<p>Profiling results for models.</p> <p>Parameters:</p> <ul> <li> models             (<code>Dict[str, RunnerResults]</code>)         \u2013          <p>Mapping of models and their runner results</p> </li> </ul>"},{"location":"package/package_profile_api/#model_navigator.package.profiling_results.ProfilingResults.to_dict","title":"to_dict","text":"<pre><code>to_dict()\n</code></pre> <p>Return results in form of dictionary.</p> Source code in <code>model_navigator/package/profiling_results.py</code> <pre><code>def to_dict(self):\n\"\"\"Return results in form of dictionary.\"\"\"\nreturn dataclass2dict(self)\n</code></pre>"},{"location":"package/package_profile_api/#model_navigator.package.profiling_results.ProfilingResults.to_file","title":"to_file","text":"<pre><code>to_file(path)\n</code></pre> <p>Save results to file.</p> <p>Parameters:</p> <ul> <li> path             (<code>Union[str, pathlib.Path]</code>)         \u2013          <p>A path to yaml files</p> </li> </ul> Source code in <code>model_navigator/package/profiling_results.py</code> <pre><code>def to_file(self, path: Union[str, pathlib.Path]):\n\"\"\"Save results to file.\n    Args:\n        path: A path to yaml files\n    \"\"\"\npath = pathlib.Path(path)\ndata = self.to_dict()\nwith path.open(\"w\") as f:\nyaml.safe_dump(data, f, sort_keys=False)\n</code></pre>"},{"location":"package/package_profile_api/#model_navigator.package.profiling_results.RunnerResults","title":"model_navigator.package.profiling_results.RunnerResults  <code>dataclass</code>","text":"<p>Result for runners.</p> <p>Parameters:</p> <ul> <li> runners             (<code>Dict[str, RunnerProfilingResults]</code>)         \u2013          <p>Mapping of runner and their profiling results</p> </li> </ul>"},{"location":"package/package_profile_api/#model_navigator.package.profiling_results.RunnerProfilingResults","title":"model_navigator.package.profiling_results.RunnerProfilingResults  <code>dataclass</code>","text":"<p>Profiling results for runner.</p> <p>Parameters:</p> <ul> <li> status             (<code>CommandStatus</code>)         \u2013          <p>Status of profiling execution</p> </li> <li> detailed             (<code>Dict[int, List[ProfilingResult]]</code>)         \u2013          <p>Result mapping - per sample id</p> </li> </ul>"},{"location":"package/package_profile_api/#model_navigator.package.profiling_results.ProfilingResult","title":"model_navigator.package.profiling_results.ProfilingResult  <code>dataclass</code>","text":"<p>Result for single profiling for sample.</p> <p>Parameters:</p> <ul> <li> batch_size             (<code>int</code>)         \u2013          <p>Size of batch used for profiling</p> </li> <li> avg_latency             (<code>float</code>)         \u2013          <p>Average latency of profiling</p> </li> <li> std_latency             (<code>float</code>)         \u2013          <p>Standard deviation of profiled latency</p> </li> <li> p50_latency             (<code>float</code>)         \u2013          <p>50th percentile of measured latency</p> </li> <li> p90_latency             (<code>float</code>)         \u2013          <p>90th percentile of measured latency</p> </li> <li> p95_latency             (<code>float</code>)         \u2013          <p>95th percentile of measured latency</p> </li> <li> p99_latency             (<code>float</code>)         \u2013          <p>99th percentile of measured latency</p> </li> <li> throughput             (<code>float</code>)         \u2013          <p>Inferences per second</p> </li> <li> request_count             (<code>int</code>)         \u2013          <p>Number of inference requests</p> </li> </ul>"},{"location":"package/package_save_api/","title":"Save","text":""},{"location":"package/package_save_api/#model_navigator.api.package.save","title":"model_navigator.api.package.save","text":"<pre><code>model_navigator.api.package.save(package, path, override=False, save_data=True)\n</code></pre> <p>Save export results into the .nav package at given path.</p> <p>Parameters:</p> <ul> <li> package             (<code>Package</code>)         \u2013          <p>A package object to prepare the package</p> </li> <li> path             (<code>Union[str, pathlib.Path]</code>)         \u2013          <p>A path to file where the package has to be saved</p> </li> <li> override             (<code>bool</code>)         \u2013          <p>flag to override existing package in provided path</p> </li> <li> save_data             (<code>bool</code>)         \u2013          <p>disable saving samples from the dataloader</p> </li> </ul> Source code in <code>model_navigator/api/package.py</code> <pre><code>def save(\npackage: Package,\npath: Union[str, pathlib.Path],\noverride: bool = False,\nsave_data: bool = True,\n) -&gt; None:\n\"\"\"Save export results into the .nav package at given path.\n    Args:\n        package: A package object to prepare the package\n        path: A path to file where the package has to be saved\n        override: flag to override existing package in provided path\n        save_data: disable saving samples from the dataloader\n    \"\"\"\nbuilder = PackageBuilder()\nbuilder.save(\npackage=package,\npath=path,\noverride=override,\nsave_data=save_data,\n)\n</code></pre>"},{"location":"pytriton/pytriton_adapter/","title":"PyTritonAdapter API","text":""},{"location":"pytriton/pytriton_adapter/#model_navigator.api.pytriton.PyTritonAdapter","title":"model_navigator.api.pytriton.PyTritonAdapter","text":"<pre><code>model_navigator.api.pytriton.PyTritonAdapter(package, strategy=None)\n</code></pre> <p>Provides model and configuration for PyTrtion deployment.</p> <p>Initialize PyTritonAdapter.</p> <p>Parameters:</p> <ul> <li> package             (<code>Package</code>)         \u2013          <p>A package object to be searched for best possible model.</p> </li> <li> strategy             (<code>Optional[RuntimeSearchStrategy]</code>)         \u2013          <p>Strategy for finding the best model. Defaults to <code>MaxThroughputAndMinLatencyStrategy</code></p> </li> </ul> Source code in <code>model_navigator/api/pytriton.py</code> <pre><code>def __init__(self, package: Package, strategy: Optional[RuntimeSearchStrategy] = None):\n\"\"\"Initialize PyTritonAdapter.\n    Args:\n        package: A package object to be searched for best possible model.\n        strategy: Strategy for finding the best model. Defaults to `MaxThroughputAndMinLatencyStrategy`\n    \"\"\"\nself._package = package\nself._strategy = MaxThroughputAndMinLatencyStrategy() if strategy is None else strategy\nself._runner = self._package.get_runner(strategy=self._strategy)\nself._batching = self._package.status.config.get(\"batch_dim\", None) == 0\n</code></pre>"},{"location":"pytriton/pytriton_adapter/#model_navigator.api.pytriton.PyTritonAdapter.batching","title":"batching  <code>property</code>","text":"<pre><code>batching: bool\n</code></pre> <p>Returns status of batching support by the runner.</p> <p>Returns:</p> <ul> <li> <code>bool</code>         \u2013          <p>True if runner supports batching, False otherwise.</p> </li> </ul>"},{"location":"pytriton/pytriton_adapter/#model_navigator.api.pytriton.PyTritonAdapter.config","title":"config  <code>property</code>","text":"<pre><code>config: ModelConfig\n</code></pre> <p>Returns config for pytriton.</p> <p>Returns:</p> <ul> <li> <code>ModelConfig</code>         \u2013          <p>ModelConfig with configuration for PyTrtion bind method.</p> </li> </ul>"},{"location":"pytriton/pytriton_adapter/#model_navigator.api.pytriton.PyTritonAdapter.inputs","title":"inputs  <code>property</code>","text":"<pre><code>inputs: List[Tensor]\n</code></pre> <p>Returns inputs configuration.</p> <p>Returns:</p> <ul> <li> <code>List[Tensor]</code>         \u2013          <p>List with Tensor objects describing inputs configuration of runner</p> </li> </ul>"},{"location":"pytriton/pytriton_adapter/#model_navigator.api.pytriton.PyTritonAdapter.outputs","title":"outputs  <code>property</code>","text":"<pre><code>outputs: List[Tensor]\n</code></pre> <p>Returns outputs configuration.</p> <p>Returns:</p> <ul> <li> <code>List[Tensor]</code>         \u2013          <p>List with Tensor objects describing outpus configuration of runner</p> </li> </ul>"},{"location":"pytriton/pytriton_adapter/#model_navigator.api.pytriton.PyTritonAdapter.runner","title":"runner  <code>property</code>","text":"<pre><code>runner: NavigatorRunner\n</code></pre> <p>Returns runner.</p> <pre><code>Runner must be activated before use with activate() method.\n</code></pre> <p>Returns:</p> <ul> <li> <code>NavigatorRunner</code>         \u2013          <p>Model Navigator runner.</p> </li> </ul>"},{"location":"pytriton/pytriton_adapter/#model_navigator.api.pytriton.ModelConfig","title":"model_navigator.api.pytriton.ModelConfig  <code>dataclass</code>","text":"<p>Additional model configuration for running model through Triton Inference Server.</p> <p>Parameters:</p> <ul> <li> batching             (<code>bool</code>)         \u2013          <p>Flag to enable/disable batching for model.</p> </li> <li> max_batch_size             (<code>int</code>)         \u2013          <p>The maximal batch size that would be handled by model.</p> </li> <li> batcher             (<code>DynamicBatcher</code>)         \u2013          <p>Configuration of Dynamic Batching for the model.</p> </li> <li> response_cache             (<code>bool</code>)         \u2013          <p>Flag to enable/disable response cache for the model</p> </li> </ul>"},{"location":"pytriton/pytriton_adapter/#model_navigator.api.pytriton.DynamicBatcher","title":"model_navigator.api.pytriton.DynamicBatcher  <code>dataclass</code>","text":"<p>Dynamic batcher configuration.</p> <p>More in Triton Inference Server documentation</p> <p>Parameters:</p> <ul> <li> max_queue_delay_microseconds             (<code>int</code>)         \u2013          <p>The maximum time, in microseconds, a request will be delayed in                           the scheduling queue to wait for additional requests for batching.</p> </li> <li> preferred_batch_size             (<code>Optional[list]</code>)         \u2013          <p>Preferred batch sizes for dynamic batching.</p> </li> <li> preserve_ordering             (<code>bool</code>)         \u2013          <p>Should the dynamic batcher preserve the ordering of responses to                 match the order of requests received by the scheduler.</p> </li> <li> priority_levels             (<code>int</code>)         \u2013          <p>The number of priority levels to be enabled for the model.</p> </li> <li> default_priority_level             (<code>int</code>)         \u2013          <p>The priority level used for requests that don't specify their priority.</p> </li> <li> default_queue_policy             (<code>Optional[QueuePolicy]</code>)         \u2013          <p>The default queue policy used for requests.</p> </li> <li> priority_queue_policy             (<code>Optional[Dict[int, QueuePolicy]]</code>)         \u2013          <p>Specify the queue policy for the priority level.</p> </li> </ul>"},{"location":"pytriton/pytriton_adapter/#model_navigator.api.pytriton.Tensor","title":"model_navigator.api.pytriton.Tensor  <code>dataclass</code>","text":"<p>Model input and output definition for Triton deployment.</p> <p>Parameters:</p> <ul> <li> shape             (<code>tuple</code>)         \u2013          <p>Shape of the input/output tensor.</p> </li> <li> dtype             (<code>Union[np.dtype, Type[np.dtype], Type[object]]</code>)         \u2013          <p>Data type of the input/output tensor.</p> </li> <li> name             (<code>Optional[str]</code>)         \u2013          <p>Name of the input/output of model.</p> </li> <li> optional             (<code>Optional[bool]</code>)         \u2013          <p>Flag to mark if input is optional.</p> </li> </ul>"},{"location":"pytriton/pytriton_adapter/#model_navigator.api.pytriton.QueuePolicy","title":"model_navigator.api.pytriton.QueuePolicy  <code>dataclass</code>","text":"<p>Model queue policy configuration.</p> <p>More in Triton Inference Server documentation</p> <p>Parameters:</p> <ul> <li> timeout_action             (<code>TimeoutAction</code>)         \u2013          <p>The action applied to timed-out request.</p> </li> <li> default_timeout_microseconds             (<code>int</code>)         \u2013          <p>The default timeout for every request, in microseconds.</p> </li> <li> allow_timeout_override             (<code>bool</code>)         \u2013          <p>Whether individual request can override the default timeout value.</p> </li> <li> max_queue_size             (<code>int</code>)         \u2013          <p>The maximum queue size for holding requests.</p> </li> </ul>"},{"location":"pytriton/pytriton_adapter/#model_navigator.api.pytriton.TimeoutAction","title":"model_navigator.api.pytriton.TimeoutAction","text":"<p>             Bases: <code>enum.Enum</code></p> <p>Timeout action definition for timeout_action QueuePolicy field.</p> <p>Parameters:</p> <ul> <li> REJECT             (<code>str</code>)         \u2013          <p>Reject the request and return error message accordingly.</p> </li> <li> DELAY             (<code>str</code>)         \u2013          <p>Delay the request until all other requests at the same (or higher) priority levels that have not reached their timeouts are processed.</p> </li> </ul>"},{"location":"pytriton/pytriton_deployment/","title":"Deploying models","text":""},{"location":"pytriton/pytriton_deployment/#pytriton","title":"PyTriton","text":"<p>The PyTriton is a Flask/FastAPI-like interface that simplifies Triton's deployment in Python environments. In general using PyTriton can serve any Python function. The Model Navigator provide a <code>runner</code> - an abstraction that connects the model checkpoint with its runtime, making the inference process more accessible and straightforward. The <code>runner</code> is a Python API through which an optimized model can serve inference.</p>"},{"location":"pytriton/pytriton_deployment/#obtaining-runner-from-package","title":"Obtaining runner from Package","text":"<p>The Navigator Package provides an API for obtaining the model for serving inference. One of the option is to obtain the <code>runner</code>:</p> <pre><code>runner = package.get_runner()\n</code></pre> <p>The default behavior is to select the model and runner which during profiling obtained the smallest latency and the largest throughput. This runner is considered as most optimal for serving inference queries. Learn more about <code>get_runner</code> method in Navigator Package API.</p> <p>In order to use the runner in PyTriton additional information for the serving model is required. For that purpose we provide a <code>PyTritonAdapter</code> that contains all minimal information required to prepare successful deployment of model using PyTriton.</p>"},{"location":"pytriton/pytriton_deployment/#using-pytritonadapter","title":"Using PyTritonAdapter","text":"<p>Model Navigator provide a dedicated <code>PyTritonAdapter</code> to retrieve the <code>runner</code> and other information required to bind a model for serving inference. Following that, you can initialize the PyTriton server using the adapter information:</p> <pre><code>pytriton_adapter = nav.pytriton.PyTritonAdapter(package=package, strategy=nav.MaxThroughputStrategy())\nrunner = pytriton_adapter.runner\nrunner.activate()\n@batch\ndef infer_func(**inputs):\nreturn runner.infer(inputs)\nwith Triton() as triton:\ntriton.bind(\nmodel_name=\"resnet50\",\ninfer_func=infer_func,\ninputs=pytriton_adapter.inputs,\noutputs=pytriton_adapter.outputs,\nconfig=pytriton_adapter.config,\n)\ntriton.serve()\n</code></pre> <p>Once the python script is executed, the model inference is served through HTTP/gRPC endpoints.</p>"},{"location":"triton/accelerators/","title":"Accelerators","text":""},{"location":"triton/accelerators/#accelerators","title":"Accelerators","text":""},{"location":"triton/accelerators/#model_navigator.api.triton.AutoMixedPrecisionAccelerator","title":"model_navigator.api.triton.AutoMixedPrecisionAccelerator  <code>dataclass</code>","text":"<p>Auto-mixed-precision accelerator for TensorFlow. Enable automatic FP16 precision.</p> <p>Currently empty - no arguments required.</p>"},{"location":"triton/accelerators/#model_navigator.api.triton.GPUIOAccelerator","title":"model_navigator.api.triton.GPUIOAccelerator  <code>dataclass</code>","text":"<p>GPU IO accelerator for TensorFlow.</p> <p>Currently empty - no arguments required.</p>"},{"location":"triton/accelerators/#model_navigator.api.triton.OpenVINOAccelerator","title":"model_navigator.api.triton.OpenVINOAccelerator  <code>dataclass</code>","text":"<p>OpenVINO optimization.</p> <p>Currently empty - no arguments required.</p>"},{"location":"triton/accelerators/#model_navigator.api.triton.OpenVINOAccelerator","title":"model_navigator.api.triton.OpenVINOAccelerator  <code>dataclass</code>","text":"<p>OpenVINO optimization.</p> <p>Currently empty - no arguments required.</p>"},{"location":"triton/accelerators/#model_navigator.api.triton.TensorRTAccelerator","title":"model_navigator.api.triton.TensorRTAccelerator  <code>dataclass</code>","text":"<p>TensorRT accelerator configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> <ul> <li> precision             (<code>TensorRTOptPrecision</code>)         \u2013          <p>The precision used for optimization</p> </li> <li> max_workspace_size             (<code>Optional[int]</code>)         \u2013          <p>The maximum GPU memory the model can use temporarily during execution</p> </li> <li> max_cached_engines             (<code>Optional[int]</code>)         \u2013          <p>The maximum number of cached TensorRT engines in dynamic TensorRT ops</p> </li> <li> minimum_segment_size             (<code>Optional[int]</code>)         \u2013          <p>The smallest model subgraph that will be considered for optimization by TensorRT</p> </li> </ul>"},{"location":"triton/accelerators/#model_navigator.api.triton.TensorRTOptPrecision","title":"model_navigator.api.triton.TensorRTOptPrecision","text":"<p>             Bases: <code>enum.Enum</code></p> <p>TensorRT optimization allowed precision.</p> <p>Parameters:</p> <ul> <li> FP16         \u2013          <p>fp16 precision</p> </li> <li> FP32         \u2013          <p>fp32 precision</p> </li> </ul>"},{"location":"triton/adding_model/","title":"Adding Model","text":""},{"location":"triton/adding_model/#model-store-api","title":"Model Store API","text":""},{"location":"triton/adding_model/#model_navigator.triton.model_repository.add_model","title":"model_navigator.triton.model_repository.add_model","text":"<pre><code>model_navigator.triton.model_repository.add_model(\nmodel_repository_path, model_name, model_path, config, model_version=1\n)\n</code></pre> <p>Generate model deployment inside provided model store path.</p> <p>The config requires specialized configuration to be passed for backend on which model is executed. Example:</p> <ul> <li>ONNX model requires ONNXModelConfig</li> <li>TensorRT model requires TensorRTModelConfig</li> <li>TorchScript or Torch-TensorRT models requires PyTorchModelConfig</li> <li>TensorFlow SavedModel or TensorFlow-TensorRT models requires TensorFlowModelConfig</li> <li>Python model requires PythonModelConfig</li> </ul> <p>Parameters:</p> <ul> <li> model_repository_path             (<code>Union[str, pathlib.Path]</code>)         \u2013          <p>Path where deployment should be created</p> </li> <li> model_name             (<code>str</code>)         \u2013          <p>Name under which model is deployed in Triton Inference Server</p> </li> <li> model_path             (<code>Union[str, pathlib.Path]</code>)         \u2013          <p>Path to model</p> </li> <li> config             (<code>Union[ONNXModelConfig, TensorRTModelConfig, PyTorchModelConfig, PythonModelConfig, TensorFlowModelConfig]</code>)         \u2013          <p>Specialized configuration of model for backend on which model is executed</p> </li> <li> model_version             (<code>int</code>)         \u2013          <p>Version of model that is deployed</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>pathlib.Path</code>         \u2013          <p>Path to created model store</p> </li> </ul> Source code in <code>model_navigator/triton/model_repository.py</code> <pre><code>def add_model(\nmodel_repository_path: Union[str, pathlib.Path],\nmodel_name: str,\nmodel_path: Union[str, pathlib.Path],\nconfig: Union[\nONNXModelConfig,\nTensorRTModelConfig,\nPyTorchModelConfig,\nPythonModelConfig,\nTensorFlowModelConfig,\n],\nmodel_version: int = 1,\n) -&gt; pathlib.Path:\n\"\"\"Generate model deployment inside provided model store path.\n    The config requires specialized configuration to be passed for backend on which model is executed. Example:\n    - ONNX model requires ONNXModelConfig\n    - TensorRT model requires TensorRTModelConfig\n    - TorchScript or Torch-TensorRT models requires PyTorchModelConfig\n    - TensorFlow SavedModel or TensorFlow-TensorRT models requires TensorFlowModelConfig\n    - Python model requires PythonModelConfig\n    Args:\n        model_repository_path: Path where deployment should be created\n        model_name: Name under which model is deployed in Triton Inference Server\n        model_path: Path to model\n        config: Specialized configuration of model for backend on which model is executed\n        model_version: Version of model that is deployed\n    Returns:\n         Path to created model store\n    \"\"\"\nif isinstance(config, ONNXModelConfig):\nmodel_config = ModelConfigBuilder.from_onnx_config(\nmodel_name=model_name,\nmodel_version=model_version,\nonnx_config=config,\n)\nelif isinstance(config, TensorFlowModelConfig):\nmodel_config = ModelConfigBuilder.from_tensorflow_config(\nmodel_name=model_name,\nmodel_version=model_version,\ntensorflow_config=config,\n)\nelif isinstance(config, PythonModelConfig):\nmodel_config = ModelConfigBuilder.from_python_config(\nmodel_name=model_name,\nmodel_version=model_version,\npython_config=config,\n)\nelif isinstance(config, PyTorchModelConfig):\nmodel_config = ModelConfigBuilder.from_pytorch_config(\nmodel_name=model_name,\nmodel_version=model_version,\npytorch_config=config,\n)\nelif isinstance(config, TensorRTModelConfig):\nmodel_config = ModelConfigBuilder.from_tensorrt_config(\nmodel_name=model_name,\nmodel_version=model_version,\ntensorrt_config=config,\n)\nelse:\nraise ModelNavigatorWrongParameterError(f\"Unsupported model config provided: {config.__class__}\")\ntriton_model_repository = _TritonModelRepository(model_repository_path=pathlib.Path(model_repository_path))\nreturn triton_model_repository.deploy_model(\nmodel_path=pathlib.Path(model_path),\nmodel_config=model_config,\n)\n</code></pre>"},{"location":"triton/adding_model/#model_navigator.triton.model_repository.add_model_from_package","title":"model_navigator.triton.model_repository.add_model_from_package","text":"<pre><code>model_navigator.triton.model_repository.add_model_from_package(\nmodel_repository_path, model_name, package, model_version=1, strategy=None, response_cache=False\n)\n</code></pre> <p>Create the Triton Model Store with optimized model and save it to <code>model_repository_path</code>.</p> <p>Parameters:</p> <ul> <li> model_repository_path             (<code>Union[str, pathlib.Path]</code>)         \u2013          <p>Path where the model store is located</p> </li> <li> model_name             (<code>str</code>)         \u2013          <p>Name under which model is deployed in Triton Inference Server</p> </li> <li> model_version             (<code>int</code>)         \u2013          <p>Version of model that is deployed</p> </li> <li> package             (<code>Package</code>)         \u2013          <p>Package for which model store is created</p> </li> <li> strategy             (<code>Optional[RuntimeSearchStrategy]</code>)         \u2013          <p>Strategy for finding the best runtime.       When not set the <code>MaxThroughputAndMinLatencyStrategy</code> is used.</p> </li> <li> response_cache             (<code>bool</code>)         \u2013          <p>Enable response cache for model</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>Path to created model store</p> </li> </ul> Source code in <code>model_navigator/triton/model_repository.py</code> <pre><code>def add_model_from_package(\nmodel_repository_path: Union[str, pathlib.Path],\nmodel_name: str,\npackage: Package,\nmodel_version: int = 1,\nstrategy: Optional[RuntimeSearchStrategy] = None,\nresponse_cache: bool = False,\n):\n\"\"\"Create the Triton Model Store with optimized model and save it to `model_repository_path`.\n    Args:\n        model_repository_path: Path where the model store is located\n        model_name: Name under which model is deployed in Triton Inference Server\n        model_version: Version of model that is deployed\n        package: Package for which model store is created\n        strategy: Strategy for finding the best runtime.\n                  When not set the `MaxThroughputAndMinLatencyStrategy` is used.\n        response_cache: Enable response cache for model\n    Returns:\n        Path to created model store\n    \"\"\"\nif package.is_empty():\nraise ModelNavigatorEmptyPackageError(\"No models available in the package. Triton deployment is not possible.\")\nif package.config.batch_dim not in [0, None]:\nraise ModelNavigatorWrongParameterError(\n\"Only models without batching or batch dimension on first place in shape are supported for Triton.\"\n)\nif strategy is None:\nstrategy = MaxThroughputAndMinLatencyStrategy()\nbatching = package.config.batch_dim == 0\nruntime_result = RuntimeAnalyzer.get_runtime(\nmodels_status=package.status.models_status,\nstrategy=strategy,\nformats=[fmt.value for fmt in TRITON_FORMATS],\nrunners=[runner.name() for runner in TRITON_RUNNERS],\n)\nmax_batch_size = max(\nprofiling_results.batch_size\nfor profiling_results in runtime_result.runner_status.result[Performance.name][\"profiling_results\"]\n)\nif runtime_result.model_status.model_config.format == Format.ONNX:\nconfig = _onnx_config_from_runtime_result(\nbatching=batching,\nmax_batch_size=max_batch_size,\nresponse_cache=response_cache,\nruntime_result=runtime_result,\n)\nelif runtime_result.model_status.model_config.format in [Format.TF_SAVEDMODEL, Format.TF_TRT]:\nconfig = _tensorflow_config_from_runtime_result(\nbatching=batching,\nmax_batch_size=max_batch_size,\nresponse_cache=response_cache,\nruntime_result=runtime_result,\n)\nelif runtime_result.model_status.model_config.format in [Format.TORCHSCRIPT, Format.TORCH_TRT]:\ninputs = input_tensor_from_metadata(\npackage.status.input_metadata,\nbatching=batching,\n)\noutputs = output_tensor_from_metadata(\npackage.status.output_metadata,\nbatching=batching,\n)\nconfig = _pytorch_config_from_runtime_result(\nbatching=batching,\nmax_batch_size=max_batch_size,\ninputs=inputs,\noutputs=outputs,\nresponse_cache=response_cache,\nruntime_result=runtime_result,\n)\nelif runtime_result.model_status.model_config.format == Format.TENSORRT:\nconfig = _tensorrt_config_from_runtime_result(\nbatching=batching,\nmax_batch_size=max_batch_size,\nresponse_cache=response_cache,\n)\nelse:\nraise ModelNavigatorError(\nf\"Unsupported model format selected: {runtime_result.model_status.model_config.format}\"\n)\nreturn add_model(\nmodel_repository_path=model_repository_path,\nmodel_name=model_name,\nmodel_version=model_version,\nmodel_path=package.workspace.path / runtime_result.model_status.model_config.path,\nconfig=config,\n)\n</code></pre>"},{"location":"triton/dynamic_batcher/","title":"Dynamic Batcher","text":""},{"location":"triton/dynamic_batcher/#dynamic-batcher","title":"Dynamic Batcher","text":""},{"location":"triton/dynamic_batcher/#model_navigator.api.triton.DynamicBatcher","title":"model_navigator.api.triton.DynamicBatcher  <code>dataclass</code>","text":"<p>Dynamic batching configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> <ul> <li> max_queue_delay_microseconds             (<code>int</code>)         \u2013          <p>The maximum time, in microseconds, a request will be delayed in                           the scheduling queue to wait for additional requests for batching.</p> </li> <li> preferred_batch_size             (<code>Optional[list]</code>)         \u2013          <p>Preferred batch sizes for dynamic batching.</p> </li> <li> preserve_ordering         \u2013          <p>Should the dynamic batcher preserve the ordering of responses to                 match the order of requests received by the scheduler.</p> </li> <li> priority_levels             (<code>int</code>)         \u2013          <p>The number of priority levels to be enabled for the model.</p> </li> <li> default_priority_level             (<code>int</code>)         \u2013          <p>The priority level used for requests that don't specify their priority.</p> </li> <li> default_queue_policy             (<code>Optional[QueuePolicy]</code>)         \u2013          <p>The default queue policy used for requests.</p> </li> <li> priority_queue_policy             (<code>Optional[Dict[int, QueuePolicy]]</code>)         \u2013          <p>Specify the queue policy for the priority level.</p> </li> </ul>"},{"location":"triton/dynamic_batcher/#model_navigator.api.triton.QueuePolicy","title":"model_navigator.api.triton.QueuePolicy  <code>dataclass</code>","text":"<p>Model queue policy configuration.</p> <p>Used for <code>default_queue_policy</code> and <code>priority_queue_policy</code> fields in DynamicBatcher configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> <ul> <li> timeout_action             (<code>TimeoutAction</code>)         \u2013          <p>The action applied to timed-out request.</p> </li> <li> default_timeout_microseconds             (<code>int</code>)         \u2013          <p>The default timeout for every request, in microseconds.</p> </li> <li> allow_timeout_override             (<code>bool</code>)         \u2013          <p>Whether individual request can override the default timeout value.</p> </li> <li> max_queue_size             (<code>int</code>)         \u2013          <p>The maximum queue size for holding requests.</p> </li> </ul>"},{"location":"triton/dynamic_batcher/#model_navigator.api.triton.TimeoutAction","title":"model_navigator.api.triton.TimeoutAction","text":"<p>             Bases: <code>enum.Enum</code></p> <p>Timeout action definition for timeout_action QueuePolicy field.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> <ul> <li> REJECT         \u2013          <p>\"REJECT\"</p> </li> <li> DELAY         \u2013          <p>\"DELAY\"</p> </li> </ul>"},{"location":"triton/inputs_and_outputs/","title":"Inputs and Outputs","text":""},{"location":"triton/inputs_and_outputs/#model-inputs-and-outputs","title":"Model Inputs and Outputs","text":""},{"location":"triton/inputs_and_outputs/#model_navigator.api.triton.InputTensorSpec","title":"model_navigator.api.triton.InputTensorSpec  <code>dataclass</code>","text":"<p>             Bases: <code>BaseTensorSpec</code></p> <p>Stores specification of single input tensor.</p> <p>This includes name, shape, dtype and more parameters available for input tensor in Triton Inference Server:</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> <ul> <li> optional             (<code>bool</code>)         \u2013          <p>Flag marking the input is optional for the model execution</p> </li> <li> format             (<code>Optional[InputTensorFormat]</code>)         \u2013          <p>The format of the input.</p> </li> <li> allow_ragged_batch             (<code>bool</code>)         \u2013          <p>Flag marking the input is allowed to be \"ragged\" in a dynamically created batch.</p> </li> </ul>"},{"location":"triton/inputs_and_outputs/#model_navigator.api.triton.InputTensorFormat","title":"model_navigator.api.triton.InputTensorFormat","text":"<p>             Bases: <code>enum.Enum</code></p> <p>Format for input tensor.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> <ul> <li> FORMAT_NONE         \u2013          <p>0</p> </li> <li> FORMAT_NHWC         \u2013          <p>1</p> </li> <li> FORMAT_NCHW         \u2013          <p>2</p> </li> </ul>"},{"location":"triton/inputs_and_outputs/#model_navigator.api.triton.OutputTensorSpec","title":"model_navigator.api.triton.OutputTensorSpec  <code>dataclass</code>","text":"<p>             Bases: <code>BaseTensorSpec</code></p> <p>Stores specification of single output tensor.</p> <p>This includes name, shape, dtype and more parameters available for output tensor in Triton Inference Server:</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> <ul> <li> label_filename             (<code>Optional[str]</code>)         \u2013          <p>The label file associated with this output.</p> </li> </ul>"},{"location":"triton/instance_groups/","title":"Instance Group","text":""},{"location":"triton/instance_groups/#model-instance-group","title":"Model Instance Group","text":""},{"location":"triton/instance_groups/#model_navigator.api.triton.InstanceGroup","title":"model_navigator.api.triton.InstanceGroup  <code>dataclass</code>","text":"<p>Configuration for model instance group.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> <ul> <li> kind             (<code>Optional[DeviceKind]</code>)         \u2013          <p>Kind of this instance group.</p> </li> <li> count             (<code>Optional[int]</code>)         \u2013          <p>For a group assigned to GPU, the number of instances created for    each GPU listed in 'gpus'. For a group assigned to CPU the number    of instances created.</p> </li> <li> name             (<code>Optional[str]</code>)         \u2013          <p>Optional name of this group of instances.</p> </li> <li> gpus             (<code>List[int]</code>)         \u2013          <p>GPU(s) where instances should be available.</p> </li> <li> passive             (<code>bool</code>)         \u2013          <p>Whether the instances within this instance group will be accepting      inference requests from the scheduler.</p> </li> <li> host_policy             (<code>Optional[str]</code>)         \u2013          <p>The host policy name that the instance to be associated with.</p> </li> <li> profile             (<code>List[str]</code>)         \u2013          <p>For TensorRT models containing multiple optimization profile, this      parameter specifies a set of optimization profiles available to this      instance group.</p> </li> </ul>"},{"location":"triton/instance_groups/#model_navigator.api.triton.DeviceKind","title":"model_navigator.api.triton.DeviceKind","text":"<p>             Bases: <code>enum.Enum</code></p> <p>Device kind for model deployment.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> <ul> <li> KIND_AUTO         \u2013          <p>\"KIND_AUTO\"</p> </li> <li> KIND_CPU         \u2013          <p>\"KIND_CPU\"</p> </li> <li> KIND_GPU         \u2013          <p>\"KIND_GPU\"</p> </li> </ul>"},{"location":"triton/sequence_batcher/","title":"Sequence Batcher","text":""},{"location":"triton/sequence_batcher/#sequence-batcher","title":"Sequence Batcher","text":""},{"location":"triton/sequence_batcher/#model_navigator.api.triton.SequenceBatcher","title":"model_navigator.api.triton.SequenceBatcher  <code>dataclass</code>","text":"<p>Sequence batching configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> <ul> <li> strategy             (<code>Optional[Union[SequenceBatcherStrategyDirect, SequenceBatcherStrategyOldest]]</code>)         \u2013          <p>The strategy used by the sequence batcher.</p> </li> <li> max_sequence_idle_microseconds             (<code>Optional[int]</code>)         \u2013          <p>The maximum time, in microseconds, that a sequence is allowed to                             be idle before it is aborted.</p> </li> <li> control_inputs             (<code>List[SequenceBatcherControlInput]</code>)         \u2013          <p>The model input(s) that the server should use to communicate             sequence start, stop, ready and similar control values to the model.</p> </li> <li> states             (<code>List[SequenceBatcherState]</code>)         \u2013          <p>The optional state that can be stored in Triton for performing     inference requests on a sequence.</p> </li> </ul>"},{"location":"triton/sequence_batcher/#model_navigator.api.triton.SequenceBatcherControl","title":"model_navigator.api.triton.SequenceBatcherControl  <code>dataclass</code>","text":"<p>Sequence Batching control configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> <ul> <li> kind             (<code>SequenceBatcherControlKind</code>)         \u2013          <p>The kind of this control.</p> </li> <li> dtype             (<code>Optional[Union[np.dtype, Type[np.dtype]]]</code>)         \u2013          <p>The control's datatype.</p> </li> <li> int32_false_true             (<code>List[int]</code>)         \u2013          <p>The control's true and false setting is indicated by setting               a value in an int32 tensor.</p> </li> <li> fp32_false_true             (<code>List[float]</code>)         \u2013          <p>The control's true and false setting is indicated by setting              a value in a fp32 tensor.</p> </li> <li> bool_false_true             (<code>List[bool]</code>)         \u2013          <p>The control's true and false setting is indicated by setting              a value in a bool tensor.</p> </li> </ul>"},{"location":"triton/sequence_batcher/#model_navigator.api.triton.SequenceBatcherControlInput","title":"model_navigator.api.triton.SequenceBatcherControlInput  <code>dataclass</code>","text":"<p>Sequence Batching control input configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> <ul> <li> input_name             (<code>str</code>)         \u2013          <p>The name of the model input.</p> </li> <li> controls             (<code>List[SequenceBatcherControl]</code>)         \u2013          <p>List of  control value(s) that should be communicated to the       model using this model input.</p> </li> </ul>"},{"location":"triton/sequence_batcher/#model_navigator.api.triton.SequenceBatcherControlKind","title":"model_navigator.api.triton.SequenceBatcherControlKind","text":"<p>             Bases: <code>enum.Enum</code></p> <p>Sequence Batching control options.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> <ul> <li> CONTROL_SEQUENCE_START         \u2013          <p>\"CONTROL_SEQUENCE_START\"</p> </li> <li> CONTROL_SEQUENCE_READY         \u2013          <p>\"CONTROL_SEQUENCE_READY\"</p> </li> <li> CONTROL_SEQUENCE_END         \u2013          <p>\"CONTROL_SEQUENCE_END\"</p> </li> <li> CONTROL_SEQUENCE_CORRID         \u2013          <p>\"CONTROL_SEQUENCE_CORRID\"</p> </li> </ul>"},{"location":"triton/sequence_batcher/#model_navigator.api.triton.SequenceBatcherInitialState","title":"model_navigator.api.triton.SequenceBatcherInitialState  <code>dataclass</code>","text":"<p>Sequence Batching initial state configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> <ul> <li> name             (<code>str</code>)         \u2013          </li> <li> shape             (<code>Tuple[int, ...]</code>)         \u2013          <p>The shape of the state tensor, not including the batch dimension.</p> </li> <li> dtype             (<code>Optional[Union[np.dtype, Type[np.dtype]]]</code>)         \u2013          <p>The data-type of the state.</p> </li> <li> zero_data             (<code>Optional[bool]</code>)         \u2013          <p>The identifier for using zeros as initial state data.</p> </li> <li> data_file             (<code>Optional[str]</code>)         \u2013          <p>The file whose content will be used as the initial data for        the state in row-major order.</p> </li> </ul>"},{"location":"triton/sequence_batcher/#model_navigator.api.triton.SequenceBatcherState","title":"model_navigator.api.triton.SequenceBatcherState  <code>dataclass</code>","text":"<p>Sequence Batching state configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> <ul> <li> input_name             (<code>str</code>)         \u2013          <p>The name of the model state input.</p> </li> <li> output_name             (<code>str</code>)         \u2013          <p>The name of the model state output.</p> </li> <li> dtype             (<code>Union[np.dtype, Type[np.dtype]]</code>)         \u2013          <p>The data-type of the state.</p> </li> <li> shape             (<code>Tuple[int, ...]</code>)         \u2013          <p>The shape of the state tensor.</p> </li> <li> initial_states             (<code>List[SequenceBatcherInitialState]</code>)         \u2013          <p>The optional field to specify the list of initial states for the model.</p> </li> </ul>"},{"location":"triton/sequence_batcher/#model_navigator.api.triton.SequenceBatcherStrategyDirect","title":"model_navigator.api.triton.SequenceBatcherStrategyDirect  <code>dataclass</code>","text":"<p>Sequence Batching strategy direct configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> <ul> <li> max_queue_delay_microseconds             (<code>int</code>)         \u2013          <p>The maximum time, in microseconds, a candidate request                           will be delayed in the sequence batch scheduling queue to                           wait for additional requests for batching.</p> </li> <li> minimum_slot_utilization             (<code>float</code>)         \u2013          <p>The minimum slot utilization that must be satisfied to                       execute the batch before 'max_queue_delay_microseconds' expires.</p> </li> </ul>"},{"location":"triton/sequence_batcher/#model_navigator.api.triton.SequenceBatcherStrategyOldest","title":"model_navigator.api.triton.SequenceBatcherStrategyOldest  <code>dataclass</code>","text":"<p>Sequence Batching strategy oldest configuration.</p> <p>Read more in Triton Inference server model configuration</p> <p>Parameters:</p> <ul> <li> max_candidate_sequences             (<code>int</code>)         \u2013          <p>Maximum number of candidate sequences that the batcher maintains.</p> </li> <li> preferred_batch_size             (<code>List[int]</code>)         \u2013          <p>Preferred batch sizes for dynamic batching of candidate sequences.</p> </li> <li> max_queue_delay_microseconds             (<code>int</code>)         \u2013          <p>The maximum time, in microseconds, a candidate request                           will be delayed in the dynamic batch scheduling queue to                           wait for additional requests for batching.</p> </li> </ul>"},{"location":"triton/specialized_configs/","title":"Specialized Configs","text":""},{"location":"triton/specialized_configs/#specialized-configs-for-triton-backends","title":"Specialized Configs for Triton Backends","text":"<p>The Python API provides specialized configuration classes that help provide only available options for the given type of model.</p>"},{"location":"triton/specialized_configs/#model_navigator.api.triton.BaseSpecializedModelConfig","title":"model_navigator.api.triton.BaseSpecializedModelConfig  <code>dataclass</code>","text":"<p>             Bases: <code>abc.ABC</code></p> <p>Common fields for specialized model configs.</p> <p>Read more in Triton Inference server documentation</p> <p>Parameters:</p> <ul> <li> max_batch_size             (<code>int</code>)         \u2013          <p>The maximal batch size that would be handled by model.</p> </li> <li> batching             (<code>bool</code>)         \u2013          <p>Flag to enable/disable batching for model.</p> </li> <li> batcher             (<code>Union[DynamicBatcher, SequenceBatcher]</code>)         \u2013          <p>Configuration of Dynamic Batching for the model.</p> </li> <li> instance_groups             (<code>List[InstanceGroup]</code>)         \u2013          <p>Instance groups configuration for multiple instances of the model</p> </li> <li> parameters             (<code>Dict[str, str]</code>)         \u2013          <p>Custom parameters for model or backend</p> </li> <li> response_cache             (<code>bool</code>)         \u2013          <p>Flag to enable/disable response cache for the model</p> </li> </ul>"},{"location":"triton/specialized_configs/#model_navigator.triton.specialized_configs.base_model_config.BaseSpecializedModelConfig.backend","title":"backend  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>backend: Backend\n</code></pre> <p>Backend property that has to be overriden by specialized configs.</p>"},{"location":"triton/specialized_configs/#model_navigator.api.triton.ONNXModelConfig","title":"model_navigator.api.triton.ONNXModelConfig  <code>dataclass</code>","text":"<p>             Bases: <code>BaseSpecializedModelConfig</code></p> <p>Specialized model config for ONNX backend supported model.</p> <p>Parameters:</p> <ul> <li> platform             (<code>Optional[Platform]</code>)         \u2013          <p>Override backend parameter with platform.       Possible options: Platform.ONNXRuntimeONNX</p> </li> <li> optimization             (<code>Optional[ONNXOptimization]</code>)         \u2013          <p>Possible optimization for ONNX models</p> </li> </ul>"},{"location":"triton/specialized_configs/#model_navigator.triton.specialized_configs.onnx_model_config.ONNXModelConfig.backend","title":"backend  <code>property</code>","text":"<pre><code>backend: Backend\n</code></pre> <p>Define backend value for config.</p>"},{"location":"triton/specialized_configs/#model_navigator.api.triton.ONNXOptimization","title":"model_navigator.api.triton.ONNXOptimization  <code>dataclass</code>","text":"<p>ONNX possible optimizations.</p> <p>Parameters:</p> <ul> <li> accelerator             (<code>Union[OpenVINOAccelerator, TensorRTAccelerator]</code>)         \u2013          <p>Execution accelerator for model</p> </li> </ul>"},{"location":"triton/specialized_configs/#model_navigator.api.triton.PythonModelConfig","title":"model_navigator.api.triton.PythonModelConfig  <code>dataclass</code>","text":"<p>             Bases: <code>BaseSpecializedModelConfig</code></p> <p>Specialized model config for Python backend supported model.</p> <p>Parameters:</p> <ul> <li> inputs             (<code>Sequence[InputTensorSpec]</code>)         \u2013          <p>Required definition of model inputs</p> </li> <li> outputs             (<code>Sequence[OutputTensorSpec]</code>)         \u2013          <p>Required definition of model outputs</p> </li> </ul>"},{"location":"triton/specialized_configs/#model_navigator.triton.specialized_configs.python_model_config.PythonModelConfig.backend","title":"backend  <code>property</code>","text":"<pre><code>backend: Backend\n</code></pre> <p>Define backend value for config.</p>"},{"location":"triton/specialized_configs/#model_navigator.api.triton.PyTorchModelConfig","title":"model_navigator.api.triton.PyTorchModelConfig  <code>dataclass</code>","text":"<p>             Bases: <code>BaseSpecializedModelConfig</code></p> <p>Specialized model config for PyTorch backend supported model.</p> <p>Parameters:</p> <ul> <li> platform             (<code>Optional[Platform]</code>)         \u2013          <p>Override backend parameter with platform.       Possible options: Platform.PyTorchLibtorch</p> </li> <li> inputs             (<code>Sequence[InputTensorSpec]</code>)         \u2013          <p>Required definition of model inputs</p> </li> <li> outputs             (<code>Sequence[OutputTensorSpec]</code>)         \u2013          <p>Required definition of model outputs</p> </li> </ul>"},{"location":"triton/specialized_configs/#model_navigator.triton.specialized_configs.pytorch_model_config.PyTorchModelConfig.backend","title":"backend  <code>property</code>","text":"<pre><code>backend: Backend\n</code></pre> <p>Define backend value for config.</p>"},{"location":"triton/specialized_configs/#model_navigator.api.triton.TensorFlowModelConfig","title":"model_navigator.api.triton.TensorFlowModelConfig  <code>dataclass</code>","text":"<p>             Bases: <code>BaseSpecializedModelConfig</code></p> <p>Specialized model config for TensorFlow backend supported model.</p> <p>Parameters:</p> <ul> <li> platform             (<code>Optional[Platform]</code>)         \u2013          <p>Override backend parameter with platform.       Possible options: Platform.TensorFlowSavedModel, Platform.TensorFlowGraphDef</p> </li> <li> optimization             (<code>Optional[TensorFlowOptimization]</code>)         \u2013          <p>Possible optimization for TensorFlow models</p> </li> </ul>"},{"location":"triton/specialized_configs/#model_navigator.triton.specialized_configs.tensorflow_model_config.TensorFlowModelConfig.backend","title":"backend  <code>property</code>","text":"<pre><code>backend: Backend\n</code></pre> <p>Define backend value for config.</p>"},{"location":"triton/specialized_configs/#model_navigator.api.triton.TensorFlowOptimization","title":"model_navigator.api.triton.TensorFlowOptimization  <code>dataclass</code>","text":"<p>TensorFlow possible optimizations.</p> <p>Parameters:</p> <ul> <li> accelerator             (<code>Union[AutoMixedPrecisionAccelerator, GPUIOAccelerator, TensorRTAccelerator]</code>)         \u2013          <p>Execution accelerator for model</p> </li> </ul>"},{"location":"triton/specialized_configs/#model_navigator.api.triton.TensorRTModelConfig","title":"model_navigator.api.triton.TensorRTModelConfig  <code>dataclass</code>","text":"<p>             Bases: <code>BaseSpecializedModelConfig</code></p> <p>Specialized model config for TensorRT platform supported model.</p> <p>Parameters:</p> <ul> <li> platform             (<code>Optional[Platform]</code>)         \u2013          <p>Override backend parameter with platform.       Possible options: Platform.TensorRTPlan</p> </li> <li> optimization             (<code>Optional[TensorRTOptimization]</code>)         \u2013          <p>Possible optimization for TensorRT models</p> </li> </ul>"},{"location":"triton/specialized_configs/#model_navigator.triton.specialized_configs.tensorrt_model_config.TensorRTModelConfig.backend","title":"backend  <code>property</code>","text":"<pre><code>backend: Backend\n</code></pre> <p>Define backend value for config.</p>"},{"location":"triton/specialized_configs/#model_navigator.api.triton.TensorRTOptimization","title":"model_navigator.api.triton.TensorRTOptimization  <code>dataclass</code>","text":"<p>TensorRT possible optimizations.</p> <p>Parameters:</p> <ul> <li> cuda_graphs             (<code>bool</code>)         \u2013          <p>Use CUDA graphs API to capture model operations and execute them more efficiently.</p> </li> <li> gather_kernel_buffer_threshold             (<code>Optional[int]</code>)         \u2013          <p>The backend may use a gather kernel to gather input data if the                             device has direct access to the source buffer and the destination                             buffer.</p> </li> <li> eager_batching             (<code>bool</code>)         \u2013          <p>Start preparing the next batch before the model instance is ready for the next inference.</p> </li> </ul>"},{"location":"triton/triton_deployment/","title":"Deploying models","text":""},{"location":"triton/triton_deployment/#deploying-a-model-on-the-triton-inference-server","title":"Deploying a model on the Triton Inference Server","text":"<p>Triton Model Navigator provides an API for working with the Triton model repository. Currently, we support adding your own model or a pre-selected model from a Navigator Package.</p> <p>The API only provides possible functionality for the given model's type and only provides offline validation of the provided configuration. In the end, the model with the configuration is created inside the provided model repository path.</p>"},{"location":"triton/triton_deployment/#adding-your-own-model-to-the-triton-model-repository","title":"Adding your own model to the Triton model repository","text":"<p>When you work with an already exported model you can provide a path to where one's model is located. Then you can use one of the specialized APIs that guides you through what options are possible for deployment of the selected model type.</p> <p>Example of deploying a TensorRT model:</p> <pre><code>import model_navigator as nav\nnav.triton.model_repository.add_model(\nmodel_repository_path=\"/path/to/triton/model/repository\",\nmodel_path=\"/path/to/model/plan/file\",\nmodel_name=\"NameOfModel\",\nconfig=nav.triton.TensorRTModelConfig(\nmax_batch_size=256,\noptimization=nav.triton.CUDAGraphOptimization(),\nresponse_cache=True,\n)\n)\n</code></pre> <p>The model catalog with the model file and configuration is going to be created inside <code>model_repository_path</code>. More about the function you can find in adding model section.</p>"},{"location":"triton/triton_deployment/#adding-model-from-package-to-the-triton-model-repository","title":"Adding model from package to the Triton model repository","text":"<p>When you want to deploy a model from a package created during the <code>optimize</code> process you can use:</p> <pre><code>import model_navigator as nav\nnav.triton.model_repository.add_model_from_package(\nmodel_repository_path=\"/path/to/triton/model/repository\",\nmodel_name=\"NameOfModel\",\npackage=package,\n)\n</code></pre> <p>The model is automatically selected based on profiling results. The default selection options can be adjusted by changing the <code>strategy</code> argument. More about the function you can find in adding model section.</p>"},{"location":"triton/triton_deployment/#using-triton-model-analyzer","title":"Using Triton Model Analyzer","text":"<p>A model added to the Triton Inference Server can be further optimized in the target environment using Triton Model Analyzer.</p> <p>Please, follow the documentation to learn more how to use Triton Model Analyzer.</p>"}]}